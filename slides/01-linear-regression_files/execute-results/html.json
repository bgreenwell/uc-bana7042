{
  "hash": "25f2facec9c9df30a0764d60bbe841fe",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Regression\"\nsubtitle: \"A Brief Review\"\nauthor: \"Brandon M. Greenwell, PhD\"\ninstitute: \"University of Cincinnati\"\nfrom: markdown+emoji\n---\n\n\n\n\n## About me {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- :man_student: B.S. & M.S. in Applied Statistics ([WSU](https://www.wright.edu/))\n- :man_student: Ph.D. in Applied Matehmatics ([AFIT](https://www.afit.edu/))\n- :clapper: Director, Data Science at [84.51˚](https://www.8451.com/) \n- :man_teacher: UC LCB adjunct (~8 years)\n- Some R packages :package: :\n  - [pdp](https://CRAN.R-project.org/package=pdp) (partial dependence plots)\n  - [vip](https://CRAN.R-project.org/package=vip) (variable importance plots)\n  - [fastshap](https://CRAN.R-project.org/package=fastshap) (faster SHAP values)\n- Some books :books: :\n  - [Hands-On Machine Learning with R](https://bradleyboehmke.github.io/HOML/) \n  - [Tree-Based Methods for Statistical Learning](https://www.routledge.com/Tree-Based-Methods-for-Statistical-Learning-in-R/Greenwell/p/book/9780367532468?srsltid=AfmBOoq9xbq6yMdXzO2BUsLfLVm0XyVDFyFmqu4sh5xkCcZBLXMUZ4jI)\n:::\n\n::: {.column width=\"50%\"}\n![](images/logos.png){width=\"50%\" fig-align=center}\n\n![](images/books.png){width=\"50%\" fig-align=center}\n:::\n\n::::\n\n\n## Ames housing data\n\n* Data describing the sale of individual residential property in Ames, Iowa\nfrom 2006 to 2010\n* There are 2930 observations on 81 variables involved in assessing home\nvalues:\n  - 23 nominal\n  - 23 ordinal\n  - 14 discrete\n  - 20 continuous\n* Paper: [https://jse.amstat.org/v19n3/decock.pdf](https://jse.amstat.org/v19n3/decock.pdf)\n\n\n## Ames housing data {.scrollable}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\names <- AmesHousing::make_ames()  # install.packages(\"AmesHousing\")\names$Sale_Price <- ames$Sale_Price / 10000\nhead(ames)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"MS_SubClass\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"MS_Zoning\"],\"name\":[2],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Lot_Frontage\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Lot_Area\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Street\"],\"name\":[5],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Alley\"],\"name\":[6],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Lot_Shape\"],\"name\":[7],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Land_Contour\"],\"name\":[8],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Utilities\"],\"name\":[9],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Lot_Config\"],\"name\":[10],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Land_Slope\"],\"name\":[11],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Neighborhood\"],\"name\":[12],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Condition_1\"],\"name\":[13],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Condition_2\"],\"name\":[14],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Bldg_Type\"],\"name\":[15],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"House_Style\"],\"name\":[16],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Overall_Qual\"],\"name\":[17],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Overall_Cond\"],\"name\":[18],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Year_Built\"],\"name\":[19],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Year_Remod_Add\"],\"name\":[20],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Roof_Style\"],\"name\":[21],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Roof_Matl\"],\"name\":[22],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Exterior_1st\"],\"name\":[23],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Exterior_2nd\"],\"name\":[24],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Mas_Vnr_Type\"],\"name\":[25],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Mas_Vnr_Area\"],\"name\":[26],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Exter_Qual\"],\"name\":[27],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Exter_Cond\"],\"name\":[28],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Foundation\"],\"name\":[29],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Bsmt_Qual\"],\"name\":[30],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Bsmt_Cond\"],\"name\":[31],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Bsmt_Exposure\"],\"name\":[32],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"BsmtFin_Type_1\"],\"name\":[33],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"BsmtFin_SF_1\"],\"name\":[34],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"BsmtFin_Type_2\"],\"name\":[35],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"BsmtFin_SF_2\"],\"name\":[36],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Bsmt_Unf_SF\"],\"name\":[37],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Total_Bsmt_SF\"],\"name\":[38],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Heating\"],\"name\":[39],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Heating_QC\"],\"name\":[40],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Central_Air\"],\"name\":[41],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Electrical\"],\"name\":[42],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"First_Flr_SF\"],\"name\":[43],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Second_Flr_SF\"],\"name\":[44],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Low_Qual_Fin_SF\"],\"name\":[45],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Gr_Liv_Area\"],\"name\":[46],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Bsmt_Full_Bath\"],\"name\":[47],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Bsmt_Half_Bath\"],\"name\":[48],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Full_Bath\"],\"name\":[49],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Half_Bath\"],\"name\":[50],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Bedroom_AbvGr\"],\"name\":[51],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Kitchen_AbvGr\"],\"name\":[52],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Kitchen_Qual\"],\"name\":[53],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"TotRms_AbvGrd\"],\"name\":[54],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Functional\"],\"name\":[55],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Fireplaces\"],\"name\":[56],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Fireplace_Qu\"],\"name\":[57],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Garage_Type\"],\"name\":[58],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Garage_Finish\"],\"name\":[59],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Garage_Cars\"],\"name\":[60],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Garage_Area\"],\"name\":[61],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Garage_Qual\"],\"name\":[62],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Garage_Cond\"],\"name\":[63],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Paved_Drive\"],\"name\":[64],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Wood_Deck_SF\"],\"name\":[65],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Open_Porch_SF\"],\"name\":[66],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Enclosed_Porch\"],\"name\":[67],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Three_season_porch\"],\"name\":[68],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Screen_Porch\"],\"name\":[69],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Pool_Area\"],\"name\":[70],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Pool_QC\"],\"name\":[71],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Fence\"],\"name\":[72],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Misc_Feature\"],\"name\":[73],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Misc_Val\"],\"name\":[74],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Mo_Sold\"],\"name\":[75],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Year_Sold\"],\"name\":[76],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Sale_Type\"],\"name\":[77],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Sale_Condition\"],\"name\":[78],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Sale_Price\"],\"name\":[79],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Longitude\"],\"name\":[80],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Latitude\"],\"name\":[81],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"One_Story_1946_and_Newer_All_Styles\",\"2\":\"Residential_Low_Density\",\"3\":\"141\",\"4\":\"31770\",\"5\":\"Pave\",\"6\":\"No_Alley_Access\",\"7\":\"Slightly_Irregular\",\"8\":\"Lvl\",\"9\":\"AllPub\",\"10\":\"Corner\",\"11\":\"Gtl\",\"12\":\"North_Ames\",\"13\":\"Norm\",\"14\":\"Norm\",\"15\":\"OneFam\",\"16\":\"One_Story\",\"17\":\"Above_Average\",\"18\":\"Average\",\"19\":\"1960\",\"20\":\"1960\",\"21\":\"Hip\",\"22\":\"CompShg\",\"23\":\"BrkFace\",\"24\":\"Plywood\",\"25\":\"Stone\",\"26\":\"112\",\"27\":\"Typical\",\"28\":\"Typical\",\"29\":\"CBlock\",\"30\":\"Typical\",\"31\":\"Good\",\"32\":\"Gd\",\"33\":\"BLQ\",\"34\":\"2\",\"35\":\"Unf\",\"36\":\"0\",\"37\":\"441\",\"38\":\"1080\",\"39\":\"GasA\",\"40\":\"Fair\",\"41\":\"Y\",\"42\":\"SBrkr\",\"43\":\"1656\",\"44\":\"0\",\"45\":\"0\",\"46\":\"1656\",\"47\":\"1\",\"48\":\"0\",\"49\":\"1\",\"50\":\"0\",\"51\":\"3\",\"52\":\"1\",\"53\":\"Typical\",\"54\":\"7\",\"55\":\"Typ\",\"56\":\"2\",\"57\":\"Good\",\"58\":\"Attchd\",\"59\":\"Fin\",\"60\":\"2\",\"61\":\"528\",\"62\":\"Typical\",\"63\":\"Typical\",\"64\":\"Partial_Pavement\",\"65\":\"210\",\"66\":\"62\",\"67\":\"0\",\"68\":\"0\",\"69\":\"0\",\"70\":\"0\",\"71\":\"No_Pool\",\"72\":\"No_Fence\",\"73\":\"None\",\"74\":\"0\",\"75\":\"5\",\"76\":\"2010\",\"77\":\"WD\",\"78\":\"Normal\",\"79\":\"21.50\",\"80\":\"-93.61975\",\"81\":\"42.05403\"},{\"1\":\"One_Story_1946_and_Newer_All_Styles\",\"2\":\"Residential_High_Density\",\"3\":\"80\",\"4\":\"11622\",\"5\":\"Pave\",\"6\":\"No_Alley_Access\",\"7\":\"Regular\",\"8\":\"Lvl\",\"9\":\"AllPub\",\"10\":\"Inside\",\"11\":\"Gtl\",\"12\":\"North_Ames\",\"13\":\"Feedr\",\"14\":\"Norm\",\"15\":\"OneFam\",\"16\":\"One_Story\",\"17\":\"Average\",\"18\":\"Above_Average\",\"19\":\"1961\",\"20\":\"1961\",\"21\":\"Gable\",\"22\":\"CompShg\",\"23\":\"VinylSd\",\"24\":\"VinylSd\",\"25\":\"None\",\"26\":\"0\",\"27\":\"Typical\",\"28\":\"Typical\",\"29\":\"CBlock\",\"30\":\"Typical\",\"31\":\"Typical\",\"32\":\"No\",\"33\":\"Rec\",\"34\":\"6\",\"35\":\"LwQ\",\"36\":\"144\",\"37\":\"270\",\"38\":\"882\",\"39\":\"GasA\",\"40\":\"Typical\",\"41\":\"Y\",\"42\":\"SBrkr\",\"43\":\"896\",\"44\":\"0\",\"45\":\"0\",\"46\":\"896\",\"47\":\"0\",\"48\":\"0\",\"49\":\"1\",\"50\":\"0\",\"51\":\"2\",\"52\":\"1\",\"53\":\"Typical\",\"54\":\"5\",\"55\":\"Typ\",\"56\":\"0\",\"57\":\"No_Fireplace\",\"58\":\"Attchd\",\"59\":\"Unf\",\"60\":\"1\",\"61\":\"730\",\"62\":\"Typical\",\"63\":\"Typical\",\"64\":\"Paved\",\"65\":\"140\",\"66\":\"0\",\"67\":\"0\",\"68\":\"0\",\"69\":\"120\",\"70\":\"0\",\"71\":\"No_Pool\",\"72\":\"Minimum_Privacy\",\"73\":\"None\",\"74\":\"0\",\"75\":\"6\",\"76\":\"2010\",\"77\":\"WD\",\"78\":\"Normal\",\"79\":\"10.50\",\"80\":\"-93.61976\",\"81\":\"42.05301\"},{\"1\":\"One_Story_1946_and_Newer_All_Styles\",\"2\":\"Residential_Low_Density\",\"3\":\"81\",\"4\":\"14267\",\"5\":\"Pave\",\"6\":\"No_Alley_Access\",\"7\":\"Slightly_Irregular\",\"8\":\"Lvl\",\"9\":\"AllPub\",\"10\":\"Corner\",\"11\":\"Gtl\",\"12\":\"North_Ames\",\"13\":\"Norm\",\"14\":\"Norm\",\"15\":\"OneFam\",\"16\":\"One_Story\",\"17\":\"Above_Average\",\"18\":\"Above_Average\",\"19\":\"1958\",\"20\":\"1958\",\"21\":\"Hip\",\"22\":\"CompShg\",\"23\":\"Wd Sdng\",\"24\":\"Wd Sdng\",\"25\":\"BrkFace\",\"26\":\"108\",\"27\":\"Typical\",\"28\":\"Typical\",\"29\":\"CBlock\",\"30\":\"Typical\",\"31\":\"Typical\",\"32\":\"No\",\"33\":\"ALQ\",\"34\":\"1\",\"35\":\"Unf\",\"36\":\"0\",\"37\":\"406\",\"38\":\"1329\",\"39\":\"GasA\",\"40\":\"Typical\",\"41\":\"Y\",\"42\":\"SBrkr\",\"43\":\"1329\",\"44\":\"0\",\"45\":\"0\",\"46\":\"1329\",\"47\":\"0\",\"48\":\"0\",\"49\":\"1\",\"50\":\"1\",\"51\":\"3\",\"52\":\"1\",\"53\":\"Good\",\"54\":\"6\",\"55\":\"Typ\",\"56\":\"0\",\"57\":\"No_Fireplace\",\"58\":\"Attchd\",\"59\":\"Unf\",\"60\":\"1\",\"61\":\"312\",\"62\":\"Typical\",\"63\":\"Typical\",\"64\":\"Paved\",\"65\":\"393\",\"66\":\"36\",\"67\":\"0\",\"68\":\"0\",\"69\":\"0\",\"70\":\"0\",\"71\":\"No_Pool\",\"72\":\"No_Fence\",\"73\":\"Gar2\",\"74\":\"12500\",\"75\":\"6\",\"76\":\"2010\",\"77\":\"WD\",\"78\":\"Normal\",\"79\":\"17.20\",\"80\":\"-93.61939\",\"81\":\"42.05266\"},{\"1\":\"One_Story_1946_and_Newer_All_Styles\",\"2\":\"Residential_Low_Density\",\"3\":\"93\",\"4\":\"11160\",\"5\":\"Pave\",\"6\":\"No_Alley_Access\",\"7\":\"Regular\",\"8\":\"Lvl\",\"9\":\"AllPub\",\"10\":\"Corner\",\"11\":\"Gtl\",\"12\":\"North_Ames\",\"13\":\"Norm\",\"14\":\"Norm\",\"15\":\"OneFam\",\"16\":\"One_Story\",\"17\":\"Good\",\"18\":\"Average\",\"19\":\"1968\",\"20\":\"1968\",\"21\":\"Hip\",\"22\":\"CompShg\",\"23\":\"BrkFace\",\"24\":\"BrkFace\",\"25\":\"None\",\"26\":\"0\",\"27\":\"Good\",\"28\":\"Typical\",\"29\":\"CBlock\",\"30\":\"Typical\",\"31\":\"Typical\",\"32\":\"No\",\"33\":\"ALQ\",\"34\":\"1\",\"35\":\"Unf\",\"36\":\"0\",\"37\":\"1045\",\"38\":\"2110\",\"39\":\"GasA\",\"40\":\"Excellent\",\"41\":\"Y\",\"42\":\"SBrkr\",\"43\":\"2110\",\"44\":\"0\",\"45\":\"0\",\"46\":\"2110\",\"47\":\"1\",\"48\":\"0\",\"49\":\"2\",\"50\":\"1\",\"51\":\"3\",\"52\":\"1\",\"53\":\"Excellent\",\"54\":\"8\",\"55\":\"Typ\",\"56\":\"2\",\"57\":\"Typical\",\"58\":\"Attchd\",\"59\":\"Fin\",\"60\":\"2\",\"61\":\"522\",\"62\":\"Typical\",\"63\":\"Typical\",\"64\":\"Paved\",\"65\":\"0\",\"66\":\"0\",\"67\":\"0\",\"68\":\"0\",\"69\":\"0\",\"70\":\"0\",\"71\":\"No_Pool\",\"72\":\"No_Fence\",\"73\":\"None\",\"74\":\"0\",\"75\":\"4\",\"76\":\"2010\",\"77\":\"WD\",\"78\":\"Normal\",\"79\":\"24.40\",\"80\":\"-93.61732\",\"81\":\"42.05125\"},{\"1\":\"Two_Story_1946_and_Newer\",\"2\":\"Residential_Low_Density\",\"3\":\"74\",\"4\":\"13830\",\"5\":\"Pave\",\"6\":\"No_Alley_Access\",\"7\":\"Slightly_Irregular\",\"8\":\"Lvl\",\"9\":\"AllPub\",\"10\":\"Inside\",\"11\":\"Gtl\",\"12\":\"Gilbert\",\"13\":\"Norm\",\"14\":\"Norm\",\"15\":\"OneFam\",\"16\":\"Two_Story\",\"17\":\"Average\",\"18\":\"Average\",\"19\":\"1997\",\"20\":\"1998\",\"21\":\"Gable\",\"22\":\"CompShg\",\"23\":\"VinylSd\",\"24\":\"VinylSd\",\"25\":\"None\",\"26\":\"0\",\"27\":\"Typical\",\"28\":\"Typical\",\"29\":\"PConc\",\"30\":\"Good\",\"31\":\"Typical\",\"32\":\"No\",\"33\":\"GLQ\",\"34\":\"3\",\"35\":\"Unf\",\"36\":\"0\",\"37\":\"137\",\"38\":\"928\",\"39\":\"GasA\",\"40\":\"Good\",\"41\":\"Y\",\"42\":\"SBrkr\",\"43\":\"928\",\"44\":\"701\",\"45\":\"0\",\"46\":\"1629\",\"47\":\"0\",\"48\":\"0\",\"49\":\"2\",\"50\":\"1\",\"51\":\"3\",\"52\":\"1\",\"53\":\"Typical\",\"54\":\"6\",\"55\":\"Typ\",\"56\":\"1\",\"57\":\"Typical\",\"58\":\"Attchd\",\"59\":\"Fin\",\"60\":\"2\",\"61\":\"482\",\"62\":\"Typical\",\"63\":\"Typical\",\"64\":\"Paved\",\"65\":\"212\",\"66\":\"34\",\"67\":\"0\",\"68\":\"0\",\"69\":\"0\",\"70\":\"0\",\"71\":\"No_Pool\",\"72\":\"Minimum_Privacy\",\"73\":\"None\",\"74\":\"0\",\"75\":\"3\",\"76\":\"2010\",\"77\":\"WD\",\"78\":\"Normal\",\"79\":\"18.99\",\"80\":\"-93.63893\",\"81\":\"42.06090\"},{\"1\":\"Two_Story_1946_and_Newer\",\"2\":\"Residential_Low_Density\",\"3\":\"78\",\"4\":\"9978\",\"5\":\"Pave\",\"6\":\"No_Alley_Access\",\"7\":\"Slightly_Irregular\",\"8\":\"Lvl\",\"9\":\"AllPub\",\"10\":\"Inside\",\"11\":\"Gtl\",\"12\":\"Gilbert\",\"13\":\"Norm\",\"14\":\"Norm\",\"15\":\"OneFam\",\"16\":\"Two_Story\",\"17\":\"Above_Average\",\"18\":\"Above_Average\",\"19\":\"1998\",\"20\":\"1998\",\"21\":\"Gable\",\"22\":\"CompShg\",\"23\":\"VinylSd\",\"24\":\"VinylSd\",\"25\":\"BrkFace\",\"26\":\"20\",\"27\":\"Typical\",\"28\":\"Typical\",\"29\":\"PConc\",\"30\":\"Typical\",\"31\":\"Typical\",\"32\":\"No\",\"33\":\"GLQ\",\"34\":\"3\",\"35\":\"Unf\",\"36\":\"0\",\"37\":\"324\",\"38\":\"926\",\"39\":\"GasA\",\"40\":\"Excellent\",\"41\":\"Y\",\"42\":\"SBrkr\",\"43\":\"926\",\"44\":\"678\",\"45\":\"0\",\"46\":\"1604\",\"47\":\"0\",\"48\":\"0\",\"49\":\"2\",\"50\":\"1\",\"51\":\"3\",\"52\":\"1\",\"53\":\"Good\",\"54\":\"7\",\"55\":\"Typ\",\"56\":\"1\",\"57\":\"Good\",\"58\":\"Attchd\",\"59\":\"Fin\",\"60\":\"2\",\"61\":\"470\",\"62\":\"Typical\",\"63\":\"Typical\",\"64\":\"Paved\",\"65\":\"360\",\"66\":\"36\",\"67\":\"0\",\"68\":\"0\",\"69\":\"0\",\"70\":\"0\",\"71\":\"No_Pool\",\"72\":\"No_Fence\",\"73\":\"None\",\"74\":\"0\",\"75\":\"6\",\"76\":\"2010\",\"77\":\"WD\",\"78\":\"Normal\",\"79\":\"19.55\",\"80\":\"-93.63893\",\"81\":\"42.06078\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n## Ames housing data\n\nWe'll focus on a handful of variables:\n\n* `Sale_Price` - Sale price of the house / \\$10K (response variable)\n* `Gr_Liv_Area` - Above grade (ground) living area square feet\n* `Overall_Qual⁠` - Rates the overall material and finish of the house\n\n\n## Statistical relationships\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\n# Simulate data from different SLR models\nset.seed(101)  # for reproducibility\nx <- seq(from = 0, to = 4, length = 100)\ny <- cbind(\n  1 + x + rnorm(length(x)),  # linear\n  1 + (x - 2)^2 + rnorm(length(x)),  # quadratic\n  1 + log(x + 0.1) + rnorm(length(x), sd = 0.3),  # logarithmic\n  1 + rnorm(length(x))  # no association\n)\n\n# Scatterplot of X vs. each Y in a 2-by-2 grid\npar(mfrow = c(2, 2))\nfor (i in 1:4) {\n  plot(x, y[, i], col = adjustcolor(\"cornflowerblue\", alpha.f = 0.7),\n       pch = 19, xlab = \"X\", ylab = \"Y\")\n}\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/slr-sim-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Are $X$ and $Y$ correlated?\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\nplot(x, y[, 3], col = adjustcolor(\"cornflowerblue\", alpha.f = 0.7),\n     pch = 19, xlab = \"X\", ylab = \"Y\")\nr <- round(cor(x, y[, 3]), digits = 3)\nlegend(\"bottomright\", legend = paste0(\"r = \", r), bty = \"n\", inset = 0.01)\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/pearson-corr-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Pearson's correlation coefficient \n\n* The (Pearson) correlation between two random variables $X$ and $Y$ is given by\n\n$$Cor\\left(X, Y\\right) = \\rho = \\frac{Cov\\left(X,Y\\right)}{\\sigma_X\\sigma_Y}$$\n\n* Given a sample of $n$ pairs $\\left\\{\\left(x_i, y_i\\right)\\right\\}_{i=1}^n$, we estimate $\\rho$ with $r = S_{xy} / \\sqrt{S_{xx}S_{yy}}$, where, for example, $$S_{xx} = \\sum_{i=1}^n\\left(x_i - \\bar{x}\\right)^2 \\text{ and } S_{xy} = \\sum_{i=1}^n\\left(x_i - \\bar{x}\\right)\\left(y_i - \\bar{y}\\right)$$\n\n\n## Pearson's correlation coefficient\n\n* Range: $-1 \\le r \\le 1$\n\n* What does it measure?\n\n  - Pearson's correlation coefficient is a **unitless** measure of the strength of the **linear** relationship between two variables\n\n* Other useful correlation measures also exist:\n\n  - Spearman's rank correlation (or Spearman's $\\rho$) only assumes a *monotonic relationship* between $X$ and $Y$\n\n    * Equivalent to computing $r$ on the *ranks* of $X$ and $Y$\n\n\n## Pearson's correlation coefficient\n\n* It is common to test the hypothesis $H_0: \\rho = 0$ vs. $H_1: \\rho \\ne 0$\n\n  - Rejecting $H_0$ is only evidence that $\\rho$ is **not exactly zero** (NOT VERY USEFUL, OR INTERESTING)\n\n  - A $p$-value **does not measure the magnitude/strength of the (linear) association**\n\n  - Sample size affects the $p$-value! :scream:\n\n\n## Are $x$ and $y$ correlated?\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\nset.seed(1051)  # for reproducibility\nn <- 1000 \nx <- rnorm(n)\ny <- 1 + 0.001*x + rnorm(n)\nplot(x, y, asp = 1, col = adjustcolor(\"black\", alpha.f = 0.3))\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/spurious-corr-1-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Pearson's correlation coefficient\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(1050)  # for reproducibility\nn <- 100\nx <- rnorm(n)\ny <- 1 + 0.001*x + rnorm(n)\ncor.test(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  x and y\nt = -0.0028012, df = 98, p-value = 0.9978\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1966901  0.1961461\nsample estimates:\n          cor \n-0.0002829617 \n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(1051)  # for reproducibility\nn <- 10000000  # n = ten million\nx <- rnorm(n)\ny <- 1 + 0.001*x + rnorm(n)\ncor.test(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  x and y\nt = 3.731, df = 9999998, p-value = 0.0001907\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.0005600528 0.0017996412\nsample estimates:\n        cor \n0.001179847 \n```\n\n\n:::\n:::\n\n:::\n:::\n\nThe real question is, are $X$ and $Y$ *practically* uncorrelated?\n\n\n## Correlation is not causation\n\n<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/ntnalq-2nNU\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen>\n\n</iframe>\n\n------------------------------------------------------------------------\n\n::: r-fit-text\n[Fun with spurious correlations](http://www.tylervigen.com/spurious-correlations){preview-link=\"true\" style=\"text-align: center\"}\n:::\n\n![](images/spurious-correlation.png)\n\n------------------------------------------------------------------------\n\n\n## All models are wrong!\n\n<center>\n\n![](images/all-models.jpeg){width=\"100%\"}\n</center>\n\nAlso, see [this talk](https://statmodeling.stat.columbia.edu/wp-content/uploads/2012/03/tarpey.pdf) by my old adviser, Thad Tarpey: \"All Models are Right... most are useless.\"\n\n\n# Simple Linear Regression (SLR)\n\n## Pearson's correlation vs. SLR\n\n* There's a formal relationship between Pearson's correlation coefficient ($\\rho$) and the SLR model\n\n* \"Simple\" linear relationships can be described by an *intercept* and *slope*:\n\n  - $y = mx + b$ (algebra)\n  - $\\mu = \\beta_0 + \\beta_1x$ (statistics)\n\n* \"Simple\" here means two variables, $x$ and $y$ (but $y$ can be linearly related to several variables)\n\n\n## Example: Ames housing\n\nCheck out [this paper](http://jse.amstat.org/v19n3/decock.pdf) for useful background on the Ames housing data in regression \n\n\n## Example: Ames housing\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(cbind(ames$Sale_Price, ames$Gr_Liv_Area))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1] [,2]\n[1,] 21.50 1656\n[2,] 10.50  896\n[3,] 17.20 1329\n[4,] 24.40 2110\n[5,] 18.99 1629\n[6,] 19.55 1604\n```\n\n\n:::\n\n```{.r .cell-code}\ncor.test(ames$Sale_Price, y = ames$Gr_Liv_Area)  # see ?cor.test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  ames$Sale_Price and ames$Gr_Liv_Area\nt = 54.061, df = 2928, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6881814 0.7244502\nsample estimates:\n      cor \n0.7067799 \n```\n\n\n:::\n:::\n\n\nThis doesn't tell us much about the nature of the linear relationship between `Gr_Liv_Area` and `Sale_Price`\n\n\n## Statistical relationships\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2) \n\np1 <- ggplot(investr::crystal, aes(x = time, y = weight)) +\n  geom_point() +\n  labs(x = \"Time (hours)\", \n       y = \"Weight (grams)\", \n       title = \"Crystal weight data\")\np2 <- ggplot(investr::arsenic, aes(x = actual, y = measured)) +\n  geom_point() +\n  labs(x = \"True amount of arsenic\", \n       y = \"Measured amount of arsenic\",\n       title = \"Arsenic concentration data\")\ngridExtra::grid.arrange(p1, p2, nrow = 1)\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/statistical-relationships-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Examples of statistical relationships\n\n-   Simple linear regression: $Y = \\beta_0 + \\beta_1 X + \\epsilon$\n\n-   Multiple linear regression: $Y = \\beta_0 + \\sum_{i=1}^p \\beta_p X_p + \\epsilon$\n\n-   Polynomial regression: $Y = \\beta_0 + \\sum_{i=1}^p \\beta_p X^p + \\epsilon$\n\n-   Logarithmic: $Y = \\beta_0 + \\beta_1 \\log\\left(X + 0.1\\right) + \\epsilon$\n\n-   Nonlinear regression: $Y = \\frac{\\beta_1 X}{\\left(\\beta_2 + X\\right)} + \\epsilon$\n\n-   Multiplicative: $Y = \\beta X \\epsilon$\n\n    -   $\\log\\left(Y\\right) = \\alpha + \\log\\left(X\\right) + \\log\\left(\\epsilon\\right)$\n\nAssuming $\\epsilon \\sim D\\left(\\mu, \\sigma\\right)$\n\n\n## Simple linear regression (SLR)\n\n* Data: $\\left\\{\\left(X_i, Y_i\\right)\\right\\}_{i=1}^n$\n\n* Model: $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$\n\n  - $Y_i$ is a continuous response\n\n  - $X_i$ is a continuous predictor\n\n  - $\\beta_0$ is the intercept of the regression line (also called the *bias term*)\n\n  - $\\beta_1$ is the slope of the regression line\n\n  - $\\epsilon_i \\stackrel{iid}{\\sim} D\\left(0, \\sigma^2\\right)$\n\n\n## Assumptions about the errors $\\epsilon_i$\n\nFor $i$ and $j$ in $\\left\\{1, 2, \\dots, n\\right\\}$ and $i \\ne j$\n\n1)  $\\quad E\\left(\\epsilon_i\\right) = 0$\n\n2)  $\\quad Var\\left(\\epsilon_i\\right) = \\sigma^2$ (homoscedacticity :scream:)\n\n3)  $\\quad Cov\\left(\\epsilon_i, \\epsilon_j\\right) = 0$ (independence)\n\n\n\nAssumptions 1--3 can be summarized as $\\epsilon_i \\stackrel{iid}{\\sim} D\\left(0, \\sigma^2\\right)$, where $iid$ refers to [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables).\n\n\n## Properties of SLR\n\n-   Simple linear regression: $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$\n\n    -   Assumes the model is **linear in the regression coefficients** $\\beta_0$ and $\\beta_1$\n\n-   The error term is a random variable; hence, $Y_i$ is also a random variable (**Why?** :thinking:)\n\n    -   What is $E\\left(Y_i|X_i\\right)$ and $Var\\left(Y_i|X_i\\right)$?\n\n-   $Cor\\left(Y_i, Y_j\\right) = 0$ $\\forall i \\ne j$ (**Why?** :thinking:)\n\n\n## Inference for a single variable {.smaller}\n\n* Is it useful to test the hypothesis that `Sale_Price` = \\$160K?\n\n. . .\n\n* No! Because `Sale_Price` is not a constant, but a random variable whose value varies from home to home (and year to year, etc.)\n\n. . .\n\n* We're more interested in questions such as:\n\n  - What is the chance that `Sale_Price` > \\$160K? (above median sale price)\n  - What is the chance that `Sale_Price` < \\$105K? (lowest decile)\n  - What is the chance that \\$129,500 < `Sale_Price` < \\$213,500? (within IQR)\n\n\n## Distribution of `Sale_Price`\n\nCan look at historgram and empirical [CDF](https://en.wikipedia.org/wiki/Cumulative_distribution_function):\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2), las = 1)\nhist(ames$Sale_Price, br = 50, xlab = \"Sale price ($)\", freq = FALSE, main = \"\")\nplot(ecdf(ames$Sale_Price), xlab = \"Sale price ($)\", main = \"\",\n     col = adjustcolor(1, alpha.f = 0.1))\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Distribution of `Sale_Price`\n\n* Histograms and ECDFs are nonparammetric in nature\n* A simple parametric approach might assume a particular distribution for `Sale_Price`\n* For instance, we might assume `Sale_Price` $\\sim N\\left(\\mu, \\sigma^2\\right)$\n* How can we estimate $\\mu$ and $\\sigma^2$?\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Maximum likelihiid estimates\nc(\"sample mean\" = mean(ames$Sale_Price), \"sample stdev\" = sd(ames$Sale_Price))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n sample mean sample stdev \n   18.079606     7.988669 \n```\n\n\n:::\n:::\n\n\n. . .\n\n* Is the normal distribution a reasonable assumption here?\n\n\n## Normal QQ plot\n\n* Normal quantile-quantile (Q-Q) plot* can be used to asses the \"normalityness\" of a set of observations\n\n* Q-Q plots can, in general, be used to compare data with any distribution!\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\nqqnorm(ames$Sale_Price, col = 2, las = 1)\nqqline(ames$Sale_Price)\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Normality tests :vomiting_face: \n\n* Normality tests, like the [Shapiro-Wilk](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test)^[In R, see `?shapiro.test` for details.] and [Anderson-Darling](https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test) tests, can also be used to assess normality\n\n  - I STRONGLY ADVISE AGAINST USING THEM!\n  \n* No data is normally distributes, what we care about is whether enough a normal approximation is close enough!\n\n* Normality tests provide a $p$-value, which only gives a yes/no conclusion\n\n\n## Normality tests :vomiting_face: \n\nRecall that $p$-values are a function of sample size!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Shapiro-Wilk test results vs. sample size\nset.seed(101)  # for reproducibility\nx <- replicate(100, c(\n  shapiro.test(rt(10, df = 40))$p.value,\n  shapiro.test(rt(100, df = 40))$p.value,\n  shapiro.test(rt(500, df = 40))$p.value,\n  shapiro.test(rt(1000, df = 40))$p.value,\n  shapiro.test(rt(2500, df = 40))$p.value,\n  shapiro.test(rt(5000, df = 40))$p.value\n))\nrownames(x) <- paste0(\"n=\", c(10, 100, 500, 1000, 2500, 5000))\nrowMeans(x < 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  n=10  n=100  n=500 n=1000 n=2500 n=5000 \n  0.01   0.08   0.11   0.22   0.24   0.39 \n```\n\n\n:::\n:::\n\n\n\n## Normality tests :vomiting_face: \n\n\n::: {.cell layout-align=\"center\" par='true'}\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n1. Are these two distributions significantly different?\n\n2. Are these two distributions practically different?\n\n\n## What can we do if the normality assumption isn't justified?\n\n* Try transformations\n  - Logarithm or square root for positive data\n  - [Power transformation](https://en.wikipedia.org/wiki/Power_transform) (like the well-known Box-Cox procedure)\n\n* Try a more appropriate distribution (e.g., Poisson or gamma distribution)\n\n* Try more advanced approaches, like the nonparametric [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))!\n\n\n## Modeling the mean response\n\n* Assume that $Y \\sim N\\left(\\mu, \\sigma^2\\right)$, where\n\n$$\\mu = \\mu\\left(x\\right) = \\beta_0 + \\beta_1 x = E\\left(Y|x\\right)$$\n\n. . .\n\n* In other words: $Y \\sim N\\left(\\beta_0 + \\beta_1 x, \\sigma^2\\right)$\n* Alternatively, we could write $Y = \\beta_0 + \\beta_1 x + \\epsilon$, where $\\epsilon \\sim N\\left(0, \\sigma^2\\right)$\n* This is called the simple linear regression (SLR) model\n\n\n## The idea behind SLR\n\n![](https://2012books.lardbucket.org/books/beginning-statistics/section_14/88a6e0919d8617c025826c1e187ad591.jpg)\n\n[Image source](https://2012books.lardbucket.org/books/beginning-statistics/s14-03-modelling-linear-relationships.html)\n\n\n## Arsenic experiment example\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\nplot(investr::arsenic, las = 1)  # see ?investr::arsenic for details\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/arsenic-plot-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Is linear regression reasonable here?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- rep(c(1:10 / 10, 1.5, 2), each = 30)\ny <- 1 + 2*x^2 + rnorm(length(x), sd = 1)\npar(mfrow = c(1, 2), las = 1)\nplot(x, y, col = \"dodgerblue2\")\n#abline(lm(y ~ x), lwd = 2)\nhist(y, main = \"\", col = \"dodgerblue2\", border = \"white\")\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/sim-quadratic-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Is linear regression reasonable here?\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\nfit <- lm(y ~ x)\nres <- residuals(fit)\nqqnorm(res, las = 1)\nqqline(res, col = 2)\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/resid-quadratic-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Least squares (LS) estimation\n\nIdea of LS is to find $\\beta_0$ and $\\beta_1$ so that the sum of squared residuals (i.e., errors) is minimized:\n$$SSE = \\sum_{i=1}^n\\left(y_i - \\beta_0 - \\beta_1x_i\\right)^2$$\n\n. . .\n\nPretty straightforward optimization problem that leads to closed-form solution (but no point in memorizing the formulas!)\n\n\n## Concept of LS estimation\n\n![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*tQkyTR9yxDcS1GKVFhdQQA.jpeg)\n\n[Image source](https://towardsdatascience.com/how-least-squares-regression-estimates-are-actually-calculated-662d237a4d7e)\n\n\n## `Sale_Price` and `Gr_Liv_Area`\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\nplot(Sale_Price ~ Gr_Liv_Area, data = ames, las = 1,\n     col = adjustcolor(1, alpha.f = 0.3))\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/ames-slr-plot-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## SLR fit\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(fit <- lm(Sale_Price ~ Gr_Liv_Area, data = ames))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area, data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.347  -3.022  -0.197   2.273  33.432 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 1.3289634  0.3269703   4.064 4.94e-05 ***\nGr_Liv_Area 0.0111694  0.0002066  54.061  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.652 on 2928 degrees of freedom\nMultiple R-squared:  0.4995,\tAdjusted R-squared:  0.4994 \nF-statistic:  2923 on 1 and 2928 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n## Is this a good fit?\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\nplot(Sale_Price ~ Gr_Liv_Area, data = ames,\n     col = adjustcolor(1, alpha.f = 0.3))\nabline(fit, lwd = 2, col = 2)  # add SLR fit\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/ames-slr-fit-plot-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nWhich assumptions seem violated to some degree?\n\n\n## Residual diagnostics\n\n* The standard residual is defined as $e_i = y_i - \\hat{y}_i$ and can be regarded as the *observed error*\n* The residuals hold a lot of properties that make them useful for diagnosing potential issues with the model (e.g., suggesting potential transformations to try)\n* Many other kinds of residuals exist for different purposes (e.g., standardized, [studentized](https://en.wikipedia.org/wiki/Studentized_residual), jackknife or [PRESS](https://en.wikipedia.org/wiki/PRESS_statistic) residuals, etc.)\n\n\n## Properties of the residuals\n\n* $\\sum_{i=1}^n e_i = 0$ \n\n* $\\sum_{i=1}^n e_i^2$ is a minimum\n\n* $\\sum_{i=1}^n X_ie_i = 0$\n\n* $\\sum_{i=1}^n \\hat{Y}_ie_i = 0$\n\n* The LS regression line passes through the point $\\left(\\bar{X}, \\bar{Y}\\right)$ (i.e., the center of the training data)\n\n\n## What can residual plots tell us?\n\n* Residuals vs. predictor values (**checking non-linearity**).\n\n* Residuals vs. fitted values (**non-constant variance, non-linearity, and outliers**)\n\n* Residuals vs. time or another sequence (**checking independence**)\n\n* Residuals vs. omitted predictor values (**missing potentially important predictors**)\n\n* Normal QQ plot of residuals (**non-normality**).\n\n* And much, much more!\n\n\n## `Sale_Price ~ Gr_Liv_Area`\n\nResidual analysis:\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\npar(mfrow = c(2, 3), las = 1)\nplot(fit, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/ames-slr-resid-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nWhat assumptions appear to be in violation?\n\n\n## Let's try a log transformation\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\nfit2 <- lm(log(Sale_Price) ~ Gr_Liv_Area, data = ames)\nplot(log(Sale_Price) ~ Gr_Liv_Area, data = ames,\n     col = adjustcolor(1, alpha.f = 0.3))\nabline(fit2, lwd = 2, col = 2)  # add SLR fit\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/ames-log-fit-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Let's try a log transformation\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(Sale_Price) ~ Gr_Liv_Area, data = ames)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.36215 -0.15145  0.03091  0.16583  0.90332 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 1.9692014  0.0169355  116.28   <2e-16 ***\nGr_Liv_Area 0.0005611  0.0000107   52.43   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2928 on 2928 degrees of freedom\nMultiple R-squared:  0.4842,\tAdjusted R-squared:  0.484 \nF-statistic:  2749 on 1 and 2928 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n## `log(Sale_Price) ~ Gr_Liv_Area`\n\nResidual analysis:\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\npar(mfrow = c(2, 3), las = 1)\nplot(fit2, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/ames-log-resid-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nAny better?\n\n\n# Multiple Linear Regression (MLR)\n\n## MLR in a nutshell :peanuts:\n\n* The (normal) multiple linear regression model assumes $Y \\sim N\\left(\\mu\\left(\\boldsymbol{x}\\right), \\sigma^2\\right)$, where $$\\mu\\left(\\boldsymbol{x}\\right) = \\beta_0 + \\sum_{i=1}^p \\beta_i x_i = \\boldsymbol{x}^\\top\\boldsymbol{\\beta}$$\n\n* LS estimation still provides unbiased estimate of $\\boldsymbol{\\beta} = \\left(\\beta_0, \\beta_1, \\dots, \\beta_p\\right)^\\top$: $\\hat{\\boldsymbol{\\beta}} = \\left(\\boldsymbol{X}^\\top\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y}$\n\n* Fitted values: $\\hat{\\boldsymbol{y}} = \\boldsymbol{X}\\left(\\boldsymbol{X}^\\top\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y} = \\boldsymbol{H}\\boldsymbol{y}$\n\n* $\\boldsymbol{H}$ is the well-known \"[hat matrix](https://en.wikipedia.org/wiki/Projection_matrix)\"\n\n\n## Polynomial regression\n\n* [Polynomial regression](https://en.wikipedia.org/wiki/Polynomial_regression) is just a special case of the MLR model\n\n* A second order model in a single predictor: $$Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon$$\n\n* A *k*-th order model in a single predictor (Typically $k \\le 3$): $$Y = \\beta_0 + \\sum_{j=1}^k\\beta_j X^j + \\epsilon$$ \n\n\n## Example: paper strength data\n\nData concerning the strength of kraft paper and the percentage of hardwood in the batch of pulp from which the paper was produced.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load the hardwood conentration data\nurl <- \"https://bgreenwell.github.io/uc-bana7052/data/hardwood.csv\"\nhardwood <- read.csv(url)\n\n# Print first few observations\nhead(hardwood)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"HwdCon\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"TsStr\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1.0\",\"2\":\"6.3\",\"_rn_\":\"1\"},{\"1\":\"1.5\",\"2\":\"11.1\",\"_rn_\":\"2\"},{\"1\":\"2.0\",\"2\":\"20.0\",\"_rn_\":\"3\"},{\"1\":\"3.0\",\"2\":\"24.0\",\"_rn_\":\"4\"},{\"1\":\"4.0\",\"2\":\"26.1\",\"_rn_\":\"5\"},{\"1\":\"4.5\",\"2\":\"30.0\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n## Example: paper strength data\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\nplot(hardwood, pch = 19)\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/hardwood-plot-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Example: paper strength data\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\nfit1 <- lm(TsStr ~ HwdCon, data = hardwood)\ninvestr::plotFit(fit1, pch = 19, col.fit = \"red2\")\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/hardwood-slr-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Example: paper strength data\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2), las = 1)\n\n# Plot residuals vs HwdCon (i.e., X)\npar(mfrow = c(1, 2))\nplot(x = hardwood$HwdCon, y = residuals(fit1), xlab = \"HwdCon\",\n     ylab = \"Residuals\", main = \"Residuals vs HwdCon\")\nabline(h = 0, lty = \"dotted\")\nplot(fit1, which = 1, caption = \"\", main = \"Residuals vs Fitted\")\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/hardwood-resid-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Example: paper strength data\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\nfit2 <- lm(TsStr ~ HwdCon + I(HwdCon^2), data = hardwood)\ninvestr::plotFit(fit2, pch = 19, col.fit = \"red2\")\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/hardwood-poly2-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Example: paper strength data\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\npar(mfrow = c(2, 3), las = 1)\nfor (i in 1:6) {  # try higher-order models\n  fit <- lm(TsStr ~ poly(HwdCon, degree = i), data = hardwood)\n  investr::plotFit(fit, main = paste(\"Degree =\", i))\n}\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/hardwood-poly-compare-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Example: paper strength data\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\npar(mfrow = c(2, 3))\nfor (i in 1:6) {  # try higher-order models\n  fit <- lm(TsStr ~poly(HwdCon, degree = i), data = hardwood)\n  investr::plotFit(fit, main = paste(\"Degree =\", i), \n                   interval = \"confidence\", shade = TRUE,\n                   xlim = c(-10, 30))\n}\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/hardwood-poly-ci-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Polynomial regression\n\nSome cautions :warning:\n\nKeep the order of the model as low as possible\n\n* Avoid interpolating the data or *over fitting*\n\n* Use the simplest model possible to explain the data, but no simpler (*parsimony*)\n\n* An $n - 1$ order model can perfectly fit a data set with $n$ observations (Why is this bad :thinking:)\n\n\n## Polynomial regression\n\nTwo model-building strategies:\n\n1. Fit the lowest order polynomial possible and build up (forward selection)\n    \n2. Fit the highest order polynomial of interest, and remove terms one at a time (backward elimination)\n    \nThese two procedures may not result in the same final model\n\nIncreasing the order can result in an ill-conditioned $\\boldsymbol{X}^\\top\\boldsymbol{X}$ and [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity) \n\n\n## Categorical variables\n\n* Categorical variables can be handled in a number of ways in linear models, including\n\n  - [Dummy encoding](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) (nominal)\n  - Orthogonal polynomials (ordinal)\n\n\n## Categorical variables\n\nLet's look at two (nominal) categorical variables:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntable(ames$Central_Air)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   N    Y \n 196 2734 \n```\n\n\n:::\n\n```{.r .cell-code}\ntable(ames$Paved_Drive)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Dirt_Gravel Partial_Pavement            Paved \n             216               62             2652 \n```\n\n\n:::\n:::\n\n\n\n## Categorical variables\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\nplot(log(Sale_Price) ~ Central_Air, data = ames, las = 1, col = c(2, 3))\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/categorical-central-air-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Categorical variables\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\nplot(log(Sale_Price) ~ Paved_Drive, data = ames, las = 1, col = c(2, 3, 4))\n```\n\n::: {.cell-output-display}\n![](01-linear-regression_files/figure-revealjs/categorical-paved-drive-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nIf one of these homes downgraded from a paved driveway to a gravel driveway, would that **cause** the sale price to decrease? (Think very carefully here!)\n\n\n## Categorical variables\n\nR dummy encodes nominal factors by default:\n\n\n::: {.cell layout-align=\"center\" par='true'}\n\n```{.r .cell-code}\nfit3 <- lm(log(Sale_Price) ~ Gr_Liv_Area + Central_Air + Paved_Drive, \n           data = ames)\nsummary(fit3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(Sale_Price) ~ Gr_Liv_Area + Central_Air + Paved_Drive, \n    data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2344 -0.1345  0.0073  0.1453  0.8502 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                 1.428e+00  2.418e-02  59.059  < 2e-16 ***\nGr_Liv_Area                 5.183e-04  9.520e-06  54.440  < 2e-16 ***\nCentral_AirY                3.464e-01  2.068e-02  16.747  < 2e-16 ***\nPaved_DrivePartial_Pavement 1.334e-01  3.733e-02   3.574 0.000358 ***\nPaved_DrivePaved            3.085e-01  1.975e-02  15.618  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2574 on 2925 degrees of freedom\nMultiple R-squared:  0.6018,\tAdjusted R-squared:  0.6013 \nF-statistic:  1105 on 4 and 2925 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nHow do you interpret the coefficients here?\n\n\n## Coefficient of determination\n\nThe coefficient of determination is the proportion of the variance in the dependent variable that is predictable from the independent variables in the model.\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\nR-squared ($R^2$)\n\n* $R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}$\n\n* $R^2$ will always increase as more terms are added to the model! \n:::\n\n::: {.column width=\"50%\"}\nAdjusted R-squared ($R_{adj}^2$)\n\n<!-- * $R_{adj}^2 = 1 - \\left(\\frac{n - 1}{n - p}\\right)\\frac{SSE}{SST}$ -->\n* $R_{adj}^2 = 1 - \\frac{MSE}{SST/\\left(n - 1\\right)}$\n\n* Penalizes $R^2$ if there are \"too many\" terms in the model\n\n* $R_{adj}^2$ and $MSE$ provide equivalent information\n:::\n\n::::\n\n\n## Variable/model selection\n\n* Variable/model selection is a very noisy problem! (Often best to avoid, if feasible)\n* Ask the domain experts about important variables (don't just rely on algorithms)\n* P(selecting the \"right\" variables) = 0 ([source](https://www.youtube.com/watch?v=DF1WsYZ94Es))\n* \"All ~~models~~ subsets of variables are wrong, but some are useful!\"\n* In regression settings, regularization (e.g., [ridge regression](https://en.wikipedia.org/wiki/Ridge_regression) and the [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics))) is often more useful! (Think about the impact of [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity) on variable selection)\n\n\n## Data splitting\n\n* If prediction is the goal (e.g., compared to inference and hypothesis testing), the model performance should be assessed rigorously\n\n* Data splitting techniques ae key and the type of data splitting to use often depends on the situation (e.g., cross-sectional vs. time-series data)\n\n* In simplest terms, random split the data into two parts: A and B. Build a model on part A and see how well it does with predicting the response in part B.\n\n* [Leakage](https://reproducible.cs.princeton.edu/) is a huge concern here, so data splitting ALWAYS has to be done carefully!\n\n\n## Data splitting: $k$-fold cross-validation {.smaller}\n\n![](https://bradleyboehmke.github.io/HOML/images/cv.png)\n\nThe [PRESS statistic](https://en.wikipedia.org/wiki/PRESS_statistic) in linear regression is a special case ($k = n$) we get for free!\n\n\n## Questions?",
    "supporting": [
      "01-linear-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}