[
  {
    "objectID": "syllabus/syllabus.html",
    "href": "syllabus/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Lindner College of Business logo"
  },
  {
    "objectID": "syllabus/syllabus.html#required-tools",
    "href": "syllabus/syllabus.html#required-tools",
    "title": "Syllabus",
    "section": "Required Tools",
    "text": "Required Tools\n\nR and RStudio\nGit & a GitHub account"
  },
  {
    "objectID": "syllabus/syllabus.html#course-materials-repository",
    "href": "syllabus/syllabus.html#course-materials-repository",
    "title": "Syllabus",
    "section": "Course Materials & Repository",
    "text": "Course Materials & Repository\n\nCourse Website: https://bgreenwell.github.io/uc-bana7042/ - Contains all syllabus, slides, and assignments.\nTextbook: No required textbook. Readings will be provided from open-source materials."
  },
  {
    "objectID": "syllabus/syllabus.html#p-professionalism",
    "href": "syllabus/syllabus.html#p-professionalism",
    "title": "Syllabus",
    "section": "P ‚Äì Professionalism",
    "text": "P ‚Äì Professionalism\n\nEnhance oral & written communication through technical documentation and project presentations.\nPractice professional habits of punctuality, preparation, respect and participation in modern software development environments."
  },
  {
    "objectID": "syllabus/syllabus.html#a-academics",
    "href": "syllabus/syllabus.html#a-academics",
    "title": "Syllabus",
    "section": "A ‚Äì Academics",
    "text": "A ‚Äì Academics\n\nDevelop foundational knowledge of statistical modeling concepts and their applications in real-world business problems.\nApply rigorous analytical techniques to critically analyze and solve technical problems using data."
  },
  {
    "objectID": "syllabus/syllabus.html#c-character",
    "href": "syllabus/syllabus.html#c-character",
    "title": "Syllabus",
    "section": "C ‚Äì Character",
    "text": "C ‚Äì Character\n\nUnderstand importance of ethics and social responsibility in statistical modeling and data analysis."
  },
  {
    "objectID": "syllabus/syllabus.html#e-engagement",
    "href": "syllabus/syllabus.html#e-engagement",
    "title": "Syllabus",
    "section": "E ‚Äì Engagement",
    "text": "E ‚Äì Engagement\n\nDevelop awareness and appreciation of involvement in the data science community."
  },
  {
    "objectID": "syllabus/syllabus.html#course-structure",
    "href": "syllabus/syllabus.html#course-structure",
    "title": "Syllabus",
    "section": "Course Structure:",
    "text": "Course Structure:\nChanges to the syllabus, due dates, course requirements or grading requirements will be made as far in advance as possible. Due dates will be clearly marked in Canvas."
  },
  {
    "objectID": "syllabus/syllabus.html#missed-andor-late-examinations-quizzes-and-graded-exercises",
    "href": "syllabus/syllabus.html#missed-andor-late-examinations-quizzes-and-graded-exercises",
    "title": "Syllabus",
    "section": "Missed and/or late examinations, quizzes, and graded exercises:",
    "text": "Missed and/or late examinations, quizzes, and graded exercises:\nLate submissions will be penalized 10% per calendar day unless prior arrangements have been made with the instructor."
  },
  {
    "objectID": "syllabus/syllabus.html#grading-scale",
    "href": "syllabus/syllabus.html#grading-scale",
    "title": "Syllabus",
    "section": "Grading Scale",
    "text": "Grading Scale\n94% and above = A\n90% = A- 87% = B+ 84% = B\n80% = B- 77% = C+ 74% = C\n70% = C- 60% = D Below 60% = F"
  },
  {
    "objectID": "slides/01-linear-regression.html#about-me",
    "href": "slides/01-linear-regression.html#about-me",
    "title": "Linear Regression",
    "section": "About me",
    "text": "About me\n\n\n\nüë®‚Äçüéì B.S. & M.S. in Applied Statistics (WSU)\nüë®‚Äçüéì Ph.D.¬†in Applied Matehmatics (AFIT)\nüé¨ Director, Data Science at 84.51Àö\nüë®‚Äçüè´ UC LCB adjunct (~8 years)\nSome R packages üì¶ :\n\npdp (partial dependence plots)\nvip (variable importance plots)\nfastshap (faster SHAP values)\n\nSome books üìö :\n\nHands-On Machine Learning with R\nTree-Based Methods for Statistical Learning",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#ames-housing-data",
    "href": "slides/01-linear-regression.html#ames-housing-data",
    "title": "Linear Regression",
    "section": "Ames housing data",
    "text": "Ames housing data\n\nData describing the sale of individual residential property in Ames, Iowa from 2006 to 2010\nThere are 2930 observations on 81 variables involved in assessing home values:\n\n23 nominal\n23 ordinal\n14 discrete\n20 continuous\n\nPaper: https://jse.amstat.org/v19n3/decock.pdf",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#ames-housing-data-1",
    "href": "slides/01-linear-regression.html#ames-housing-data-1",
    "title": "Linear Regression",
    "section": "Ames housing data",
    "text": "Ames housing data\n\n\nShow R code\names &lt;- AmesHousing::make_ames()  # install.packages(\"AmesHousing\")\names$Sale_Price &lt;- ames$Sale_Price / 10000\nhead(ames)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#ames-housing-data-2",
    "href": "slides/01-linear-regression.html#ames-housing-data-2",
    "title": "Linear Regression",
    "section": "Ames housing data",
    "text": "Ames housing data\nWe‚Äôll focus on a handful of variables:\n\nSale_Price - Sale price of the house / $10K (response variable)\nGr_Liv_Area - Above grade (ground) living area square feet\nOverall_Qual‚Å† - Rates the overall material and finish of the house",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#statistical-relationships",
    "href": "slides/01-linear-regression.html#statistical-relationships",
    "title": "Linear Regression",
    "section": "Statistical relationships",
    "text": "Statistical relationships\n\n\nShow R code\n# Simulate data from different SLR models\nset.seed(101)  # for reproducibility\nx &lt;- seq(from = 0, to = 4, length = 100)\ny &lt;- cbind(\n  1 + x + rnorm(length(x)),  # linear\n  1 + (x - 2)^2 + rnorm(length(x)),  # quadratic\n  1 + log(x + 0.1) + rnorm(length(x), sd = 0.3),  # logarithmic\n  1 + rnorm(length(x))  # no association\n)\n\n# Scatterplot of X vs. each Y in a 2-by-2 grid\npar(mfrow = c(2, 2))\nfor (i in 1:4) {\n  plot(x, y[, i], col = adjustcolor(\"cornflowerblue\", alpha.f = 0.7),\n       pch = 19, xlab = \"X\", ylab = \"Y\")\n}",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#are-x-and-y-correlated",
    "href": "slides/01-linear-regression.html#are-x-and-y-correlated",
    "title": "Linear Regression",
    "section": "Are \\(X\\) and \\(Y\\) correlated?",
    "text": "Are \\(X\\) and \\(Y\\) correlated?\n\n\nShow R code\nplot(x, y[, 3], col = adjustcolor(\"cornflowerblue\", alpha.f = 0.7),\n     pch = 19, xlab = \"X\", ylab = \"Y\")\nr &lt;- round(cor(x, y[, 3]), digits = 3)\nlegend(\"bottomright\", legend = paste0(\"r = \", r), bty = \"n\", inset = 0.01)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#pearsons-correlation-coefficient",
    "href": "slides/01-linear-regression.html#pearsons-correlation-coefficient",
    "title": "Linear Regression",
    "section": "Pearson‚Äôs correlation coefficient",
    "text": "Pearson‚Äôs correlation coefficient\n\nThe (Pearson) correlation between two random variables \\(X\\) and \\(Y\\) is given by\n\n\\[Cor\\left(X, Y\\right) = \\rho = \\frac{Cov\\left(X,Y\\right)}{\\sigma_X\\sigma_Y}\\]\n\nGiven a sample of \\(n\\) pairs \\(\\left\\{\\left(x_i, y_i\\right)\\right\\}_{i=1}^n\\), we estimate \\(\\rho\\) with \\(r = S_{xy} / \\sqrt{S_{xx}S_{yy}}\\), where, for example, \\[S_{xx} = \\sum_{i=1}^n\\left(x_i - \\bar{x}\\right)^2 \\text{ and } S_{xy} = \\sum_{i=1}^n\\left(x_i - \\bar{x}\\right)\\left(y_i - \\bar{y}\\right)\\]",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#pearsons-correlation-coefficient-1",
    "href": "slides/01-linear-regression.html#pearsons-correlation-coefficient-1",
    "title": "Linear Regression",
    "section": "Pearson‚Äôs correlation coefficient",
    "text": "Pearson‚Äôs correlation coefficient\n\nRange: \\(-1 \\le r \\le 1\\)\nWhat does it measure?\n\nPearson‚Äôs correlation coefficient is a unitless measure of the strength of the linear relationship between two variables\n\nOther useful correlation measures also exist:\n\nSpearman‚Äôs rank correlation (or Spearman‚Äôs \\(\\rho\\)) only assumes a monotonic relationship between \\(X\\) and \\(Y\\)\n\nEquivalent to computing \\(r\\) on the ranks of \\(X\\) and \\(Y\\)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#pearsons-correlation-coefficient-2",
    "href": "slides/01-linear-regression.html#pearsons-correlation-coefficient-2",
    "title": "Linear Regression",
    "section": "Pearson‚Äôs correlation coefficient",
    "text": "Pearson‚Äôs correlation coefficient\n\nIt is common to test the hypothesis \\(H_0: \\rho = 0\\) vs.¬†\\(H_1: \\rho \\ne 0\\)\n\nRejecting \\(H_0\\) is only evidence that \\(\\rho\\) is not exactly zero (NOT VERY USEFUL, OR INTERESTING)\nA \\(p\\)-value does not measure the magnitude/strength of the (linear) association\nSample size affects the \\(p\\)-value! üò±",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#are-x-and-y-correlated-1",
    "href": "slides/01-linear-regression.html#are-x-and-y-correlated-1",
    "title": "Linear Regression",
    "section": "Are \\(x\\) and \\(y\\) correlated?",
    "text": "Are \\(x\\) and \\(y\\) correlated?\n\n\nShow R code\nset.seed(1051)  # for reproducibility\nn &lt;- 1000 \nx &lt;- rnorm(n)\ny &lt;- 1 + 0.001*x + rnorm(n)\nplot(x, y, asp = 1, col = adjustcolor(\"black\", alpha.f = 0.3))",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#pearsons-correlation-coefficient-3",
    "href": "slides/01-linear-regression.html#pearsons-correlation-coefficient-3",
    "title": "Linear Regression",
    "section": "Pearson‚Äôs correlation coefficient",
    "text": "Pearson‚Äôs correlation coefficient\n\n\n\n\nShow R code\nset.seed(1050)  # for reproducibility\nn &lt;- 100\nx &lt;- rnorm(n)\ny &lt;- 1 + 0.001*x + rnorm(n)\ncor.test(x, y)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y\nt = -0.0028012, df = 98, p-value = 0.9978\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1966901  0.1961461\nsample estimates:\n          cor \n-0.0002829617 \n\n\n\n\n\nShow R code\nset.seed(1051)  # for reproducibility\nn &lt;- 10000000  # n = ten million\nx &lt;- rnorm(n)\ny &lt;- 1 + 0.001*x + rnorm(n)\ncor.test(x, y)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y\nt = 3.731, df = 9999998, p-value = 0.0001907\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.0005600528 0.0017996412\nsample estimates:\n        cor \n0.001179847 \n\n\n\nThe real question is, are \\(X\\) and \\(Y\\) practically uncorrelated?",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#correlation-is-not-causation",
    "href": "slides/01-linear-regression.html#correlation-is-not-causation",
    "title": "Linear Regression",
    "section": "Correlation is not causation",
    "text": "Correlation is not causation",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#all-models-are-wrong",
    "href": "slides/01-linear-regression.html#all-models-are-wrong",
    "title": "Linear Regression",
    "section": "All models are wrong!",
    "text": "All models are wrong!\n\n\n\nAlso, see this talk by my old adviser, Thad Tarpey: ‚ÄúAll Models are Right‚Ä¶ most are useless.‚Äù",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#pearsons-correlation-vs.-slr",
    "href": "slides/01-linear-regression.html#pearsons-correlation-vs.-slr",
    "title": "Linear Regression",
    "section": "Pearson‚Äôs correlation vs.¬†SLR",
    "text": "Pearson‚Äôs correlation vs.¬†SLR\n\nThere‚Äôs a formal relationship between Pearson‚Äôs correlation coefficient (\\(\\rho\\)) and the SLR model\n‚ÄúSimple‚Äù linear relationships can be described by an intercept and slope:\n\n\\(y = mx + b\\) (algebra)\n\\(\\mu = \\beta_0 + \\beta_1x\\) (statistics)\n\n‚ÄúSimple‚Äù here means two variables, \\(x\\) and \\(y\\) (but \\(y\\) can be linearly related to several variables)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#example-ames-housing",
    "href": "slides/01-linear-regression.html#example-ames-housing",
    "title": "Linear Regression",
    "section": "Example: Ames housing",
    "text": "Example: Ames housing\nCheck out this paper for useful background on the Ames housing data in regression",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#example-ames-housing-1",
    "href": "slides/01-linear-regression.html#example-ames-housing-1",
    "title": "Linear Regression",
    "section": "Example: Ames housing",
    "text": "Example: Ames housing\n\n\nShow R code\nhead(cbind(ames$Sale_Price, ames$Gr_Liv_Area))\n\n\n      [,1] [,2]\n[1,] 21.50 1656\n[2,] 10.50  896\n[3,] 17.20 1329\n[4,] 24.40 2110\n[5,] 18.99 1629\n[6,] 19.55 1604\n\n\nShow R code\ncor.test(ames$Sale_Price, y = ames$Gr_Liv_Area)  # see ?cor.test\n\n\n\n    Pearson's product-moment correlation\n\ndata:  ames$Sale_Price and ames$Gr_Liv_Area\nt = 54.061, df = 2928, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6881814 0.7244502\nsample estimates:\n      cor \n0.7067799 \n\n\nThis doesn‚Äôt tell us much about the nature of the linear relationship between Gr_Liv_Area and Sale_Price",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#statistical-relationships-1",
    "href": "slides/01-linear-regression.html#statistical-relationships-1",
    "title": "Linear Regression",
    "section": "Statistical relationships",
    "text": "Statistical relationships\n\n\nShow R code\nlibrary(ggplot2) \n\np1 &lt;- ggplot(investr::crystal, aes(x = time, y = weight)) +\n  geom_point() +\n  labs(x = \"Time (hours)\", \n       y = \"Weight (grams)\", \n       title = \"Crystal weight data\")\np2 &lt;- ggplot(investr::arsenic, aes(x = actual, y = measured)) +\n  geom_point() +\n  labs(x = \"True amount of arsenic\", \n       y = \"Measured amount of arsenic\",\n       title = \"Arsenic concentration data\")\ngridExtra::grid.arrange(p1, p2, nrow = 1)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#examples-of-statistical-relationships",
    "href": "slides/01-linear-regression.html#examples-of-statistical-relationships",
    "title": "Linear Regression",
    "section": "Examples of statistical relationships",
    "text": "Examples of statistical relationships\n\nSimple linear regression: \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)\nMultiple linear regression: \\(Y = \\beta_0 + \\sum_{i=1}^p \\beta_p X_p + \\epsilon\\)\nPolynomial regression: \\(Y = \\beta_0 + \\sum_{i=1}^p \\beta_p X^p + \\epsilon\\)\nLogarithmic: \\(Y = \\beta_0 + \\beta_1 \\log\\left(X + 0.1\\right) + \\epsilon\\)\nNonlinear regression: \\(Y = \\frac{\\beta_1 X}{\\left(\\beta_2 + X\\right)} + \\epsilon\\)\nMultiplicative: \\(Y = \\beta X \\epsilon\\)\n\n\\(\\log\\left(Y\\right) = \\alpha + \\log\\left(X\\right) + \\log\\left(\\epsilon\\right)\\)\n\n\nAssuming \\(\\epsilon \\sim D\\left(\\mu, \\sigma\\right)\\)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#simple-linear-regression-slr-1",
    "href": "slides/01-linear-regression.html#simple-linear-regression-slr-1",
    "title": "Linear Regression",
    "section": "Simple linear regression (SLR)",
    "text": "Simple linear regression (SLR)\n\nData: \\(\\left\\{\\left(X_i, Y_i\\right)\\right\\}_{i=1}^n\\)\nModel: \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\)\n\n\\(Y_i\\) is a continuous response\n\\(X_i\\) is a continuous predictor\n\\(\\beta_0\\) is the intercept of the regression line (also called the bias term)\n\\(\\beta_1\\) is the slope of the regression line\n\\(\\epsilon_i \\stackrel{iid}{\\sim} D\\left(0, \\sigma^2\\right)\\)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#assumptions-about-the-errors-epsilon_i",
    "href": "slides/01-linear-regression.html#assumptions-about-the-errors-epsilon_i",
    "title": "Linear Regression",
    "section": "Assumptions about the errors \\(\\epsilon_i\\)",
    "text": "Assumptions about the errors \\(\\epsilon_i\\)\nFor \\(i\\) and \\(j\\) in \\(\\left\\{1, 2, \\dots, n\\right\\}\\) and \\(i \\ne j\\)\n\n\\(\\quad E\\left(\\epsilon_i\\right) = 0\\)\n\\(\\quad Var\\left(\\epsilon_i\\right) = \\sigma^2\\) (homoscedacticity üò±)\n\\(\\quad Cov\\left(\\epsilon_i, \\epsilon_j\\right) = 0\\) (independence)\n\nAssumptions 1‚Äì3 can be summarized as \\(\\epsilon_i \\stackrel{iid}{\\sim} D\\left(0, \\sigma^2\\right)\\), where \\(iid\\) refers to independent and identically distributed.",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#properties-of-slr",
    "href": "slides/01-linear-regression.html#properties-of-slr",
    "title": "Linear Regression",
    "section": "Properties of SLR",
    "text": "Properties of SLR\n\nSimple linear regression: \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\)\n\nAssumes the model is linear in the regression coefficients \\(\\beta_0\\) and \\(\\beta_1\\)\n\nThe error term is a random variable; hence, \\(Y_i\\) is also a random variable (Why? ü§î)\n\nWhat is \\(E\\left(Y_i|X_i\\right)\\) and \\(Var\\left(Y_i|X_i\\right)\\)?\n\n\\(Cor\\left(Y_i, Y_j\\right) = 0\\) \\(\\forall i \\ne j\\) (Why? ü§î)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#inference-for-a-single-variable",
    "href": "slides/01-linear-regression.html#inference-for-a-single-variable",
    "title": "Linear Regression",
    "section": "Inference for a single variable",
    "text": "Inference for a single variable\n\nIs it useful to test the hypothesis that Sale_Price = $160K?\n\n\n\nNo! Because Sale_Price is not a constant, but a random variable whose value varies from home to home (and year to year, etc.)\n\n\n\n\nWe‚Äôre more interested in questions such as:\n\nWhat is the chance that Sale_Price &gt; $160K? (above median sale price)\nWhat is the chance that Sale_Price &lt; $105K? (lowest decile)\nWhat is the chance that $129,500 &lt; Sale_Price &lt; $213,500? (within IQR)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#distribution-of-sale_price",
    "href": "slides/01-linear-regression.html#distribution-of-sale_price",
    "title": "Linear Regression",
    "section": "Distribution of Sale_Price",
    "text": "Distribution of Sale_Price\nCan look at historgram and empirical CDF:\n\n\nShow R code\npar(mfrow = c(1, 2), las = 1)\nhist(ames$Sale_Price, br = 50, xlab = \"Sale price ($)\", freq = FALSE, main = \"\")\nplot(ecdf(ames$Sale_Price), xlab = \"Sale price ($)\", main = \"\",\n     col = adjustcolor(1, alpha.f = 0.1))",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#distribution-of-sale_price-1",
    "href": "slides/01-linear-regression.html#distribution-of-sale_price-1",
    "title": "Linear Regression",
    "section": "Distribution of Sale_Price",
    "text": "Distribution of Sale_Price\n\nHistograms and ECDFs are nonparammetric in nature\nA simple parametric approach might assume a particular distribution for Sale_Price\nFor instance, we might assume Sale_Price \\(\\sim N\\left(\\mu, \\sigma^2\\right)\\)\nHow can we estimate \\(\\mu\\) and \\(\\sigma^2\\)?\n\n\n\n\nShow R code\n# Maximum likelihiid estimates\nc(\"sample mean\" = mean(ames$Sale_Price), \"sample stdev\" = sd(ames$Sale_Price))\n\n\n sample mean sample stdev \n   18.079606     7.988669 \n\n\n\n\n\nIs the normal distribution a reasonable assumption here?",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#normal-qq-plot",
    "href": "slides/01-linear-regression.html#normal-qq-plot",
    "title": "Linear Regression",
    "section": "Normal QQ plot",
    "text": "Normal QQ plot\n\nNormal quantile-quantile (Q-Q) plot* can be used to asses the ‚Äúnormalityness‚Äù of a set of observations\nQ-Q plots can, in general, be used to compare data with any distribution!\n\n\n\nShow R code\nqqnorm(ames$Sale_Price, col = 2, las = 1)\nqqline(ames$Sale_Price)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#normality-tests",
    "href": "slides/01-linear-regression.html#normality-tests",
    "title": "Linear Regression",
    "section": "Normality tests ü§Æ",
    "text": "Normality tests ü§Æ\n\nNormality tests, like the Shapiro-Wilk1 and Anderson-Darling tests, can also be used to assess normality\n\nI STRONGLY ADVISE AGAINST USING THEM!\n\nNo data is normally distributes, what we care about is whether enough a normal approximation is close enough!\nNormality tests provide a \\(p\\)-value, which only gives a yes/no conclusion\n\nIn R, see ?shapiro.test for details.",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#normality-tests-1",
    "href": "slides/01-linear-regression.html#normality-tests-1",
    "title": "Linear Regression",
    "section": "Normality tests ü§Æ",
    "text": "Normality tests ü§Æ\nRecall that \\(p\\)-values are a function of sample size!\n\n\nShow R code\n# Shapiro-Wilk test results vs. sample size\nset.seed(101)  # for reproducibility\nx &lt;- replicate(100, c(\n  shapiro.test(rt(10, df = 40))$p.value,\n  shapiro.test(rt(100, df = 40))$p.value,\n  shapiro.test(rt(500, df = 40))$p.value,\n  shapiro.test(rt(1000, df = 40))$p.value,\n  shapiro.test(rt(2500, df = 40))$p.value,\n  shapiro.test(rt(5000, df = 40))$p.value\n))\nrownames(x) &lt;- paste0(\"n=\", c(10, 100, 500, 1000, 2500, 5000))\nrowMeans(x &lt; 0.05)\n\n\n  n=10  n=100  n=500 n=1000 n=2500 n=5000 \n  0.01   0.08   0.11   0.22   0.24   0.39",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#normality-tests-2",
    "href": "slides/01-linear-regression.html#normality-tests-2",
    "title": "Linear Regression",
    "section": "Normality tests ü§Æ",
    "text": "Normality tests ü§Æ\n\n\nAre these two distributions significantly different?\nAre these two distributions practically different?",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#what-can-we-do-if-the-normality-assumption-isnt-justified",
    "href": "slides/01-linear-regression.html#what-can-we-do-if-the-normality-assumption-isnt-justified",
    "title": "Linear Regression",
    "section": "What can we do if the normality assumption isn‚Äôt justified?",
    "text": "What can we do if the normality assumption isn‚Äôt justified?\n\nTry transformations\n\nLogarithm or square root for positive data\nPower transformation (like the well-known Box-Cox procedure)\n\nTry a more appropriate distribution (e.g., Poisson or gamma distribution)\nTry more advanced approaches, like the nonparametric bootstrapping!",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#modeling-the-mean-response",
    "href": "slides/01-linear-regression.html#modeling-the-mean-response",
    "title": "Linear Regression",
    "section": "Modeling the mean response",
    "text": "Modeling the mean response\n\nAssume that \\(Y \\sim N\\left(\\mu, \\sigma^2\\right)\\), where\n\n\\[\\mu = \\mu\\left(x\\right) = \\beta_0 + \\beta_1 x = E\\left(Y|x\\right)\\]\n\n\nIn other words: \\(Y \\sim N\\left(\\beta_0 + \\beta_1 x, \\sigma^2\\right)\\)\nAlternatively, we could write \\(Y = \\beta_0 + \\beta_1 x + \\epsilon\\), where \\(\\epsilon \\sim N\\left(0, \\sigma^2\\right)\\)\nThis is called the simple linear regression (SLR) model",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#the-idea-behind-slr",
    "href": "slides/01-linear-regression.html#the-idea-behind-slr",
    "title": "Linear Regression",
    "section": "The idea behind SLR",
    "text": "The idea behind SLR\n\nImage source",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#arsenic-experiment-example",
    "href": "slides/01-linear-regression.html#arsenic-experiment-example",
    "title": "Linear Regression",
    "section": "Arsenic experiment example",
    "text": "Arsenic experiment example\n\n\nShow R code\nplot(investr::arsenic, las = 1)  # see ?investr::arsenic for details",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#is-linear-regression-reasonable-here",
    "href": "slides/01-linear-regression.html#is-linear-regression-reasonable-here",
    "title": "Linear Regression",
    "section": "Is linear regression reasonable here?",
    "text": "Is linear regression reasonable here?\n\n\nShow R code\nx &lt;- rep(c(1:10 / 10, 1.5, 2), each = 30)\ny &lt;- 1 + 2*x^2 + rnorm(length(x), sd = 1)\npar(mfrow = c(1, 2), las = 1)\nplot(x, y, col = \"dodgerblue2\")\n#abline(lm(y ~ x), lwd = 2)\nhist(y, main = \"\", col = \"dodgerblue2\", border = \"white\")",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#is-linear-regression-reasonable-here-1",
    "href": "slides/01-linear-regression.html#is-linear-regression-reasonable-here-1",
    "title": "Linear Regression",
    "section": "Is linear regression reasonable here?",
    "text": "Is linear regression reasonable here?\n\n\nShow R code\nfit &lt;- lm(y ~ x)\nres &lt;- residuals(fit)\nqqnorm(res, las = 1)\nqqline(res, col = 2)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#least-squares-ls-estimation",
    "href": "slides/01-linear-regression.html#least-squares-ls-estimation",
    "title": "Linear Regression",
    "section": "Least squares (LS) estimation",
    "text": "Least squares (LS) estimation\nIdea of LS is to find \\(\\beta_0\\) and \\(\\beta_1\\) so that the sum of squared residuals (i.e., errors) is minimized: \\[SSE = \\sum_{i=1}^n\\left(y_i - \\beta_0 - \\beta_1x_i\\right)^2\\]\n\nPretty straightforward optimization problem that leads to closed-form solution (but no point in memorizing the formulas!)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#concept-of-ls-estimation",
    "href": "slides/01-linear-regression.html#concept-of-ls-estimation",
    "title": "Linear Regression",
    "section": "Concept of LS estimation",
    "text": "Concept of LS estimation\n\nImage source",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#sale_price-and-gr_liv_area",
    "href": "slides/01-linear-regression.html#sale_price-and-gr_liv_area",
    "title": "Linear Regression",
    "section": "Sale_Price and Gr_Liv_Area",
    "text": "Sale_Price and Gr_Liv_Area\n\n\nShow R code\nplot(Sale_Price ~ Gr_Liv_Area, data = ames, las = 1,\n     col = adjustcolor(1, alpha.f = 0.3))",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#slr-fit",
    "href": "slides/01-linear-regression.html#slr-fit",
    "title": "Linear Regression",
    "section": "SLR fit",
    "text": "SLR fit\n\n\nShow R code\nsummary(fit &lt;- lm(Sale_Price ~ Gr_Liv_Area, data = ames))\n\n\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area, data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.347  -3.022  -0.197   2.273  33.432 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.3289634  0.3269703   4.064 4.94e-05 ***\nGr_Liv_Area 0.0111694  0.0002066  54.061  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.652 on 2928 degrees of freedom\nMultiple R-squared:  0.4995,    Adjusted R-squared:  0.4994 \nF-statistic:  2923 on 1 and 2928 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#is-this-a-good-fit",
    "href": "slides/01-linear-regression.html#is-this-a-good-fit",
    "title": "Linear Regression",
    "section": "Is this a good fit?",
    "text": "Is this a good fit?\n\n\nShow R code\nplot(Sale_Price ~ Gr_Liv_Area, data = ames,\n     col = adjustcolor(1, alpha.f = 0.3))\nabline(fit, lwd = 2, col = 2)  # add SLR fit\n\n\n\nWhich assumptions seem violated to some degree?",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#residual-diagnostics",
    "href": "slides/01-linear-regression.html#residual-diagnostics",
    "title": "Linear Regression",
    "section": "Residual diagnostics",
    "text": "Residual diagnostics\n\nThe standard residual is defined as \\(e_i = y_i - \\hat{y}_i\\) and can be regarded as the observed error\nThe residuals hold a lot of properties that make them useful for diagnosing potential issues with the model (e.g., suggesting potential transformations to try)\nMany other kinds of residuals exist for different purposes (e.g., standardized, studentized, jackknife or PRESS residuals, etc.)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#properties-of-the-residuals",
    "href": "slides/01-linear-regression.html#properties-of-the-residuals",
    "title": "Linear Regression",
    "section": "Properties of the residuals",
    "text": "Properties of the residuals\n\n\\(\\sum_{i=1}^n e_i = 0\\)\n\\(\\sum_{i=1}^n e_i^2\\) is a minimum\n\\(\\sum_{i=1}^n X_ie_i = 0\\)\n\\(\\sum_{i=1}^n \\hat{Y}_ie_i = 0\\)\nThe LS regression line passes through the point \\(\\left(\\bar{X}, \\bar{Y}\\right)\\) (i.e., the center of the training data)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#what-can-residual-plots-tell-us",
    "href": "slides/01-linear-regression.html#what-can-residual-plots-tell-us",
    "title": "Linear Regression",
    "section": "What can residual plots tell us?",
    "text": "What can residual plots tell us?\n\nResiduals vs.¬†predictor values (checking non-linearity).\nResiduals vs.¬†fitted values (non-constant variance, non-linearity, and outliers)\nResiduals vs.¬†time or another sequence (checking independence)\nResiduals vs.¬†omitted predictor values (missing potentially important predictors)\nNormal QQ plot of residuals (non-normality).\nAnd much, much more!",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#sale_price-gr_liv_area",
    "href": "slides/01-linear-regression.html#sale_price-gr_liv_area",
    "title": "Linear Regression",
    "section": "Sale_Price ~ Gr_Liv_Area",
    "text": "Sale_Price ~ Gr_Liv_Area\nResidual analysis:\n\n\nShow R code\npar(mfrow = c(2, 3), las = 1)\nplot(fit, which = 1:6)\n\n\n\nWhat assumptions appear to be in violation?",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#lets-try-a-log-transformation",
    "href": "slides/01-linear-regression.html#lets-try-a-log-transformation",
    "title": "Linear Regression",
    "section": "Let‚Äôs try a log transformation",
    "text": "Let‚Äôs try a log transformation\n\n\nShow R code\nfit2 &lt;- lm(log(Sale_Price) ~ Gr_Liv_Area, data = ames)\nplot(log(Sale_Price) ~ Gr_Liv_Area, data = ames,\n     col = adjustcolor(1, alpha.f = 0.3))\nabline(fit2, lwd = 2, col = 2)  # add SLR fit",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#lets-try-a-log-transformation-1",
    "href": "slides/01-linear-regression.html#lets-try-a-log-transformation-1",
    "title": "Linear Regression",
    "section": "Let‚Äôs try a log transformation",
    "text": "Let‚Äôs try a log transformation\n\n\nShow R code\nsummary(fit2)\n\n\n\nCall:\nlm(formula = log(Sale_Price) ~ Gr_Liv_Area, data = ames)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.36215 -0.15145  0.03091  0.16583  0.90332 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.9692014  0.0169355  116.28   &lt;2e-16 ***\nGr_Liv_Area 0.0005611  0.0000107   52.43   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2928 on 2928 degrees of freedom\nMultiple R-squared:  0.4842,    Adjusted R-squared:  0.484 \nF-statistic:  2749 on 1 and 2928 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#logsale_price-gr_liv_area",
    "href": "slides/01-linear-regression.html#logsale_price-gr_liv_area",
    "title": "Linear Regression",
    "section": "log(Sale_Price) ~ Gr_Liv_Area",
    "text": "log(Sale_Price) ~ Gr_Liv_Area\nResidual analysis:\n\n\nShow R code\npar(mfrow = c(2, 3), las = 1)\nplot(fit2, which = 1:6)\n\n\n\nAny better?",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#mlr-in-a-nutshell",
    "href": "slides/01-linear-regression.html#mlr-in-a-nutshell",
    "title": "Linear Regression",
    "section": "MLR in a nutshell ü•ú",
    "text": "MLR in a nutshell ü•ú\n\nThe (normal) multiple linear regression model assumes \\(Y \\sim N\\left(\\mu\\left(\\boldsymbol{x}\\right), \\sigma^2\\right)\\), where \\[\\mu\\left(\\boldsymbol{x}\\right) = \\beta_0 + \\sum_{i=1}^p \\beta_i x_i = \\boldsymbol{x}^\\top\\boldsymbol{\\beta}\\]\nLS estimation still provides unbiased estimate of \\(\\boldsymbol{\\beta} = \\left(\\beta_0, \\beta_1, \\dots, \\beta_p\\right)^\\top\\): \\(\\hat{\\boldsymbol{\\beta}} = \\left(\\boldsymbol{X}^\\top\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y}\\)\nFitted values: \\(\\hat{\\boldsymbol{y}} = \\boldsymbol{X}\\left(\\boldsymbol{X}^\\top\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y} = \\boldsymbol{H}\\boldsymbol{y}\\)\n\\(\\boldsymbol{H}\\) is the well-known ‚Äúhat matrix‚Äù",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#polynomial-regression",
    "href": "slides/01-linear-regression.html#polynomial-regression",
    "title": "Linear Regression",
    "section": "Polynomial regression",
    "text": "Polynomial regression\n\nPolynomial regression is just a special case of the MLR model\nA second order model in a single predictor: \\[Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon\\]\nA k-th order model in a single predictor (Typically \\(k \\le 3\\)): \\[Y = \\beta_0 + \\sum_{j=1}^k\\beta_j X^j + \\epsilon\\]",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#example-paper-strength-data",
    "href": "slides/01-linear-regression.html#example-paper-strength-data",
    "title": "Linear Regression",
    "section": "Example: paper strength data",
    "text": "Example: paper strength data\nData concerning the strength of kraft paper and the percentage of hardwood in the batch of pulp from which the paper was produced.\n\n\nShow R code\n# Load the hardwood conentration data\nurl &lt;- \"https://bgreenwell.github.io/uc-bana7052/data/hardwood.csv\"\nhardwood &lt;- read.csv(url)\n\n# Print first few observations\nhead(hardwood)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#example-paper-strength-data-1",
    "href": "slides/01-linear-regression.html#example-paper-strength-data-1",
    "title": "Linear Regression",
    "section": "Example: paper strength data",
    "text": "Example: paper strength data\n\n\nShow R code\nplot(hardwood, pch = 19)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#example-paper-strength-data-2",
    "href": "slides/01-linear-regression.html#example-paper-strength-data-2",
    "title": "Linear Regression",
    "section": "Example: paper strength data",
    "text": "Example: paper strength data\n\n\nShow R code\nfit1 &lt;- lm(TsStr ~ HwdCon, data = hardwood)\ninvestr::plotFit(fit1, pch = 19, col.fit = \"red2\")",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#example-paper-strength-data-3",
    "href": "slides/01-linear-regression.html#example-paper-strength-data-3",
    "title": "Linear Regression",
    "section": "Example: paper strength data",
    "text": "Example: paper strength data\n\n\nShow R code\npar(mfrow = c(1, 2), las = 1)\n\n# Plot residuals vs HwdCon (i.e., X)\npar(mfrow = c(1, 2))\nplot(x = hardwood$HwdCon, y = residuals(fit1), xlab = \"HwdCon\",\n     ylab = \"Residuals\", main = \"Residuals vs HwdCon\")\nabline(h = 0, lty = \"dotted\")\nplot(fit1, which = 1, caption = \"\", main = \"Residuals vs Fitted\")",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#example-paper-strength-data-4",
    "href": "slides/01-linear-regression.html#example-paper-strength-data-4",
    "title": "Linear Regression",
    "section": "Example: paper strength data",
    "text": "Example: paper strength data\n\n\nShow R code\nfit2 &lt;- lm(TsStr ~ HwdCon + I(HwdCon^2), data = hardwood)\ninvestr::plotFit(fit2, pch = 19, col.fit = \"red2\")",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#example-paper-strength-data-5",
    "href": "slides/01-linear-regression.html#example-paper-strength-data-5",
    "title": "Linear Regression",
    "section": "Example: paper strength data",
    "text": "Example: paper strength data\n\n\nShow R code\npar(mfrow = c(2, 3), las = 1)\nfor (i in 1:6) {  # try higher-order models\n  fit &lt;- lm(TsStr ~ poly(HwdCon, degree = i), data = hardwood)\n  investr::plotFit(fit, main = paste(\"Degree =\", i))\n}",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#example-paper-strength-data-6",
    "href": "slides/01-linear-regression.html#example-paper-strength-data-6",
    "title": "Linear Regression",
    "section": "Example: paper strength data",
    "text": "Example: paper strength data\n\n\nShow R code\npar(mfrow = c(2, 3))\nfor (i in 1:6) {  # try higher-order models\n  fit &lt;- lm(TsStr ~poly(HwdCon, degree = i), data = hardwood)\n  investr::plotFit(fit, main = paste(\"Degree =\", i), \n                   interval = \"confidence\", shade = TRUE,\n                   xlim = c(-10, 30))\n}",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#polynomial-regression-1",
    "href": "slides/01-linear-regression.html#polynomial-regression-1",
    "title": "Linear Regression",
    "section": "Polynomial regression",
    "text": "Polynomial regression\nSome cautions ‚ö†Ô∏è\nKeep the order of the model as low as possible\n\nAvoid interpolating the data or over fitting\nUse the simplest model possible to explain the data, but no simpler (parsimony)\nAn \\(n - 1\\) order model can perfectly fit a data set with \\(n\\) observations (Why is this bad ü§î)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#polynomial-regression-2",
    "href": "slides/01-linear-regression.html#polynomial-regression-2",
    "title": "Linear Regression",
    "section": "Polynomial regression",
    "text": "Polynomial regression\nTwo model-building strategies:\n\nFit the lowest order polynomial possible and build up (forward selection)\nFit the highest order polynomial of interest, and remove terms one at a time (backward elimination)\n\nThese two procedures may not result in the same final model\nIncreasing the order can result in an ill-conditioned \\(\\boldsymbol{X}^\\top\\boldsymbol{X}\\) and multicollinearity",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#categorical-variables",
    "href": "slides/01-linear-regression.html#categorical-variables",
    "title": "Linear Regression",
    "section": "Categorical variables",
    "text": "Categorical variables\n\nCategorical variables can be handled in a number of ways in linear models, including\n\nDummy encoding (nominal)\nOrthogonal polynomials (ordinal)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#categorical-variables-1",
    "href": "slides/01-linear-regression.html#categorical-variables-1",
    "title": "Linear Regression",
    "section": "Categorical variables",
    "text": "Categorical variables\nLet‚Äôs look at two (nominal) categorical variables:\n\n\nShow R code\ntable(ames$Central_Air)\n\n\n\n   N    Y \n 196 2734 \n\n\nShow R code\ntable(ames$Paved_Drive)\n\n\n\n     Dirt_Gravel Partial_Pavement            Paved \n             216               62             2652",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#categorical-variables-2",
    "href": "slides/01-linear-regression.html#categorical-variables-2",
    "title": "Linear Regression",
    "section": "Categorical variables",
    "text": "Categorical variables\n\n\nShow R code\nplot(log(Sale_Price) ~ Central_Air, data = ames, las = 1, col = c(2, 3))",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#categorical-variables-3",
    "href": "slides/01-linear-regression.html#categorical-variables-3",
    "title": "Linear Regression",
    "section": "Categorical variables",
    "text": "Categorical variables\n\n\nShow R code\nplot(log(Sale_Price) ~ Paved_Drive, data = ames, las = 1, col = c(2, 3, 4))\n\n\n\nIf one of these homes downgraded from a paved driveway to a gravel driveway, would that cause the sale price to decrease? (Think very carefully here!)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#categorical-variables-4",
    "href": "slides/01-linear-regression.html#categorical-variables-4",
    "title": "Linear Regression",
    "section": "Categorical variables",
    "text": "Categorical variables\nR dummy encodes nominal factors by default:\n\n\nShow R code\nfit3 &lt;- lm(log(Sale_Price) ~ Gr_Liv_Area + Central_Air + Paved_Drive, \n           data = ames)\nsummary(fit3)\n\n\n\nCall:\nlm(formula = log(Sale_Price) ~ Gr_Liv_Area + Central_Air + Paved_Drive, \n    data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2344 -0.1345  0.0073  0.1453  0.8502 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 1.428e+00  2.418e-02  59.059  &lt; 2e-16 ***\nGr_Liv_Area                 5.183e-04  9.520e-06  54.440  &lt; 2e-16 ***\nCentral_AirY                3.464e-01  2.068e-02  16.747  &lt; 2e-16 ***\nPaved_DrivePartial_Pavement 1.334e-01  3.733e-02   3.574 0.000358 ***\nPaved_DrivePaved            3.085e-01  1.975e-02  15.618  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2574 on 2925 degrees of freedom\nMultiple R-squared:  0.6018,    Adjusted R-squared:  0.6013 \nF-statistic:  1105 on 4 and 2925 DF,  p-value: &lt; 2.2e-16\n\n\nHow do you interpret the coefficients here?",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#coefficient-of-determination",
    "href": "slides/01-linear-regression.html#coefficient-of-determination",
    "title": "Linear Regression",
    "section": "Coefficient of determination",
    "text": "Coefficient of determination\nThe coefficient of determination is the proportion of the variance in the dependent variable that is predictable from the independent variables in the model.\n\n\nR-squared (\\(R^2\\))\n\n\\(R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\\)\n\\(R^2\\) will always increase as more terms are added to the model!\n\n\nAdjusted R-squared (\\(R_{adj}^2\\))\n\n\n\\(R_{adj}^2 = 1 - \\frac{MSE}{SST/\\left(n - 1\\right)}\\)\nPenalizes \\(R^2\\) if there are ‚Äútoo many‚Äù terms in the model\n\\(R_{adj}^2\\) and \\(MSE\\) provide equivalent information",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#variablemodel-selection",
    "href": "slides/01-linear-regression.html#variablemodel-selection",
    "title": "Linear Regression",
    "section": "Variable/model selection",
    "text": "Variable/model selection\n\nVariable/model selection is a very noisy problem! (Often best to avoid, if feasible)\nAsk the domain experts about important variables (don‚Äôt just rely on algorithms)\nP(selecting the ‚Äúright‚Äù variables) = 0 (source)\n‚ÄúAll models subsets of variables are wrong, but some are useful!‚Äù\nIn regression settings, regularization (e.g., ridge regression and the LASSO) is often more useful! (Think about the impact of multicollinearity on variable selection)",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#data-splitting",
    "href": "slides/01-linear-regression.html#data-splitting",
    "title": "Linear Regression",
    "section": "Data splitting",
    "text": "Data splitting\n\nIf prediction is the goal (e.g., compared to inference and hypothesis testing), the model performance should be assessed rigorously\nData splitting techniques ae key and the type of data splitting to use often depends on the situation (e.g., cross-sectional vs.¬†time-series data)\nIn simplest terms, random split the data into two parts: A and B. Build a model on part A and see how well it does with predicting the response in part B.\nLeakage is a huge concern here, so data splitting ALWAYS has to be done carefully!",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#data-splitting-k-fold-cross-validation",
    "href": "slides/01-linear-regression.html#data-splitting-k-fold-cross-validation",
    "title": "Linear Regression",
    "section": "Data splitting: \\(k\\)-fold cross-validation",
    "text": "Data splitting: \\(k\\)-fold cross-validation\n\nThe PRESS statistic in linear regression is a special case (\\(k = n\\)) we get for free!",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#questions",
    "href": "slides/01-linear-regression.html#questions",
    "title": "Linear Regression",
    "section": "Questions?",
    "text": "Questions?",
    "crumbs": [
      "Slides",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#logistic-regression",
    "href": "slides/04-logistic-regression-3.html#logistic-regression",
    "title": "Logistic Regression (Part III)",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nIn logistic regression, we use the logit: \\[\n\\text{logit}\\left(p\\right) = \\boldsymbol{x}^\\top\\boldsymbol{\\beta} = \\eta\n\\] which results in \\[\np = \\left[1 + \\exp\\left(-\\eta\\right)\\right]^{-1} = F\\left(\\eta\\right)\n\\]\nTechnically, \\(F\\) can be any monotonic function that maps \\(\\eta\\) to \\(\\left[0, 1\\right]\\) (e.g., any CDF will work)\n\\(F^{-1}\\) is called the link function",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#the-probit-model",
    "href": "slides/04-logistic-regression-3.html#the-probit-model",
    "title": "Logistic Regression (Part III)",
    "section": "The probit model",
    "text": "The probit model\n\nThe probit model uses \\(F\\left(\\eta\\right) = \\Phi\\left(\\eta\\right)\\) (i.e., the CDF of a standard normal distribution) \\[\nP\\left(Y = 1 | \\boldsymbol{x}\\right) = \\Phi\\left(\\beta_0 + \\beta_1x_1 + \\cdots\\right) = \\Phi\\left(\\boldsymbol{x}^\\top\\boldsymbol{\\beta}\\right)\n\\]\n\\(F^{-1} = \\Phi^{-1}\\) is called the probit link, which yields \\[\n\\text{probit}\\left(p\\right) = \\Phi^{-1}\\left(p\\right) = \\boldsymbol{x}^\\top\\boldsymbol{\\beta}\n\\]\nThe term probit is short for probability unit\nProposed in Bliss (1934) and still common for modeling dose-response relationships",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#dobsons-beetle-data",
    "href": "slides/04-logistic-regression-3.html#dobsons-beetle-data",
    "title": "Logistic Regression (Part III)",
    "section": "Dobson‚Äôs beetle data",
    "text": "Dobson‚Äôs beetle data\nThe data give the number of flour beetles killed after five hour exposure to the insecticide carbon disulphide at eight different concentrations.\n\n\nShow R code\nbeetle &lt;- investr::beetle\nfit &lt;- glm(cbind(y, n-y) ~ ldose, data = beetle, \n           family = binomial(link = \"probit\"))\ninvestr::plotFit(\n  fit, pch = 19, cex = 1.2, lwd = 2, \n  xlab = \"Log dose of carbon disulphide\", ylab = \"Proportion killed\",\n  interval = \"confidence\", shade = TRUE, col.conf = \"lightskyblue\"\n)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#other-link-functions",
    "href": "slides/04-logistic-regression-3.html#other-link-functions",
    "title": "Logistic Regression (Part III)",
    "section": "Other link functions",
    "text": "Other link functions\nCommon link function for binary regression include:\n\nLogit (most common and the default an most software)\nProbit (next most common)\nLog-log\nComplimentary log-log\nCauchit",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#application-to-wcgs-data-set",
    "href": "slides/04-logistic-regression-3.html#application-to-wcgs-data-set",
    "title": "Logistic Regression (Part III)",
    "section": "Application to wcgs data set",
    "text": "Application to wcgs data set\nComparing coefficients from different link functions:\n\n\nShow R code\nwcgs &lt;- na.omit(subset(faraway::wcgs, select = -c(typechd, timechd, behave)))\n\n# Fit binary GLMs with different link functions\nfit.logit &lt;- glm(chd ~ ., data = wcgs, family = binomial(link = \"logit\"))\nfit.probit &lt;- glm(chd ~ ., data = wcgs, family = binomial(link = \"probit\"))\nfit.cloglog &lt;- glm(chd ~ ., data = wcgs, family = binomial(link = \"cloglog\"))\nfit.cauchit &lt;- glm(chd ~ ., data = wcgs, family = binomial(link = \"cauchit\"))\n\n# Compare coefficients\ncoefs &lt;- cbind(\n  \"logit\" = coef(fit.logit),\n  \"probit\" = coef(fit.probit),\n  \"cloglog\" = coef(fit.cloglog),\n  \"cauchit\" = coef(fit.cauchit)\n)\nround(coefs, digits = 3)\n\n\n               logit probit cloglog cauchit\n(Intercept)  -12.246 -6.452 -11.377 -24.086\nage            0.062  0.031   0.057   0.115\nheight         0.007  0.003   0.007   0.053\nweight         0.009  0.005   0.007   0.003\nsdp            0.018  0.010   0.017   0.042\ndbp           -0.001 -0.001  -0.001  -0.001\nchol           0.011  0.006   0.010   0.020\ncigs           0.021  0.011   0.019   0.043\ndibepB        -0.658 -0.341  -0.604  -1.320\narcuspresent   0.211  0.101   0.206   0.711",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#application-to-wcgs-data-set-1",
    "href": "slides/04-logistic-regression-3.html#application-to-wcgs-data-set-1",
    "title": "Logistic Regression (Part III)",
    "section": "Application to wcgs data set",
    "text": "Application to wcgs data set\nComparing fitted values from different link functions:\n\n\nShow R code\n# Compare fitted values (i.e., predicted probabilities)\npreds &lt;- cbind(\n  \"logit\" = fitted(fit.logit),\n  \"probit\" = fitted(fit.probit),\n  \"cloglog\" = fitted(fit.cloglog),\n  \"cauchit\" = fitted(fit.cauchit)\n)\nhead(round(preds, digits = 3))\n\n\n  logit probit cloglog cauchit\n1 0.071  0.072   0.072   0.076\n2 0.073  0.074   0.075   0.084\n3 0.010  0.006   0.011   0.038\n4 0.010  0.006   0.012   0.039\n5 0.169  0.167   0.168   0.150\n6 0.034  0.033   0.035   0.051",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#application-to-wcgs-data-set-2",
    "href": "slides/04-logistic-regression-3.html#application-to-wcgs-data-set-2",
    "title": "Logistic Regression (Part III)",
    "section": "Application to wcgs data set",
    "text": "Application to wcgs data set\nComparing fitted values from different link functions:\n\n\nShow R code\nplot(sort(preds[, \"logit\"]), type = \"l\", ylab = \"Fitted value\")\nlines(sort(preds[, \"probit\"]), col = 2)\nlines(sort(preds[, \"cloglog\"]), col = 3)\nlines(sort(preds[, \"cauchit\"]), col = 4)\nlegend(\"topleft\", legend = c(\"logit\", \"probit\", \"cloglog\", \"cauchit\"), \n       col = 1:4, lty = 1, inset = 0.01)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#as-a-latent-variable-model",
    "href": "slides/04-logistic-regression-3.html#as-a-latent-variable-model",
    "title": "Logistic Regression (Part III)",
    "section": "As a latent variable model",
    "text": "As a latent variable model\n\nLogistic regression (and the other link functions) has an equivalent formulation as a latent variable model\nConsider a linear model with continuous outcome \\(Y^\\star = \\boldsymbol{x}^\\top\\boldsymbol{\\beta} + \\epsilon\\)\nImagine that we can only observe the binary variable \\[\nY = \\begin{cases}\n1 \\quad \\text{if } Y^\\star &gt; 0, \\\\\n0 \\quad \\text{otherwise}\n\\end{cases}\n\\]\nAssuming \\(\\epsilon \\sim \\text{Logistic}\\left(0, 1\\right)\\) leads to the usual logit model\nAssuming \\(\\epsilon \\sim N\\left(0, 1\\right)\\) leads to the probit model\nAnd so on‚Ä¶",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#application-surrogate-residual",
    "href": "slides/04-logistic-regression-3.html#application-surrogate-residual",
    "title": "Logistic Regression (Part III)",
    "section": "Application: surrogate residual",
    "text": "Application: surrogate residual\n\nThere are several types of residuals defined for logistic regression (e.g., deviance residuals and Pearson residuals)\nThe surrogate residual act like the usual residual from a normal linear model and has similar properties!\nSee Liu and Zhang (2018), Greenwell et al.¬†(2018), and Cheng et al.¬†(2020) for details\nA novel R-squared measure based on the surrogate idea was proposed in Liu et al.¬†(2022)\nFor implementation, see R packages sure, SurrogateRsq, and PAsso",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#o-rings-example",
    "href": "slides/04-logistic-regression-3.html#o-rings-example",
    "title": "Logistic Regression (Part III)",
    "section": "O-rings example",
    "text": "O-rings example\nOn January 28, 1986, the Space Shuttle Challenger broke apart 73 seconds into its flight, killing all seven crew members aboard. The crash was linked to the failure of O-ring seals in the rocket engines. Data was collected on the 23 previous shuttle missions. The launch temperature on the day of the crash was 31 \\(^\\circ\\)F.\n\n\nShow R code\nhead(orings &lt;- faraway::orings)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#o-rings-example-1",
    "href": "slides/04-logistic-regression-3.html#o-rings-example-1",
    "title": "Logistic Regression (Part III)",
    "section": "O-rings example",
    "text": "O-rings example\n\n\nShow R code\nplot(damage/6 ~ temp, data = orings, xlim = c(25, 85), pch = 19,\n     ylim = c(0, 1), ylab = \"Proportion damaged\")",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#o-rings-example-2",
    "href": "slides/04-logistic-regression-3.html#o-rings-example-2",
    "title": "Logistic Regression (Part III)",
    "section": "O-rings example",
    "text": "O-rings example\nExpand binomial data into independent Bernoulli trials\n\n\nShow R code\ntmp &lt;- rep(orings$temp, each = 6)\ndmg &lt;- sapply(orings$damage, FUN = function(x) rep(c(0, 1), times = c(6 - x, x)))\norings2 &lt;- data.frame(\"temp\" = tmp, \"damage\" = as.vector(dmg))\nhead(orings2, n = 15)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#o-rings-example-3",
    "href": "slides/04-logistic-regression-3.html#o-rings-example-3",
    "title": "Logistic Regression (Part III)",
    "section": "O-rings example",
    "text": "O-rings example\nHere we‚Äôll just fit a logistic regression to the expanded bernoulli version of the data\n\n\nShow R code\n# Fit a logistic regression (LR) model using 0/1 version of the data\norings.lr &lt;- glm(damage ~ temp, data = orings2, \n                 family = binomial(link = \"logit\"))\nsummary(orings.lr)\n\n\n\nCall:\nglm(formula = damage ~ temp, family = binomial(link = \"logit\"), \n    data = orings2)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 11.66299    3.29616   3.538 0.000403 ***\ntemp        -0.21623    0.05318  -4.066 4.77e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 76.745  on 137  degrees of freedom\nResidual deviance: 54.759  on 136  degrees of freedom\nAIC: 58.759\n\nNumber of Fisher Scoring iterations: 6",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#o-rings-example-4",
    "href": "slides/04-logistic-regression-3.html#o-rings-example-4",
    "title": "Logistic Regression (Part III)",
    "section": "O-rings example",
    "text": "O-rings example\nWhat‚Äôs the estimated probability that an O-ring will be damaged at 31 \\(^\\circ\\)F? Give a point estimate as well as a 95% confidence interval.\n\n\n\nShow R code\npredict(orings.lr, newdata = data.frame(\"temp\" = 31), type = \"response\", \n        se = TRUE)\n\n\n$fit\n        1 \n0.9930342 \n\n$se.fit\n         1 \n0.01153302 \n\n$residual.scale\n[1] 1\n\n\nShow R code\n# Better approach for a 95% CI?\npred &lt;- predict(orings.lr, newdata = data.frame(\"temp\" = 31), \n                type = \"link\", se = TRUE)\nplogis(pred$fit + c(-qnorm(0.975), qnorm(0.975)) * pred$se.fit)\n\n\n[1] 0.8444824 0.9997329",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#o-rings-example-5",
    "href": "slides/04-logistic-regression-3.html#o-rings-example-5",
    "title": "Logistic Regression (Part III)",
    "section": "O-rings example",
    "text": "O-rings example\n\n\nShow R code\n# Is this extrapolating?\nplot(damage / 6 ~ temp, data = orings, pch = 19, cex = 1.3,\n     col = adjustcolor(1, alpha.f = 0.3), xlim = c(0, 100), ylim = c(0, 1))\nx &lt;- seq(from = 0, to = 100, length = 1000)\ny &lt;- predict(orings.lr, newdata = data.frame(\"temp\" = x), \n             type = \"response\")\nlines(x, y, lwd = 2, col = 2)\nabline(v = 31, lty = 2, col = 3)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#o-rings-examples",
    "href": "slides/04-logistic-regression-3.html#o-rings-examples",
    "title": "Logistic Regression (Part III)",
    "section": "O-rings examples",
    "text": "O-rings examples\nMore interesting question: at what temperature(s) can we expect the risk/probability of damage to exceed 0.8?\n\nThis is a problem of inverse estimation, which is the purpose of the investr package!\n\n\nShow R code\n# To install from CRAN, use\n#\n# &gt; install.packages(\"investr\")\n#\n# See ?investr::invest for details and examples\ninvestr::invest(orings.lr, y0 = 0.8, interval = \"Wald\", lower = 40, upper = 60)\n\n\n estimate     lower     upper        se \n47.525881 39.993462 55.058299  3.843141",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#o-rings-example-6",
    "href": "slides/04-logistic-regression-3.html#o-rings-example-6",
    "title": "Logistic Regression (Part III)",
    "section": "O-rings example",
    "text": "O-rings example\nHere‚Äôs an equivalent logistic regression model fit to the original binomial version of the data (need to provide number of successes and number of failures)\n\n\nShow R code\norings.lr2 &lt;- glm(cbind(damage, 6 - damage) ~ temp, data = orings,\n                  family = binomial(link = \"logit\"))\nsummary(orings.lr2)\n\n\n\nCall:\nglm(formula = cbind(damage, 6 - damage) ~ temp, family = binomial(link = \"logit\"), \n    data = orings)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 11.66299    3.29626   3.538 0.000403 ***\ntemp        -0.21623    0.05318  -4.066 4.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 38.898  on 22  degrees of freedom\nResidual deviance: 16.912  on 21  degrees of freedom\nAIC: 33.675\n\nNumber of Fisher Scoring iterations: 6",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#overdispersion-1",
    "href": "slides/04-logistic-regression-3.html#overdispersion-1",
    "title": "Logistic Regression (Part III)",
    "section": "Overdispersion",
    "text": "Overdispersion\n\nFor a Bernoulli random variable \\(Y\\), \\(E(Y) = p\\) and \\(V(Y) = p(1 - p)\\).\nSometimes the data exhibit variance greater than expected. Adding a dispersion parameter makes the model more flexible: \\(V(Y) = \\sigma^2 p(1 - p)\\).\nOverdispersion occurs when variability exceeds expectations under the response distribution. Underdispersion is less common.",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#overdispersion-2",
    "href": "slides/04-logistic-regression-3.html#overdispersion-2",
    "title": "Logistic Regression (Part III)",
    "section": "Overdispersion",
    "text": "Overdispersion\n\nFor a correctly specified model, the Pearson chi-square statistic and deviance, divided by their degrees of freedom, should be about one. Values much larger indicate overdispersion.\nSuch goodness-of-fit statistics require replicated data. Problems like outliers, wrong link function, omitted terms, or untransformed predictors can inflate goodness-of-fit statistics.\nA large difference between the Pearson statistic and deviance suggests data sparsity.",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#o-rings-example-7",
    "href": "slides/04-logistic-regression-3.html#o-rings-example-7",
    "title": "Logistic Regression (Part III)",
    "section": "O-rings example",
    "text": "O-rings example\nYou can check for overdispersion manually or by using performance:\n\n\nShow R code\nperformance::check_overdispersion(orings.lr2)\n\n\n# Overdispersion test\n\n dispersion ratio = 1.206\n          p-value = 0.616",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#overdispersion-3",
    "href": "slides/04-logistic-regression-3.html#overdispersion-3",
    "title": "Logistic Regression (Part III)",
    "section": "Overdispersion",
    "text": "Overdispersion\nTwo common (but equivalent) ways to handle overdispersion in R:\n\nEstimate the dispersion parameter \\(\\sigma^2\\) and provide it to summary() to adjust the standard errors approriately\nUse the quasibinomial() family",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#overdispersion-4",
    "href": "slides/04-logistic-regression-3.html#overdispersion-4",
    "title": "Logistic Regression (Part III)",
    "section": "Overdispersion",
    "text": "Overdispersion\nEstimate the dispersion parameter \\(\\sigma^2\\); analagous to MSE in linear regression\n\n\nShow R code\n(sigma2 &lt;- sum(residuals(orings.lr2, type = \"pearson\") ^ 2) / orings.lr2$df.residual)\n\n\n[1] 1.336542\n\n\nShow R code\n# Print model summary based on estimated dispersion parameter\nsummary(orings.lr2, dispersion = sigma2)\n\n\n\nCall:\nglm(formula = cbind(damage, 6 - damage) ~ temp, family = binomial(link = \"logit\"), \n    data = orings)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 11.66299    3.81077   3.061 0.002209 ** \ntemp        -0.21623    0.06148  -3.517 0.000436 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1.336542)\n\n    Null deviance: 38.898  on 22  degrees of freedom\nResidual deviance: 16.912  on 21  degrees of freedom\nAIC: 33.675\n\nNumber of Fisher Scoring iterations: 6",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#overdispersion-5",
    "href": "slides/04-logistic-regression-3.html#overdispersion-5",
    "title": "Logistic Regression (Part III)",
    "section": "Overdispersion",
    "text": "Overdispersion\nCan use quasibinomial() family to account for over-dispersion automatically (notice the estimated coeficients don‚Äôt change)\n\n\nShow R code\norings.qb &lt;- glm(cbind(damage, 6 - damage) ~ temp, data = orings, \n                 family = quasibinomial)\nsummary(orings.qb)\n\n\n\nCall:\nglm(formula = cbind(damage, 6 - damage) ~ temp, family = quasibinomial, \n    data = orings)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) 11.66299    3.81077   3.061  0.00594 **\ntemp        -0.21623    0.06148  -3.517  0.00205 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 1.336542)\n\n    Null deviance: 38.898  on 22  degrees of freedom\nResidual deviance: 16.912  on 21  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 6",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#generalized-additive-models-gams",
    "href": "slides/04-logistic-regression-3.html#generalized-additive-models-gams",
    "title": "Logistic Regression (Part III)",
    "section": "Generalized additive models (GAMs)",
    "text": "Generalized additive models (GAMs)\n\n\\(\\text{logit}\\left(p\\right) = \\beta_0 + f_1\\left(x_1\\right) + f_2\\left(x_2\\right) + \\dots + f_p\\left(x_p\\right)\\)\n\nThe \\(f_i\\) are referred to as shape functions or term contributions and are often modeled using splines\nCan also include pairwise interactions of the form \\(f_{ij}\\left(x_i, x_j\\right)\\)\n\nEasy to interpret (e.g., just plot the individual shape functions)!\n\nInteraction effects can be understood with heat maps, etc.\n\nModern GAMs, called GA2Ms, automatically include relevant pairwise interaction effects",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#explainable-boosting-machines-ebms",
    "href": "slides/04-logistic-regression-3.html#explainable-boosting-machines-ebms",
    "title": "Logistic Regression (Part III)",
    "section": "Explainable boosting machines (EBMs)",
    "text": "Explainable boosting machines (EBMs)\n\nEBM ‚âà .darkblue[GA2M] + .green[Boosting] + .tomato[Bagging]\nPython library: interpret",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#western-collaborative-group-study",
    "href": "slides/02-logistic-regression-1.html#western-collaborative-group-study",
    "title": "Logistic Regression (Part I)",
    "section": "Western collaborative group study",
    "text": "Western collaborative group study\n\\(N = 3154\\) healthy young men aged 39‚Äì59 from the San Francisco area were assessed for their personality type. All were free from coronary heart disease at the start of the research. Eight and a half years later change in this situation was recorded. See ?faraway::wcgs in R.\n\n\nShow R code\n# install.packages(\"faraway\")\nhead(wcgs &lt;- faraway::wcgs)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#structure-of-wcgs-data",
    "href": "slides/02-logistic-regression-1.html#structure-of-wcgs-data",
    "title": "Logistic Regression (Part I)",
    "section": "Structure of wcgs data",
    "text": "Structure of wcgs data\n\n\nShow R code\nstr(wcgs)\n\n\n'data.frame':   3154 obs. of  13 variables:\n $ age    : int  49 42 42 41 59 44 44 40 43 42 ...\n $ height : int  73 70 69 68 70 72 72 71 72 70 ...\n $ weight : int  150 160 160 152 150 204 164 150 190 175 ...\n $ sdp    : int  110 154 110 124 144 150 130 138 146 132 ...\n $ dbp    : int  76 84 78 78 86 90 84 60 76 90 ...\n $ chol   : int  225 177 181 132 255 182 155 140 149 325 ...\n $ behave : Factor w/ 4 levels \"A1\",\"A2\",\"B3\",..: 2 2 3 4 3 4 4 2 3 2 ...\n $ cigs   : int  25 20 0 20 20 0 0 0 25 0 ...\n $ dibep  : Factor w/ 2 levels \"A\",\"B\": 1 1 2 2 2 2 2 1 2 1 ...\n $ chd    : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 2 1 1 1 1 1 ...\n $ typechd: Factor w/ 4 levels \"angina\",\"infdeath\",..: 3 3 3 3 2 3 3 3 3 3 ...\n $ timechd: int  1664 3071 3071 3064 1885 3102 3074 3071 3064 1032 ...\n $ arcus  : Factor w/ 2 levels \"absent\",\"present\": 1 2 1 1 2 1 1 1 1 2 ...",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#description-of-each-variable",
    "href": "slides/02-logistic-regression-1.html#description-of-each-variable",
    "title": "Logistic Regression (Part I)",
    "section": "Description of each variable",
    "text": "Description of each variable\n\n\nShow R code\n?wcgs\n\n\n\nage: age in years\nheight: height in inches\nweight: weight in pounds\nsdp: systolic blood pressure in mm Hg\ndbp: diastolic blood pressure in mm Hg\nchol: fasting serum cholesterol in mm %\nbehave: behavior type which is a factor with levels A1 A2 B3 B4\ncigs: number of cigarettes smoked per day\ndibep: behavior type a factor with levels A (Agressive) B (Passive)\nchd: coronary heat disease developed is a factor with levels no yes\ntypechd: type of coronary heart disease is a factor with levels angina infdeath none silent\ntimechd: time of CHD event or end of follow-up\narcus: arcus senilis is a factor with levels absent present",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#for-now-well-focus-on-3-variables",
    "href": "slides/02-logistic-regression-1.html#for-now-well-focus-on-3-variables",
    "title": "Logistic Regression (Part I)",
    "section": "For now, we‚Äôll focus on 3 variables",
    "text": "For now, we‚Äôll focus on 3 variables\n\n\nShow R code\nsummary(wcgs[, c(\"chd\", \"height\", \"cigs\")])\n\n\n  chd           height           cigs     \n no :2897   Min.   :60.00   Min.   : 0.0  \n yes: 257   1st Qu.:68.00   1st Qu.: 0.0  \n            Median :70.00   Median : 0.0  \n            Mean   :69.78   Mean   :11.6  \n            3rd Qu.:72.00   3rd Qu.:20.0  \n            Max.   :78.00   Max.   :99.0  \n\n\n\n Anything interesting stick out?",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data",
    "text": "Visualizing discrete data\nPie chart of response ü§Æ\n\n\nShow R code\n# Construct a pie chart of the (binary) response; I'm not a fan of pie charts in general\nptab &lt;- prop.table(table(wcgs$chd))  # convert frequencies to proportions\nptab  # inspect output\n\n\n\n        no        yes \n0.91851617 0.08148383 \n\n\nShow R code\npie(prop.table(table(wcgs$chd)),\n    main = \"Pie chart of Coronary Heart Disease\")",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data-1",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data-1",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data",
    "text": "Visualizing discrete data\nBar charts tend to be more effective\n\n\nShow R code\nbarplot(ptab, las = 1, col = \"forestgreen\")",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data-2",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data-2",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data",
    "text": "Visualizing discrete data\nMosaic plot showing relationship between cigs and chd; not incredibly useful IMO unless both variables are categorical\n\n\nShow R code\nplot(chd ~ cigs, data = wcgs)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data-3",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data-3",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data",
    "text": "Visualizing discrete data\nNonparametric density plot of height by chd status using lattice graphics:\n\n\nShow R code\nlibrary(lattice)\n\ndensityplot(~ height, groups = chd, data = wcgs, auto.key = TRUE)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data-4",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data-4",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data",
    "text": "Visualizing discrete data\nBoxplot of cigs vs.¬†chd status\n\n\nShow R code\nplot(cigs ~ chd, data = wcgs, col = c(2, 3))",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data-5",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data-5",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data",
    "text": "Visualizing discrete data\nBoxplot of cigs vs.¬†chd status\n\n\nShow R code\nplot(cigs ~ chd, data = wcgs, col = c(2, 3))",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data-6",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data-6",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data",
    "text": "Visualizing discrete data\nBoxplot of height vs.¬†chd status with notches\n\n\nShow R code\nplot(height ~ chd, data = wcgs, col = c(2, 3), notch = TRUE)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data-detour",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data-detour",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data (detour)",
    "text": "Visualizing discrete data (detour)\n\nDecision trees üå≤üå¥üå≥ are immensely useful for exploring new data sets!\nSome useful resources:\n\nSee the documentation for R packages rpart, party, and partykit\nFifty Years of Classification and Regression Trees\nRecursive Partitioning and Applications (logistic regression and trees)\nThe ultimate tree book üòé",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data-7",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data-7",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data",
    "text": "Visualizing discrete data\nStandard CART-like decision tree using rpart and rpart.plot (all variables allowed):\n\n\nShow R code\nrpart.plot::rpart.plot(rpart::rpart(chd ~ ., data = wcgs))",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data-8",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data-8",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data",
    "text": "Visualizing discrete data\nConditional inference tree using partykit (using only variables of interest):\n\n\nShow R code\nplot(partykit::ctree(chd ~ height + cigs, data = wcgs))",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#observations-so-far",
    "href": "slides/02-logistic-regression-1.html#observations-so-far",
    "title": "Logistic Regression (Part I)",
    "section": "Observations so far‚Ä¶",
    "text": "Observations so far‚Ä¶\n\nIt seems that the cigs is positively associated with the binary response chd\nIt is not clear how, if at all, height is associated with chd\nQuestion: how can we build a model to examine these potential associations?",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#linear-models",
    "href": "slides/02-logistic-regression-1.html#linear-models",
    "title": "Logistic Regression (Part I)",
    "section": "Linear models",
    "text": "Linear models\nRecall that in linear regression we model the conditional mean response as a linear function in some fixed, but known parameters \\(\\boldsymbol{\\beta}\\):\n\\[\nE\\left(Y|\\boldsymbol{x}\\right) = \\beta_0 + \\beta_1x_1 + \\dots \\beta_px_p = \\boldsymbol{\\beta}^\\top\\boldsymbol{x}\n\\]\n\nIf \\(Y\\) is a binary random variable, then what is \\(E\\left(Y|\\boldsymbol{x}\\right)\\)?",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#the-linear-probability-lp-model",
    "href": "slides/02-logistic-regression-1.html#the-linear-probability-lp-model",
    "title": "Logistic Regression (Part I)",
    "section": "The linear probability (LP) model",
    "text": "The linear probability (LP) model\nIt turns out that \\(E\\left(Y|\\boldsymbol{x}\\right) = P\\left(Y = 1|\\boldsymbol{x}\\right)\\). The LP model assumes that \\[\nP\\left(Y = 1|\\boldsymbol{x}\\right) = \\beta_0 + \\beta_1x_1 + \\dots \\beta_px_p = \\boldsymbol{\\beta}^\\top\\boldsymbol{x}\n\\]\n\nIs this reasonable?",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#the-lp-model-chd-vs.-cigs",
    "href": "slides/02-logistic-regression-1.html#the-lp-model-chd-vs.-cigs",
    "title": "Logistic Regression (Part I)",
    "section": "The LP model: chd vs.¬†cigs",
    "text": "The LP model: chd vs.¬†cigs\n\n\nShow R code\ny &lt;- ifelse(wcgs$chd == \"yes\", 1, 0)\nsummary(fit &lt;- lm(y ~ cigs, data = wcgs))\n\n\n\nCall:\nlm(formula = y ~ cigs, data = wcgs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.25268 -0.09794 -0.05876 -0.05876  0.94124 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.0587608  0.0062041   9.471  &lt; 2e-16 ***\ncigs        0.0019588  0.0003339   5.867 4.91e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2722 on 3152 degrees of freedom\nMultiple R-squared:  0.0108,    Adjusted R-squared:  0.01049 \nF-statistic: 34.42 on 1 and 3152 DF,  p-value: 4.91e-09\n\n\nAre the standard errors here appropriate? Why/why not?",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#the-lp-model-chd-vs.-cigs-1",
    "href": "slides/02-logistic-regression-1.html#the-lp-model-chd-vs.-cigs-1",
    "title": "Logistic Regression (Part I)",
    "section": "The LP model: chd vs.¬†cigs",
    "text": "The LP model: chd vs.¬†cigs\n\n\nShow R code\nplot(y ~ cigs, data = wcgs, ylim = c(0, 1), las = 1)\nabline(fit, col = 2)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#the-logistic-regression-lr-model",
    "href": "slides/02-logistic-regression-1.html#the-logistic-regression-lr-model",
    "title": "Logistic Regression (Part I)",
    "section": "The logistic regression (LR) model",
    "text": "The logistic regression (LR) model\nAssume \\(Y \\sim \\mathrm{Bernoulli}\\left(p\\right)\\), where \\[\np = p\\left(\\boldsymbol{x}\\right) = P\\left(Y = 1|\\boldsymbol{x}\\right) = E\\left(Y|\\boldsymbol{x}\\right)\n\\] and \\[\n\\mathrm{logit}\\left(p\\right) = \\log\\left(\\frac{p}{1-p}\\right) = \\boldsymbol{\\beta}^\\top\\boldsymbol{x}\n\\] In other words, LR models the logit of the mean response as a linear function in \\(\\boldsymbol{\\beta}\\); we‚Äôll refer to the term \\(\\eta = \\boldsymbol{\\beta}^\\top\\boldsymbol{x}\\) as the linear predictor. Why does this make more sense?",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#the-logistic-regression-lr-model-1",
    "href": "slides/02-logistic-regression-1.html#the-logistic-regression-lr-model-1",
    "title": "Logistic Regression (Part I)",
    "section": "The logistic regression (LR) model",
    "text": "The logistic regression (LR) model\nCan always solve for \\(p\\) to get predictions on the raw probability scale (Homework 2 üòÑ): \\[\np\\left(\\boldsymbol{x}\\right) = \\frac{\\exp\\left(\\boldsymbol{\\beta}^\\top\\boldsymbol{x}\\right)}{1 + \\exp\\left(\\boldsymbol{\\beta}^\\top\\boldsymbol{x}\\right)}\n\\]\n\nNote how the LR model is nonlinear in \\(p\\)!",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r",
    "href": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r",
    "title": "Logistic Regression (Part I)",
    "section": "Fitting an LR model in R",
    "text": "Fitting an LR model in R\n\nUse the glm() function instead of lm()\nGLM stands for generalized linear model, which includes the LR and ordinary linear regression models as special cases\nMany (but not all) of the models we‚Äôll discuss in throughout this course belong to the class of GLMs\nNote how we have to specifcy the family argument! (see ?glm for details)\nThe response can be a 0/1 indicator or a factor variable (be careful with interpretation and which class is used as the baseline):\n\n\n\nShow R code\nlevels(wcgs$chd)\n\n\n[1] \"no\"  \"yes\"",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r-1",
    "href": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r-1",
    "title": "Logistic Regression (Part I)",
    "section": "Fitting an LR model in R",
    "text": "Fitting an LR model in R\n\n\nShow R code\nsummary(fit.lr &lt;- glm(chd ~ cigs, data = wcgs, family = binomial))\n\n\n\nCall:\nglm(formula = chd ~ cigs, family = binomial, data = wcgs)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.742160   0.092111 -29.770  &lt; 2e-16 ***\ncigs         0.023220   0.004042   5.744 9.22e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1781.2  on 3153  degrees of freedom\nResidual deviance: 1750.0  on 3152  degrees of freedom\nAIC: 1754\n\nNumber of Fisher Scoring iterations: 5",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r-2",
    "href": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r-2",
    "title": "Logistic Regression (Part I)",
    "section": "Fitting an LR model in R",
    "text": "Fitting an LR model in R\nLet‚Äôs compare the fitted LR and LP models:\n\n\nShow R code\nprob &lt;- predict(fit.lr, newdata = data.frame(cigs = 0:99), type = \"response\")\nplot(y ~ cigs, data = wcgs, ylab = \"chd\", las = 1, xlim = c(0, 99))\nlines(0:99, y = prob, col = 2)\nabline(fit, col = 3, lty = 2)\nlegend(\"topright\", legend = c(\"LR fit\", \"LP fit\"), lty = c(1, 2), col = c(2, 3))",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r-3",
    "href": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r-3",
    "title": "Logistic Regression (Part I)",
    "section": "Fitting an LR model in R",
    "text": "Fitting an LR model in R\nLet‚Äôs compare the fitted LR and LP models:\n\n\nShow R code\nprob &lt;- predict(fit.lr, newdata = data.frame(cigs = 0:999), type = \"response\")\nplot(y ~ cigs, data = wcgs, ylab = \"chd\", las = 1, xlim = c(0, 999))\nlines(0:999, y = prob, col = 2)\nabline(fit, col = 3, lty = 2)\nlegend(\"topright\", legend = c(\"LR fit\", \"LP fit\"), lty = c(1, 2), col = c(2, 3))",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r-4",
    "href": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r-4",
    "title": "Logistic Regression (Part I)",
    "section": "Fitting an LR model in R",
    "text": "Fitting an LR model in R\nNow let‚Äôs include an additional predictor (i.e., height):\n\n\nShow R code\nsummary(fit.lr2 &lt;- glm(chd ~ cigs + height, data = wcgs, family = binomial))\n\n\n\nCall:\nglm(formula = chd ~ cigs + height, family = binomial, data = wcgs)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -4.50161    1.84186  -2.444   0.0145 *  \ncigs         0.02313    0.00404   5.724 1.04e-08 ***\nheight       0.02521    0.02633   0.957   0.3383    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1781.2  on 3153  degrees of freedom\nResidual deviance: 1749.0  on 3151  degrees of freedom\nAIC: 1755\n\nNumber of Fisher Scoring iterations: 5",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#interpreting-lr-coefficients",
    "href": "slides/02-logistic-regression-1.html#interpreting-lr-coefficients",
    "title": "Logistic Regression (Part I)",
    "section": "Interpreting LR coefficients",
    "text": "Interpreting LR coefficients\n\nLet \\(p = P\\left(Y = 1\\right)\\) and \\(1 - p = P\\left(Y = 0\\right)\\)\nThe odds of \\(Y = 1\\) occuring is defined as \\(p / \\left(1 - p\\right)\\)\nFor a fair coin ü™ô, the probability of getting tails is \\(p = 0.5\\). Therefore, the odds of getting tails vs.¬†heads is \\(p / (1 - p) = 0.5 / 0.5 = 1\\). (We might also say the odds of getting tails is ‚Äú1 to 1‚Äù).\n\n\n\n\nFor a fair die üé≤, what are the odds of rolling a 2?",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#interpreting-lr-coefficients-1",
    "href": "slides/02-logistic-regression-1.html#interpreting-lr-coefficients-1",
    "title": "Logistic Regression (Part I)",
    "section": "Interpreting LR coefficients",
    "text": "Interpreting LR coefficients\nThe logit models the log odds of success (i.e., \\(Y = 1|\\boldsymbol{x}\\)) \\[\n\\log\\left(\\mathrm{odds}\\right) = \\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots \\beta_px_p\n\\] Exponentiating both, we get \\[\n\\mathrm{odds} = \\frac{p}{1-p} = \\exp{\\left(\\beta_0\\right)}\\times\\exp{\\left(\\beta_1x_1\\right)}\\times\\exp{\\left(\\beta_2x_2\\right)}\\times\\dots\\times\\exp{\\left(\\beta_px_p\\right)}\n\\]\n\nIn the LR model, \\(\\beta_i\\) represents the change in the log odds when \\(x_i\\) increases by one unit (all else held constant)\nIn the LR model, \\(\\exp\\left(\\beta_i\\right)\\) represents the multiplicative increase in the odds when \\(x_i\\) increases by one unit (all else held constant)\nCANNOT interpret the coefficients in terms of \\(p\\) directly‚Ä¶effect plots to the rescue üõü!!",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#wcgs-study",
    "href": "slides/02-logistic-regression-1.html#wcgs-study",
    "title": "Logistic Regression (Part I)",
    "section": "WCGS study",
    "text": "WCGS study\n\n\nShow R code\ncoef(fit.lr2)\n\n\n(Intercept)        cigs      height \n-4.50161397  0.02312740  0.02520779 \n\n\n\n\nHolding height constant, for every additional cigarette smoked per day the predicted log odds of developing chd increases by 0.023\nHolding height constant, for every additional cigarette smoked per day the predicted odds of developing chd increases multiplicatively by 1.023",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#effect-plots",
    "href": "slides/02-logistic-regression-1.html#effect-plots",
    "title": "Logistic Regression (Part I)",
    "section": "Effect plots",
    "text": "Effect plots\nLot‚Äôs of different methods and packages:\n\nMarginal effects via effects library\nPartial dependence (PD) plots and individual conditional expectation (ICE) plots via the pdp package\nMarginal effect and PD plots via the plotmo library\nAnd many, many more (see R‚Äôs ML Task View)\nAll have there own assumptions and drawbacks; typically, similar in shape when the model is additive in nature (i.e., no interaction effects)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#effect-plots-1",
    "href": "slides/02-logistic-regression-1.html#effect-plots-1",
    "title": "Logistic Regression (Part I)",
    "section": "Effect plots",
    "text": "Effect plots\nThe plotmo library is an ‚Äúeasy button‚Äù for quick and dirty effect plots (other variables are held fixed at their median or most frequent value) and supports a wide range of models\n\n\nShow R code\nplotmo::plotmo(fit.lr2)\n\n\n plotmo grid:    cigs height\n                    0     70",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#effect-plots-2",
    "href": "slides/02-logistic-regression-1.html#effect-plots-2",
    "title": "Logistic Regression (Part I)",
    "section": "Effect plots",
    "text": "Effect plots\nI generally prefer PD plots; see Greenwell (2017) for details\n\n\nShow R code\nlibrary(pdp)\nlibrary(ggplot2)\n\ntheme_set(theme_bw())\npartial(fit.lr2, pred.var = \"cigs\", prob = TRUE, plot = TRUE, \n        plot.engine = \"ggplot2\", rug = TRUE) + \n    ylim(0, 1) + \n    ylab(\"Probability\")",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#effect-plots-3",
    "href": "slides/02-logistic-regression-1.html#effect-plots-3",
    "title": "Logistic Regression (Part I)",
    "section": "Effect plots",
    "text": "Effect plots\nCan easily extend these methods to two or three variables:\n\n\nShow R code\npd.2d &lt;- partial(fit.lr2, pred.var = c(\"cigs\", \"height\"), chull = TRUE)\nplotPartial(pd.2d)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#effect-plots-4",
    "href": "slides/02-logistic-regression-1.html#effect-plots-4",
    "title": "Logistic Regression (Part I)",
    "section": "Effect plots",
    "text": "Effect plots\nThree-dimensional plots look cool, but generally aren‚Äôt all that useful:\n\n\nShow R code\nlattice::wireframe(yhat ~ height * cigs, data = pd.2d, shade = TRUE)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#ml-estimation-bernoulli",
    "href": "slides/02-logistic-regression-1.html#ml-estimation-bernoulli",
    "title": "Logistic Regression (Part I)",
    "section": "ML estimation (Bernoulli)",
    "text": "ML estimation (Bernoulli)\n\nIn the linear model, LS and ML estimation are equivalent and rather straightforward\nIn LR, ML estimation is more common1\nRecall that if \\(Y_i \\stackrel{iid}{\\sim} Bernoulli\\left(p\\right)\\), then the likilhood function is defined as\n\n\\[\nL\\left(p\\right) = \\prod_{i=1}^n p^{y_i} \\left(1-p\\right)^{1-y_i}\n\\]\nTechnically, there are a number of ways to estimate the parameters of an LR model, but ML estimation is most common.",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#ml-estimation-bernoulli-1",
    "href": "slides/02-logistic-regression-1.html#ml-estimation-bernoulli-1",
    "title": "Logistic Regression (Part I)",
    "section": "ML estimation (Bernoulli)",
    "text": "ML estimation (Bernoulli)\n\nTheoretically, goal is to maximize \\(L\\left(p\\right)\\)\nIn practice, it‚Äôs easier to work with the log likelihood \\(l\\left(p\\right) = log\\left[L\\left(p\\right)\\right]\\)\n\n\\[\n\\begin{align}\nl\\left(p\\right) &= \\log\\left[\\prod_{i=1}^n p^{y_i} \\left(1-p\\right)^{1-y_i}\\right]\\\\\n&= \\cdots\\\\\n&= \\log\\left(p\\right)\\sum_{i=1}^ny_i + \\log\\left(1-p\\right)\\sum_{i=1}^n\\left(n-y_i\\right)\n\\end{align}\n\\]",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#ml-estimation-lr",
    "href": "slides/02-logistic-regression-1.html#ml-estimation-lr",
    "title": "Logistic Regression (Part I)",
    "section": "ML estimation (LR)",
    "text": "ML estimation (LR)\n\nIn LR, \\[\np_i = p\\left(\\boldsymbol{x}_i\\right) = \\frac{\\exp\\left(\\boldsymbol{\\beta}^\\top\\boldsymbol{x}_i\\right)}{1 + \\exp\\left(\\boldsymbol{\\beta}^\\top\\boldsymbol{x}_i\\right)}\n\\]\nThis becomes a more complicated optimization problem!\nEquivalent to minimizing log loss in machine learning\nLog loss is a proper scoring rule",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#ml-estimation-lr-1",
    "href": "slides/02-logistic-regression-1.html#ml-estimation-lr-1",
    "title": "Logistic Regression (Part I)",
    "section": "ML estimation (LR)",
    "text": "ML estimation (LR)\nMaximizing the log-likelihood is equivalent to minimizing the negative log-likelihood (which is more convenient):\n\n\nShow R code\n# Response (as a binary 0/1 variable)\ny &lt;- ifelse(wcgs$chd == \"yes\", 1, 0)\n\n# Model matrix; includes a column for the intercept by default\nX &lt;- model.matrix(~ cigs + height, data = wcgs)\n\n# Function to compute the negative log-liklihood (as a function of the betas)\nnll &lt;- function(beta) {\n  lp &lt;- X %*% beta  # linear predictor; same as b0 + b1*x1 + b2*x2 + ...\n  -sum(y * lp - log(1 + exp(lp)))\n}\n\n# Use general optimization; would be better to use gradient and hessian info (e.g., first and second derivative info)\nlp.fit &lt;- lm(y ~ cigs + height, data = wcgs)\noptim(coef(lp.fit), fn = nll, \n      control = list(\"maxit\" = 9999, \"reltol\" = 1e-20))\n\n\n$par\n(Intercept)        cigs      height \n-4.50161158  0.02312743  0.02520775 \n\n$value\n[1] 874.5246\n\n$counts\nfunction gradient \n     658       NA \n\n$convergence\n[1] 0\n\n$message\nNULL",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#wald-statistic",
    "href": "slides/02-logistic-regression-1.html#wald-statistic",
    "title": "Logistic Regression (Part I)",
    "section": "Wald statistic",
    "text": "Wald statistic\n\nConsider the usual marginal test: \\(H_0: \\beta_j = 0\\) vs.¬†\\(H_1: \\beta_j \\ne 0\\)\nAssymptotically speaking, \\(Z_i = \\hat{\\beta}_i / \\mathrm{SE}\\left(\\hat{\\beta}_i\\right)\\) has a standard normal distribution under \\(H_0\\)\nThis leads to the usual Wald-based confidence intervals, etc.\nBetter (but more complicated) approaches available, like likelihood ratio tests, profile likelihood methods, and the bootstrap.",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#extending-the-general-linear-f-test",
    "href": "slides/02-logistic-regression-1.html#extending-the-general-linear-f-test",
    "title": "Logistic Regression (Part I)",
    "section": "Extending the general linear F-test",
    "text": "Extending the general linear F-test\nIn LR, we move from a general linear F-test to a likelihood ratio test based on the \\(\\chi^2\\) distribution: \\[\nX = -2\\log\\left(\\frac{L_{H_0}}{L_{H_1}}\\right) = D_{H_0} - D_{H_1} \\sim \\chi^2\\left(df\\right)\n\\]\nTry by hand:\n\n\nShow R code\nfit.H0 &lt;- glm(chd ~ cigs, data = wcgs, family = binomial)\nfit.H1 &lt;- glm(chd ~ cigs + height, data = wcgs, family = binomial)\nD.H0 &lt;- deviance(fit.H0)  # same as `-2*logLik(fit.H0)`\nD.H1 &lt;- deviance(fit.H1)  # same as `-2*logLik(fit.H1)`\nchisq.stat &lt;- D.H0 - D.H1\npval &lt;- pchisq(chisq.stat, df = 1, lower.tail = FALSE)\nc(\"stat\" = chisq.stat, \"pval\" = pval)\n\n\n     stat      pval \n0.9202473 0.3374101",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#extending-the-general-linear-f-test-1",
    "href": "slides/02-logistic-regression-1.html#extending-the-general-linear-f-test-1",
    "title": "Logistic Regression (Part I)",
    "section": "Extending the general linear F-test",
    "text": "Extending the general linear F-test\nCan do this automatically using R‚Äôs anova() function:\n\n\nShow R code\nanova(fit.H0, fit.H1, test = \"Chi\")",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#confidence-intervals",
    "href": "slides/02-logistic-regression-1.html#confidence-intervals",
    "title": "Logistic Regression (Part I)",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nLot‚Äôs of ways to do this (some bad, some better). R defaults to using a profile likelihood method:\n\n\nShow R code\nconfint(fit.lr2, level = 0.95)\n\n\n                  2.5 %      97.5 %\n(Intercept) -8.13475465 -0.91297018\ncigs         0.01514949  0.03100534\nheight      -0.02619902  0.07702835",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#questions",
    "href": "slides/02-logistic-regression-1.html#questions",
    "title": "Logistic Regression (Part I)",
    "section": "Questions‚ùì‚ùì‚ùì",
    "text": "Questions‚ùì‚ùì‚ùì",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BANA 7042: Statistical Modeling",
    "section": "",
    "text": "Welcome to the course website for BANA 7042: Statistical Modeling."
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "BANA 7042: Statistical Modeling",
    "section": "Quick Links",
    "text": "Quick Links\n\nSyllabus\nSlides\nAssignments"
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "BANA 7042: Statistical Modeling",
    "section": "Instructor",
    "text": "Instructor\nBrandon Greenwell"
  },
  {
    "objectID": "index.html#announcements",
    "href": "index.html#announcements",
    "title": "BANA 7042: Statistical Modeling",
    "section": "Announcements",
    "text": "Announcements\n\nWelcome to the class!"
  },
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "Assignments and exams.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nSubtitle\n\n\n\n\n\n\n\n\nAssignment 4\n\n\nLogistic Regression III\n\n\n\n\n\n\nAssignment 5\n\n\nMultinomial & Ordinal Regression\n\n\n\n\n\n\nBANA 7042: Assignment 1\n\n\nLinear Regression\n\n\n\n\n\n\nBANA 7042: Assignment 2\n\n\nLogistic regression (part I)\n\n\n\n\n\n\nBANA 7042: Assignment 3 (20 points)\n\n\nLogistic regression (part II)\n\n\n\n\n\n\nBANA 7042: Assignment 6\n\n\nCount Regression\n\n\n\n\n\n\nBANA 7042: Final Exam\n\n\n¬†\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/final.html",
    "href": "assignments/final.html",
    "title": "BANA 7042: Final Assignment/Exam",
    "section": "",
    "text": "Note: Please be sure to properly label all figures and include a caption for each!\nQuestion 1. The data below gives the number of new AIDS cases each year in a country, from 1981 onwards (Source: Venables and Ripley (2003), slightly modified).\ncases &lt;- c(12, 14, 33, 50, 67, 74, 123, 141, 165, 204, 253, 246, 240, 246, 232)\nyear &lt;- 1:15 + 1980\npar(mfrow = c(2, 2))  # set up 2x2 plotting grid\nplot(&lt;my-model&gt;, which = 1:4)\nQuestion2\nSource: Bilder and Loughin (2015)\nIn order to maximize sales, items within grocery stores are strategically placed to draw customer attention. This exercise examines one type of item‚Äîbreakfast cereal. Typically, in large grocery stores, boxes of cereal are placed on sets of shelves located on one side of the aisle. By placing particular boxes of cereals on specific shelves, grocery stores may better attract customers to them. To investigate this further, a random sample of size 10 was taken from each of four shelves at a Dillons grocery store in Manhattan, KS. These data are given in the cereal.csv file. The response variable is the shelf number, which is numbered from bottom (1) to top (4), and the explanatory variables are the sugar, fat, and sodium content of the cereals. Using these data, complete the following:\n# Function to rescale a variable to be within (0, 1)\nstand01 &lt;- function (x) { \n    (x - min(x)) / (max(x) - min(x)) \n}\n\n# Standardize and rescale data for analysis\ncereal2 &lt;- data.frame(\n  Shelf = cereal$Shelf, \n  sugar = stand01(cereal$sugar_g / cereal$size_g), \n  fat = stand01(cereal$fat_g / cereal$size_g), \n  sodium = stand01(cereal$sodium_mg / cereal$size_g)\n)\nboxplot(sugar ~ Shelf, data = cereal2, ylab = \"Sugar\", xlab = \"Shelf\", \n        pars = list(outpch = NA))\nstripchart(cereal2$sugar ~ cereal2$Shelf, lwd = 2, col = \"red\", \n           method = \"jitter\", vertical = TRUE, pch = 1, add = TRUE)\nBased on your visual analysis, discuss if possible content differences exist among the shelves.\n#install.packages(\"pdp\")\nlibrary(pdp)\npfun &lt;- function(object, newdata) {\n  probs &lt;- predict(object, newdata = newdata, type = \"probs\")\n  colMeans(probs)\n}\n# Assuming the name of your model is `fit`\npd &lt;- partial(fit, pred.var = \"sugar\", pred.fun = pfun, plot = FALSE)\nlattice::xyplot(yhat ~ sugar|yhat.id, data = pd, type = \"l\", \n                ylab = \"probability\")\nQuestion 3 The failure of an O-ring on the space shuttle Challenger‚Äôs booster rockets led to its destruction in 1986. Using data on previous space shuttle launches, Dalal et al.¬†(1989) examine the probability of an O-ring failure as a function of temperature at launch and combustion pressure; in class, we looked at only temperature. Data from their paper is included in the challenger.csv file. Below are the variables:\n‚Ä¢ Flight: Flight number ‚Ä¢ Temp: Temperature (F) at launch ‚Ä¢ Pressure: Combustion pressure (psi) ‚Ä¢ O.ring: Number of primary field O-ring failures ‚Ä¢ Number: Total number of primary field O-rings (six total, three each for the two booster rockets)\nThe response variable is O.ring, and the explanatory variables are Temp and Pressure. Complete the following:\nQuestion 4. Consider the Bikeshare data set from the ISLR2 package. The data can be loaded into R using the following code:\n# install.packages(\"ISLR2\")  # run this first if you don't have ISLR2 installed\nbike &lt;- ISLR2::Bikeshare\nRead the help page for the data set using ?ISLR2::Bikeshare in the R console or by visiting the package documentation at the above link. Build an appropriate model using bikers as the response against the following predictors: workingday, temp, weathersit, mnth, and hr. Create a 1-2 page report describing the data, variables, and model you‚Äôve built (e.g., what kind of model did you choose and why). Be sure to include a well-formatted regression table with coefficients and standard errors. Any evidence of overdispersion or zero-inflation? If so, how did you deal with it. Provide an interpretation for each coefficient (for categorical variables, you only need to provide an interpretation for one of the categories). Discuss which variables seem the most important in predicting the response. Under what conditions does bike rentals seem to be highest? Do these results make sense? Include effect plots for what you consider to be the two most important predictors and describe any trends you see (be sure to explain how you selected these predictors in the first place). What recommendations can you make to the bike rental agency to improve the rental sales? DO NOT INCLUDE ANY R CODE OR DIRECT OUTPUT. Use well-formatted tables and graphics with captions. This is a report for a stakeholder, so simplicity, good grammar, and complete sentences are key. (Feel free to put R code and output in an appendix if you feel it‚Äôs necessary. Hint: It‚Äôs not.)"
  },
  {
    "objectID": "assignments/final.html#footnotes",
    "href": "assignments/final.html#footnotes",
    "title": "BANA 7042: Final Assignment/Exam",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe author of nnet::multinom() suggests this can help with the convergence of parameter estimates when using the function. We recommend the 0-1 rescaling because slow convergence occurs here when estimating the model in part (d) without re-scaling.‚Ü©Ô∏é"
  },
  {
    "objectID": "assignments/assignment-02.html",
    "href": "assignments/assignment-02.html",
    "title": "BANA 7042: Assignment 2",
    "section": "",
    "text": "Please clearly circle (or highlight) your multiple choice answers\nQuestion 1: True or false. In the LR model, it is assumed that there is a linear relationship between the predictor variables and the mean response (i.e., probability of success or \\(Y = 1\\)). If false, write the correct statement.\nQuestion 2 Why is ordinary linear regression not appropriate for binary outcomes?\n\nIt only has one weight per feature\nIt only has one output value\nThe bias parameter skews the output value\nIts predictions are not restricted to values between 0 and 1\n\nQuestion 3 Large values of the log-likelihood statistic indicates that:\n\nThere is potential multicollinearity in the data\nThe associated model fits the data well\nAs the predictor variables increase in value, the likelihood of the outcome occurring decreases\nThe associated model is a poor fit of the data\n\nQuestion 4 LR is used to predict the probability of a ___?\n\nBinary independent variable\nBinary dependent variable.\nContinuous dependent variable.\nContinuous independent variable.\n\nQuestion 5 In a logistic regression, if the predicted logit is 0, what‚Äôs the transformed probability?\n\n0\n1\n0.5\n0.05",
    "crumbs": [
      "Assignments",
      "BANA 7042: Assignment 2"
    ]
  },
  {
    "objectID": "assignments/assignment-02.html#part-i-multiple-choice-1-point-each",
    "href": "assignments/assignment-02.html#part-i-multiple-choice-1-point-each",
    "title": "BANA 7042: Assignment 2",
    "section": "",
    "text": "Please clearly circle (or highlight) your multiple choice answers\nQuestion 1: True or false. In the LR model, it is assumed that there is a linear relationship between the predictor variables and the mean response (i.e., probability of success or \\(Y = 1\\)). If false, write the correct statement.\nQuestion 2 Why is ordinary linear regression not appropriate for binary outcomes?\n\nIt only has one weight per feature\nIt only has one output value\nThe bias parameter skews the output value\nIts predictions are not restricted to values between 0 and 1\n\nQuestion 3 Large values of the log-likelihood statistic indicates that:\n\nThere is potential multicollinearity in the data\nThe associated model fits the data well\nAs the predictor variables increase in value, the likelihood of the outcome occurring decreases\nThe associated model is a poor fit of the data\n\nQuestion 4 LR is used to predict the probability of a ___?\n\nBinary independent variable\nBinary dependent variable.\nContinuous dependent variable.\nContinuous independent variable.\n\nQuestion 5 In a logistic regression, if the predicted logit is 0, what‚Äôs the transformed probability?\n\n0\n1\n0.5\n0.05",
    "crumbs": [
      "Assignments",
      "BANA 7042: Assignment 2"
    ]
  },
  {
    "objectID": "assignments/assignment-02.html#part-ii-short-answer-response-3-points-each",
    "href": "assignments/assignment-02.html#part-ii-short-answer-response-3-points-each",
    "title": "BANA 7042: Assignment 2",
    "section": "Part II: short answer response (3 points each)",
    "text": "Part II: short answer response (3 points each)\nQuestion 6. For ease of notation, let \\(\\eta = \\boldsymbol{x}^\\top\\boldsymbol{\\beta} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1 + \\cdots + \\beta_p x_p\\) be the linear predictor. For the LR model, where \\[\n\\mathrm{logit}\\left(p\\right) = \\boldsymbol{x}^\\top\\boldsymbol{\\beta},\n\\] show that \\[\np = \\frac{1}{1 + \\exp\\left(-\\eta\\right)}\n\\]\nQuestion 7: (2 pts.) Using historical data from past offers sent to customers, your company uses a logistic regression model to predict the likelihood that a customer will redeem a particular offer that was sent to them (e.g., a coupon for milk). The model is to be used on a new set of \\(n = 500\\) customers from the same population that the original training sample came from. However, your stakeholder asks another question. In addition to predicting the probability that each of the 500 new customers will redeem the offer sent to them, you‚Äôre asked to also provide an estimate of the total number of offers that will be redeeemd. Discuss how you would approach this problem. (No wrong asnwer here so don‚Äôt Google it, I just want to hear your group‚Äôs solution in detail.)\nFor the next few questions, you‚Äôll be using the blood donation data available from the UC Irvine Machine Learning Repository; you can download the data from here.\nQuestion 8 Import the data into R and fit a simple logistic regression model using Donated_Blood as the binary response and Recency as the predictor. Interpret the coefficient of Recenecy (be sure to use full sentences and include the units). How would you interpret the estimated intercept in this model?\nQuestion 9 Plot the fitted LR model from Question 8 on the probability scale (i.e., Recency on the \\(x\\)-axis and the predicted probability of donating on the \\(y\\)-axis). The donation center asks you to estimate the Recency associated with a predicted likelihood of donating of \\(p = 0.5\\). (In Pharmacology, this is referred to as the median effective dose.) Provide an estimate using your fitted model and describe how you arrived at this value.\nQuestion 10 Fit an LR model using all four predictors and provide an interpretation for each coefficient estimate. Without ‚ÄúGoogling‚Äù or ‚ÄúChatGPT-ing‚Äù, come up with a way of measuring the realtive ‚Äúimportance‚Äù of each variable. (There‚Äôs a million ways to do this, none of which are perfect, so please use your own intuition and expertise to devise a way to accomplish this.) Which variable seems most important in predicting Donate_Blood? Provide a table with the ranking of each variable. Describe how your group arrived at these rankings. What are some potential drawbacks of your approach?",
    "crumbs": [
      "Assignments",
      "BANA 7042: Assignment 2"
    ]
  },
  {
    "objectID": "assignments/assignment-06.html",
    "href": "assignments/assignment-06.html",
    "title": "BANA 7042: Assignment 6",
    "section": "",
    "text": "This assignment covers count regression models (Poisson, Negative Binomial).\nComing Soon",
    "crumbs": [
      "Assignments",
      "BANA 7042: Assignment 6"
    ]
  },
  {
    "objectID": "assignments/assignment-06.html#overview",
    "href": "assignments/assignment-06.html#overview",
    "title": "BANA 7042: Assignment 6",
    "section": "",
    "text": "This assignment covers count regression models (Poisson, Negative Binomial).\nComing Soon",
    "crumbs": [
      "Assignments",
      "BANA 7042: Assignment 6"
    ]
  },
  {
    "objectID": "assignments/assignment-05.html",
    "href": "assignments/assignment-05.html",
    "title": "Assignment 5",
    "section": "",
    "text": "This assignment covers multinomial and ordinal regression models.\nComing Soon",
    "crumbs": [
      "Assignments",
      "Assignment 5"
    ]
  },
  {
    "objectID": "assignments/assignment-05.html#overview",
    "href": "assignments/assignment-05.html#overview",
    "title": "Assignment 5",
    "section": "",
    "text": "This assignment covers multinomial and ordinal regression models.\nComing Soon",
    "crumbs": [
      "Assignments",
      "Assignment 5"
    ]
  },
  {
    "objectID": "assignments/assignment-04.html",
    "href": "assignments/assignment-04.html",
    "title": "Assignment 4",
    "section": "",
    "text": "This assignment covers advanced topics in logistic regression.\nComing Soon",
    "crumbs": [
      "Assignments",
      "Assignment 4"
    ]
  },
  {
    "objectID": "assignments/assignment-04.html#overview",
    "href": "assignments/assignment-04.html#overview",
    "title": "Assignment 4",
    "section": "",
    "text": "This assignment covers advanced topics in logistic regression.\nComing Soon",
    "crumbs": [
      "Assignments",
      "Assignment 4"
    ]
  },
  {
    "objectID": "assignments/assignment-03.html",
    "href": "assignments/assignment-03.html",
    "title": "BANA 7042: Assignment 3 (20 points)",
    "section": "",
    "text": "Note: Please be sure to properly label all figures and include a caption for each!\nFor this assignment, you‚Äôll be working with data from the direct marketing campaigns (phone calls) of a Portuguese banking institution. The goal is to predict the likelihood that a client will subscribe a term deposit (i.e., the binary variable labeled y). The data are available from the UC Irvine Machine Learning Repository.\nDirect URL: https://archive.ics.uci.edu/dataset/222/bank+marketing\nI have already downloaded the data and split them into two samples, which are available on the corresponding Assignments tab in our course‚Äôs Canvas page:\n\nbank.csv - these data will be used to build/train your logistic regression models;\nbank_new.csv - these data will be used for testing your final logistic regression model.\n\nQuestion 1: Use the bank.csv data to build an initial logistic regression model using all available predictors.\n\nDoes there appear to be any issues with the model? If so, please elaborate on what you see.\nUse the methods described in class to explore and potentially reduce the model further and explain your process (e.g., look for potential multicollinearity and redundant features, etc.).\nDraw an ROC curve and calibration plot for your final model. Which plot do you think is more useful given the task at hand?\nRead the background documentation for these data here. Does there appear to be any leakage variables in the data? If so, describe which ones and refit your model without those predictors. How do the ROC and calibration curves compare to the previous model (e.g., better or worst)? Does this make sense?\n\nQuestion 2: Try exploring forward selection (BS) and backward elimination (BE). For FS, start with an intercept-only model. For BE, you can start with your final model from Question 1. How do the results compare between FS and BE? How do they compare to your final model from Question 1?\nQuestion 3: Using the Brier score metric (see function below), compute the LOCO-based variable importance for all of the variables in your final model and display the results using a simple dotchart (do this using the Brier score). Do the results make sense? (You can use and modify the R code we walked through in class.)\n\n# Brier score function to compute MSE between binary y and probability p\n#\n# Args:\n#   y - the binary 0/1 outcome\n#   p - the predicted probability that y=1\nbrier_score &lt;- function(y, p) {\n  mean((y - p) ^ 2)\n}\n\nQuestion 4: Apply your final model to the bank_new.csv data to obtain the predicted probability that each customer will subscribe to a term deposit if offered. Using these results, construct a cumulative gains chart (like we did in class). If we used your model to identify the 1000 customers most likely to subscribe, how many successes can we expect? Be sure to include your plot as well.",
    "crumbs": [
      "Assignments",
      "BANA 7042: Assignment 3 (20 points)"
    ]
  },
  {
    "objectID": "assignments/assignment-01.html",
    "href": "assignments/assignment-01.html",
    "title": "BANA 7042: Assignment 1",
    "section": "",
    "text": "Please clearly circle (or highlight) your multiple choice answers\nQuestion 1: In a multiple linear regression model, which of the following are considered as random?\nQuestion 2: When developing a linear regression model, adding additional predictors to the model will\nQuestion 3: What is the best way to identify potential multicollinearity?\nQuestion 4: When building a linear regression model of the form \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\\) for a data set, the presence of multicollinearity in the data may result in _________ standard errors for the slope coefficients (\\(\\beta_1\\) and \\(\\beta_2\\)) than if the data came from an orthogonal design (i.e., when \\(X_1\\) and \\(X_2\\) are uncorrelated).\nQuestion 5: What does the following residual versus fitted value plot suggest about a model between a single predictor \\(X\\) and \\(Y\\)?\nQuestion 6: When a linear regression model is being developed, adding additional variables to the model will\nQuestion 7: The ordinary residuals refer to\nQuestion 8: For a fitted simple linear regression model, which one of the following properties is NOT true?\nQuestion 9: The diagonal elements of hat matrix, also referred to as the hat values or leverage values, measures the influence of observation \\(i\\) on the regression line when removing observation \\(i\\).\nQuestion 10: In a regression study, a 95% confidence interval for \\(\\beta_1\\) was given as: \\(\\left(-5, 2\\right)\\). What does this confidence interval mean?\nQuestion 11: In a regression study, a 95% confidence interval for \\(\\beta_1\\) was given as: \\(\\left(-5, 2\\right)\\). Which of following is correct?\nQuestion 12: Point A in the far right is likely to be\nQuestion 13: Which of the following cases can lead to multicollinearity?\nQuestion 14: What does the last line of output ‚ÄúF-statistic‚Ä¶.p-value: ‚Ä¶‚Äù indicate in the following multiple regression output?\nCall:\nlm(formula = mpg ~ cyl + disp + hp + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0562 -1.4636 -0.4281  1.2854  5.8269 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 40.82854    2.75747  14.807 1.76e-14 ***\ncyl         -1.29332    0.65588  -1.972 0.058947 .  \ndisp         0.01160    0.01173   0.989 0.331386    \nhp          -0.02054    0.01215  -1.691 0.102379    \nwt          -3.85390    1.01547  -3.795 0.000759 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.513 on 27 degrees of freedom\nMultiple R-squared:  0.8486,    Adjusted R-squared:  0.8262 \nF-statistic: 37.84 on 4 and 27 DF,  p-value: 1.061e-10\nQuestion 15: If one wishes to incorporate seasonal dummy variables for monthly data into a regression model, how many dummy variables should be in the model?\nQuestion 16: Consider a simple linear regression model where \\(\\log(Y) = \\beta_0 + \\beta_1 X + \\varepsilon\\). If \\(\\beta_1 = 0.05\\), which best describes this coefficient‚Äôs interpretation?",
    "crumbs": [
      "Assignments",
      "BANA 7042: Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-01.html#part-ii-short-answer-response",
    "href": "assignments/assignment-01.html#part-ii-short-answer-response",
    "title": "BANA 7042: Assignment 1",
    "section": "Part II: short answer response",
    "text": "Part II: short answer response\nFor the multiple linear regression model \\(Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\dots + \\beta_p X_{p-1, i} + \\epsilon_i\\)\nQuestion 17: (2 pts.) Recall that the variance inflation factor for \\(X_j\\) is defined as \\(VIF_j = \\left(1 - R_j^2\\right)^{-1}\\). Explain in one sentence what \\(R_j^2\\) is. Explain in one sentence why it is better to use VIFs than pairwise correlations when trying to detect the presence of multicollinearity in a regression data set.\nQuestion 18: (2 pts.) For variable selection criteria based on residuals, among \\(PRESS\\), \\(SSE\\), \\(R^2\\), \\(R_{adj}^2\\), and \\(MSE\\), which are ‚Äúgood‚Äù criteria for variable selection? Explain your reason in two sentences.\nQuestion 19: (4 pts.) Please state one possible violation of normal linear regression model assumptions for each of the residual plots below (one sentence for each plot):\n\n\n\n\n\n\n\n\n\nQuestion 20: (4 pts.) Suppose you want to build a linear regression for response variable weight ( \\(Y\\) ) using covariate height ( \\(X_1\\) ) and gender ( \\(X_2 = 0\\) for female and \\(X_2 = 1\\) for male).\n\nSuppose you want to allow the slope (of height) to be the same for different gender but different intercept, how would you build the linear regression model? Please specify the model in one line.\nSuppose you want to allow both slope (of height) and intercept to be different for different gender, how would you build the linear regression model? Please specify the model in one line.\n\nQuestion 21: (2 pts.) An engineer has stated: ‚ÄúReduction of the number of candidate explanatory variables should always be done using the objective forward stepwise regression procedure.‚Äù Discuss.\nQuestion 22: (4 pts.) A junior investment analyst used a polynomial regression model of relatively high order in a research seminar on municipal bonds and obtained an \\(R^2\\) of 0.991 in the regression of net interest yield of bond ( \\(Y\\) ) on industrial diversity index of municipality ( \\(X\\) ) for seven bond issues. A colleague, unimpressed, said: ‚ÄúYou overfitted. Your curve follows the random effects in the data.‚Äù Comment on the colleague‚Äôs criticism.\nQuestion 23: (4 pts.) In a regression study of factors affecting learning time for a certain task (measured in minutes), gender of learner was included as a predictor variable (\\(X_2\\)) that was coded \\(X_2 = 1\\) if male and \\(X_2 = 0\\) if female. It was found that the estimated coefficient of \\(X_2\\) was \\(\\widehat{\\beta}_2 = 22.3\\) with a standard error of 3.8. An observer questioned whether the coding scheme for gender is fair because it results in a positive coefficient, leading to longer learning times for males than females. Comment.\nQuestion 24: (2 pts.) A student stated: ‚ÄúAdding predictor variables to a regression model can never reduce \\(R^2\\), so we should include all available predictor variables in the model.‚Äù Comment.\nQuestion 25: (2 pts.) Evaluate the following statement: ‚ÄúFor the least squares method to be fully valid, it is required that the distribution of \\(Y\\) be normal.‚Äù\nQuestion 26: (2 pts.) What is a residual? Why are residuals important in regression analysis?\nQuestion 27: (3 pts.) Explain the purpose of \\(k\\)-fold cross-validation in regression modeling. Why is it generally preferred over a single train-test split for assessing model performance? (2-3 sentences)\nQuestion 28: (3 pts.) Consider a dataset where the response variable \\(Y\\) is binary (0 or 1). Explain why ordinary linear regression is not appropriate for modeling this relationship, and what problem arises if you try to use it. (2-3 sentences)\nQuestion 29 (Bonus): (5 pts.) An analyst wanted to fit the regression model \\(Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\dots + \\beta_p X_{p-1, i} + \\epsilon_i\\) by the method of least squares when it is known that \\(\\beta_2 = 4\\). How can the analyst obtain the desired fit using standard statistical software (e.g., R, Python, or SAS)? No need to run any code, just describe in general how you could accomplish this.",
    "crumbs": [
      "Assignments",
      "BANA 7042: Assignment 1"
    ]
  },
  {
    "objectID": "assignments/final-exam.html",
    "href": "assignments/final-exam.html",
    "title": "BANA 7042: Final Exam",
    "section": "",
    "text": "Note: Please be sure to properly label all figures and include a caption for each!\nBefore answering the following questions, please do the following:",
    "crumbs": [
      "Assignments",
      "BANA 7042: Final Exam"
    ]
  },
  {
    "objectID": "assignments/final-exam.html#questions",
    "href": "assignments/final-exam.html#questions",
    "title": "BANA 7042: Final Exam",
    "section": "Questions",
    "text": "Questions\nThe winequality.csv data set includes data on both red and white wines; for this analysis, you‚Äôll only be focusing on the red wines. The response variable of interest is quality.\n\nSubset the data to only include observations on red wines. Describe the distribution of the response. Based on the nature of the response variable, what type of regression model would you suggest starting with (and why)?\nExplore the data using summary statistics and graphics. Do any of the variables appear to have an association with the overal quality score? If so, which? Describe the nature of these associations.\nConstruct a binary response according to the following rule \\[\nY = \\begin{cases}\n  1, \\quad \\texttt{quality} \\ge 7 \\\\\n  0, \\quad \\texttt{quality} &lt; 7\n\\end{cases}\n\\]\n\nFit a logistic regression to the data using all possible main effects (i.e., include each variable, but no interaction effects). Assess the performance of this model. Does the model seem well-calibrated? Discuss and provide a plot of a calibration curve.\n\nInterpret the effect of the predictor alcohol on the odds that quality &gt;= 7. Construct an effect plot visualizing the effect of alcohol on the probability that quality &gt;= 7 and describe the relationship. Does this plot look linear or nonlinear? If nonlinear, discuss how this is possible.\nDiscuss reasons why the modeling approach used in 3) is ill-advised for modeling these data.\nFit an ordinal regression model to the data using the original response (i.e., quality) using the orm() function from R package rms. Construct an effect plot for each predictor showing the effect on the predicted probability that quality &gt;= 7. From these, try to determine the top three predictors solely in terms of their effect on the predicted probability that quality &gt;= 7.\nConsider a single observation \\(x_0\\) (i.e., a single red wine) with the following characteristics:\n\n\n\n                            \nfixed.acidity         7.3000\nvolatile.acidity      0.6500\ncitric.acid           0.0000\nresidual.sugar        1.2000\nchlorides             0.0650\nfree.sulfur.dioxide  15.0000\ntotal.sulfur.dioxide 21.0000\ndensity               0.9946\npH                    3.3900\nsulphates             0.4700\nalcohol              10.0000\n\n\nBased on your fitted model from 6), provide estimates for the following quantities:\n\n\\(\\mathrm{P}\\left(\\texttt{quality} = 7 | x_0\\right)\\)\n\\(\\mathrm{P}\\left(\\texttt{quality} \\ge 7 | x_0\\right)\\)\n\\(\\mathrm{P}\\left(\\texttt{quality} = 9 | x_0\\right)\\)\n\\(\\mathrm{P}\\left(\\texttt{quality} \\le 9 | x_0\\right)\\)\n\n\nYou‚Äôre asked to use your model from part 6) to provide predictions for the white wines included in the original sample. Discuss whether or not you think this is a reasonable request and why. What would you do in practice (e.g., what if this was your boss asking)?",
    "crumbs": [
      "Assignments",
      "BANA 7042: Final Exam"
    ]
  },
  {
    "objectID": "code/lecture-1.html",
    "href": "code/lecture-1.html",
    "title": "BANA 7042",
    "section": "",
    "text": "# Print first few rows of Ames housing data\names &lt;- AmesHousing::make_ames()  # install.packages(\"AmesHousing\")\names$Sale_Price &lt;- ames$Sale_Price / 10000\nhead(ames)\n\n\nA tibble: 6 √ó 81\n\n\nMS_SubClass\nMS_Zoning\nLot_Frontage\nLot_Area\nStreet\nAlley\nLot_Shape\nLand_Contour\nUtilities\nLot_Config\n‚ãØ\nFence\nMisc_Feature\nMisc_Val\nMo_Sold\nYear_Sold\nSale_Type\nSale_Condition\nSale_Price\nLongitude\nLatitude\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n‚ãØ\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nOne_Story_1946_and_Newer_All_Styles\nResidential_Low_Density\n141\n31770\nPave\nNo_Alley_Access\nSlightly_Irregular\nLvl\nAllPub\nCorner\n‚ãØ\nNo_Fence\nNone\n0\n5\n2010\nWD\nNormal\n21.50\n-93.61975\n42.05403\n\n\nOne_Story_1946_and_Newer_All_Styles\nResidential_High_Density\n80\n11622\nPave\nNo_Alley_Access\nRegular\nLvl\nAllPub\nInside\n‚ãØ\nMinimum_Privacy\nNone\n0\n6\n2010\nWD\nNormal\n10.50\n-93.61976\n42.05301\n\n\nOne_Story_1946_and_Newer_All_Styles\nResidential_Low_Density\n81\n14267\nPave\nNo_Alley_Access\nSlightly_Irregular\nLvl\nAllPub\nCorner\n‚ãØ\nNo_Fence\nGar2\n12500\n6\n2010\nWD\nNormal\n17.20\n-93.61939\n42.05266\n\n\nOne_Story_1946_and_Newer_All_Styles\nResidential_Low_Density\n93\n11160\nPave\nNo_Alley_Access\nRegular\nLvl\nAllPub\nCorner\n‚ãØ\nNo_Fence\nNone\n0\n4\n2010\nWD\nNormal\n24.40\n-93.61732\n42.05125\n\n\nTwo_Story_1946_and_Newer\nResidential_Low_Density\n74\n13830\nPave\nNo_Alley_Access\nSlightly_Irregular\nLvl\nAllPub\nInside\n‚ãØ\nMinimum_Privacy\nNone\n0\n3\n2010\nWD\nNormal\n18.99\n-93.63893\n42.06090\n\n\nTwo_Story_1946_and_Newer\nResidential_Low_Density\n78\n9978\nPave\nNo_Alley_Access\nSlightly_Irregular\nLvl\nAllPub\nInside\n‚ãØ\nNo_Fence\nNone\n0\n6\n2010\nWD\nNormal\n19.55\n-93.63893\n42.06078\n\n\n\n\n\n\nhist(ames$Sale_Price, br = 50, xlab = \"Sale price ($)\", freq = FALSE, main = \"\", las = 1)\n\n\n\n\n\n\n\n\n\nplot(ecdf(ames$Sale_Price), xlab = \"Sale price ($)\", main = \"\",\n     col = adjustcolor(1, alpha.f = 0.1), las = 1)"
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#multinomial-data",
    "href": "slides/05-multinomial-ordinal-regression.html#multinomial-data",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "Multinomial data",
    "text": "Multinomial data\n\nThe multinomial distribution is an extension of the binomial where the outcome can take on more than two values\nThe categories can be nominal (e.g., have no natural ordering) or ordinal in nature (e.g., low &lt; medium &lt; high)\nWe‚Äôll start with the case of a nominal outcome having more than two categories\nLet \\(Y\\) be a discrete r.v. that can take on one of \\(J\\) categories with \\(\\left\\{\\mathrm{P}\\left(Y = j\\right) = p_j\\right\\}_{j=1}^J\\), where \\(\\sum_{j=1}^Jp_j=1\\)",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#multinomial-logit-model",
    "href": "slides/05-multinomial-ordinal-regression.html#multinomial-logit-model",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "Multinomial logit model",
    "text": "Multinomial logit model\n\nSimilar to binary regression, we need a way to link the probabilities \\(p_j\\) to the predictors \\(\\boldsymbol{x} = \\left(x_1, x_2, \\dots, x_p\\right)^\\top\\)\nNeed to ensure each \\(0 \\le p_j \\le 1\\) and that \\(\\sum_{j=1}^Jp_j = 1\\)\nIdea is similar to fitting several logistic regressions using one of the categories as the reference or baseline (say, \\(j = 1\\))\nTo that end, we define the \\(J-1\\) logits \\(\\eta_j = \\boldsymbol{x}^\\top\\boldsymbol{\\beta}_j = \\log\\left(p_j / p_1\\right)\\), where \\(j \\ne 1\\)\nNotice we have a set of coefficients for each comparison!",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#multinomial-logit-model-1",
    "href": "slides/05-multinomial-ordinal-regression.html#multinomial-logit-model-1",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "Multinomial logit model",
    "text": "Multinomial logit model\n\nTo ensure \\(\\sum_{j=1}^Jp_j = 1\\), we have \\[\np_i = \\frac{\\exp\\left(\\eta_i\\right)}{1 + \\sum_{j=2}^J\\exp\\left(\\eta_j\\right)}\n\\]\nThis implies that \\(p_1 = 1 - \\sum_{j=2}^Jp_j\\)",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-1",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-1",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\n\nWe‚Äôll consider a sample taken from a subset of the 1996 American National Election Study\nContained in the nes96 data frame in the faraway üì¶\n\n\n\nShow R code\nstr(nes96 &lt;- faraway::nes96)\n\n\n'data.frame':   944 obs. of  10 variables:\n $ popul : int  0 190 31 83 640 110 100 31 180 2800 ...\n $ TVnews: int  7 1 7 4 7 3 7 1 7 0 ...\n $ selfLR: Ord.factor w/ 7 levels \"extLib\"&lt;\"Lib\"&lt;..: 7 3 2 3 5 3 5 5 4 3 ...\n $ ClinLR: Ord.factor w/ 7 levels \"extLib\"&lt;\"Lib\"&lt;..: 1 3 2 4 6 4 6 4 6 3 ...\n $ DoleLR: Ord.factor w/ 7 levels \"extLib\"&lt;\"Lib\"&lt;..: 6 5 6 5 4 6 4 5 3 7 ...\n $ PID   : Ord.factor w/ 7 levels \"strDem\"&lt;\"weakDem\"&lt;..: 7 2 2 2 1 2 2 5 4 1 ...\n $ age   : int  36 20 24 28 68 21 77 21 31 39 ...\n $ educ  : Ord.factor w/ 7 levels \"MS\"&lt;\"HSdrop\"&lt;..: 3 4 6 6 6 4 4 4 4 3 ...\n $ income: Ord.factor w/ 24 levels \"$3Kminus\"&lt;\"$3K-$5K\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ vote  : Factor w/ 2 levels \"Clinton\",\"Dole\": 2 1 1 1 1 1 1 1 1 1 ...",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-2",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-2",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\n\nFor simplicity, we‚Äôll consider three variables: age, education level, and income groups of each respondent\nSome of the factors are ‚Äúordered‚Äù by default; E.g., income (respondent‚Äôs education level):\n\n\nlevels(nes96$income)\n\n [1] \"$3Kminus\"   \"$3K-$5K\"    \"$5K-$7K\"    \"$7K-$9K\"    \"$9K-$10K\"  \n [6] \"$10K-$11K\"  \"$11K-$12K\"  \"$12K-$13K\"  \"$13K-$14K\"  \"$14K-$15K\" \n[11] \"$15K-$17K\"  \"$17K-$20K\"  \"$20K-$22K\"  \"$22K-$25K\"  \"$25K-$30K\" \n[16] \"$30K-$35K\"  \"$35K-$40K\"  \"$40K-$45K\"  \"$45K-$50K\"  \"$50K-$60K\" \n[21] \"$60K-$75K\"  \"$75K-$90K\"  \"$90K-$105K\" \"$105Kplus\" \n\n\n\nSame goes for PID and educ (respondent‚Äôs party identification and education, respectively)",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-3",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-3",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nHere‚Äôs a cleaned up version of the data we‚Äôll work with:\n\n\nShow R code\n# Condense party identification (PID) column into three categories\nparty &lt;- nes96$PID\nlevels(party) &lt;- c(\n  \"Democrat\", \"Democrat\",\n  \"Independent\", \"Independent\", \"Independent\", \n  \"Republican\", \"Republican\"\n)\n\n# Convert income to numeric\ninca &lt;- c(1.5, 4, 6, 8, 9.5, 10.5, 11.5, 12.5, 13.5, 14.5, 16, 18.5, 21, 23.5,\n          27.5, 32.5, 37.5, 42.5, 47.5, 55, 67.5, 82.5, 97.5, 115)\nincome &lt;- inca[unclass(nes96$income)]\n\n# Construct new data set for analysis\nrnes96 &lt;- data.frame(\n  \"party\" = party, \n  \"income\" = income, \n  \"education\" = nes96$educ, \n  \"age\" = nes96$age\n)\n\n# Print summary of data set\nsummary(rnes96)\n\n\n         party         income        education        age       \n Democrat   :380   Min.   :  1.50   MS    : 13   Min.   :19.00  \n Independent:239   1st Qu.: 23.50   HSdrop: 52   1st Qu.:34.00  \n Republican :325   Median : 37.50   HS    :248   Median :44.00  \n                   Mean   : 46.58   Coll  :187   Mean   :47.04  \n                   3rd Qu.: 67.50   CCdeg : 90   3rd Qu.:58.00  \n                   Max.   :115.00   BAdeg :227   Max.   :91.00  \n                                    MAdeg :127",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-4",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-4",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nLet‚Äôs continue with some visual exploration:\n\n\nShow R code\nlibrary(dplyr)\nlibrary(ggplot2)\n\ntheme_set(theme_bw())\n\n# Aggregate data; what's happening here?\negp &lt;- group_by(rnes96, education, party) %&gt;% \n  summarise(count = n()) %&gt;%\n  group_by(education) %&gt;% \n  mutate(etotal = sum(count), proportion = count/etotal)\n\n# Plot results\nggplot(egp, aes(x = education, y = proportion, group = party, \n                linetype = party, color = party)) + \n  geom_line(size = 2)",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-5",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-5",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nLet‚Äôs continue with some visual exploration:\n\n\nShow R code\n# Aggregate data; what's happening here?\nigp &lt;- mutate(rnes96, income_group = cut_number(income, 7)) %&gt;% \n  group_by(income_group, party) %&gt;% \n  summarise(count = n()) %&gt;% \n  group_by(income_group) %&gt;% \n  mutate(etotal = sum(count), proportion = count / etotal)\n\n# Plot results\nggplot(igp, aes(x = income_group, y = proportion, group = party, \n                linetype = party, color = party)) +\n  scale_x_discrete(guide = guide_axis(angle = 45)) +\n  geom_line(size = 2)",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-6",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-6",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nLet‚Äôs continue with some visual exploration:\n\n\nShow R code\n# Aggregate data; what's happening here?\nagp &lt;- rnes96 %&gt;% \n  group_by(age, party) %&gt;% \n  summarise(count = n()) %&gt;% \n  group_by(age) %&gt;% \n  mutate(etotal = sum(count), proportion = count / etotal)\n\n# Plot results\nggplot(agp, aes(x = age, y = proportion, group = party, \n                linetype = party, color = party)) +\n  geom_line(size = 1, alpha = 0.5)",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-7",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-7",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nDefine the following probabilities:\n\n\\(p_{d} = P\\left(\\text{voting democrat}\\right)\\);\n\\(p_{i} = P\\left(\\text{voting independent}\\right)\\);\n\\(p_{r} = P\\left(\\text{voting republican}\\right)\\),\n\nwhere \\(p_d + p_i + p_r = 1\\). Assume for now that income is the only independent variable of interest.",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-8",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-8",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nThe multinomial logit model effectively fits several logits (one for every class except the baseline, which is arbitrary; here, it‚Äôs democrat):\n\n\\(\\log\\left(p_{i} / p_{d}\\right) = \\beta_0 + \\beta_1 \\mathtt{income}\\quad\\) (log odds of voting independent vs.¬†democrat);\n\\(\\log\\left(p_{r} / p_{d}\\right) = \\alpha_0 + \\alpha_1 \\mathtt{income}\\quad\\) (log odds of voting republican vs.¬†democrat).\n\nHere we use \\(\\beta_i\\) and \\(\\alpha_i\\) to remind us that the estimated coefficients between the two logits will be different.",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-9",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-9",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nMultinomial logit model using all three predictors:\n\n\nShow R code\nlibrary(nnet)  # install.packages(\"nnet\")\n\n(mfit &lt;- multinom(party ~ age + education + income, data = rnes96, trace = FALSE))\n\n\nCall:\nmultinom(formula = party ~ age + education + income, data = rnes96, \n    trace = FALSE)\n\nCoefficients:\n            (Intercept)          age education.L education.Q education.C\nIndependent   -1.197260 0.0001534525  0.06351451  -0.1217038   0.1119542\nRepublican    -1.642656 0.0081943691  1.19413345  -1.2292869   0.1544575\n            education^4 education^5 education^6     income\nIndependent -0.07657336   0.1360851  0.15427826 0.01623911\nRepublican  -0.02827297  -0.1221176 -0.03741389 0.01724679\n\nResidual Deviance: 1968.333 \nAIC: 2004.333",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#brief-digression",
    "href": "slides/05-multinomial-ordinal-regression.html#brief-digression",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "Brief digression‚Ä¶",
    "text": "Brief digression‚Ä¶\n\nBy default, R encodes ordered factors using orthogonal polynomials\nAmes Housing example:\n\n\n\nShow R code\names &lt;- AmesHousing::make_ames()\nggplot(ames, aes(x = Overall_Qual, y = Sale_Price)) +#log(Sale_Price))) +\n  geom_boxplot(aes(color = Overall_Qual)) +\n  scale_x_discrete(guide = guide_axis(angle = 45))",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-10",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-10",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nNo \\(p\\)-values here!\n\n\nShow R code\nsummary(mfit)\n\n\nCall:\nmultinom(formula = party ~ age + education + income, data = rnes96, \n    trace = FALSE)\n\nCoefficients:\n            (Intercept)          age education.L education.Q education.C\nIndependent   -1.197260 0.0001534525  0.06351451  -0.1217038   0.1119542\nRepublican    -1.642656 0.0081943691  1.19413345  -1.2292869   0.1544575\n            education^4 education^5 education^6     income\nIndependent -0.07657336   0.1360851  0.15427826 0.01623911\nRepublican  -0.02827297  -0.1221176 -0.03741389 0.01724679\n\nStd. Errors:\n            (Intercept)         age education.L education.Q education.C\nIndependent   0.3265951 0.005374592   0.4571884   0.4142859   0.3498491\nRepublican    0.3312877 0.004902668   0.6502670   0.6041924   0.4866432\n            education^4 education^5 education^6      income\nIndependent   0.2883031   0.2494706   0.2171578 0.003108585\nRepublican    0.3605620   0.2696036   0.2031859 0.002881745\n\nResidual Deviance: 1968.333 \nAIC: 2004.333",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-11",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-11",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\n\nHow do we interpret the coefficients? How about for income?\nAll else held constant, for every ‚ö†Ô∏èone-unit increase in income‚ö†Ô∏è, the multinomial log odds of voting republican, relative to democrat increase by 0.017.\nGross‚Ä¶ ü§¢\nEffect plots to the rescue!",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-12",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-12",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nLook at predicted probabilities:\n\nhead(probs &lt;- predict(mfit, type = \"probs\"))\n\n   Democrat Independent Republican\n1 0.5923052   0.1975326  0.2101622\n2 0.5919378   0.1687055  0.2393567\n3 0.5970789   0.1732058  0.2297154\n4 0.5924809   0.1719775  0.2355417\n5 0.5423563   0.1583973  0.2992464\n6 0.5907590   0.1683954  0.2408456\n\n# Sanity check\nhead(rowSums(probs))\n\n1 2 3 4 5 6 \n1 1 1 1 1 1",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-13",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-13",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\n\n\nShow R code\nlibrary(pdp)  # for partial dependence (PD) plots\n\n# Compute partial dependence of party identification on income\npfun &lt;- function(object, newdata) {\n  probs &lt;- predict(object, newdata = newdata, type = \"probs\")\n  colMeans(probs)  # return average probability for each class\n}\npd.inc &lt;- partial(mfit, pred.var = \"income\", pred.fun = pfun)\nggplot(pd.inc, aes(x = income, y = yhat, linetype = yhat.id, color = yhat.id)) +\n  geom_line(size = 2) +\n  xlab(\"Income group midpoint (in thousands)\") +\n  ylab(\"Partial dependence\")",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-14",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-14",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nCan perform classification, if desired‚Ä¶üôÑ\n\n\nShow R code\ntable(\"Predicted\" = predict(mfit), \"Actual\" = rnes96$party)\n\n\n             Actual\nPredicted     Democrat Independent Republican\n  Democrat         277         130        169\n  Independent        4           7          5\n  Republican        99         102        151\n\n\n\nUmmm‚Ä¶majority of actual Republicans are classified as Democrats?!",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-15",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-15",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nFor kicks, try stepwise selection; since the model is based on a (multinomial) likelihood, the AIC/BIC are well-defined and the usual stepwise procedures are still valid:\n\n\nShow R code\n(mfit2 &lt;- MASS::stepAIC(mfit, direction = \"both\", scope = list(\"upper\" = ~.^2)))\n\n\nStart:  AIC=2004.33\nparty ~ age + education + income\n\n                   Df    AIC\n- education        12 1996.5\n- age               2 2003.6\n&lt;none&gt;                2004.3\n+ age:income        2 2006.7\n+ age:education    12 2009.3\n+ education:income 12 2013.2\n- income            2 2045.9\n\nStep:  AIC=1996.54\nparty ~ age + income\n\n             Df    AIC\n- age         2 1993.4\n&lt;none&gt;          1996.5\n+ age:income  2 1998.8\n+ education  12 2004.3\n- income      2 2048.8\n\nStep:  AIC=1993.42\nparty ~ income\n\n            Df    AIC\n&lt;none&gt;         1993.4\n+ age        2 1996.5\n+ education 12 2003.6\n- income     2 2045.3\n\n\nCall:\nmultinom(formula = party ~ income, data = rnes96, trace = FALSE)\n\nCoefficients:\n            (Intercept)     income\nIndependent  -1.1749331 0.01608683\nRepublican   -0.9503591 0.01766457\n\nResidual Deviance: 1985.424 \nAIC: 1993.424",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-16",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-16",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nFor comparison, fit a (default) random forest:\n\n\nShow R code\nset.seed(2008)  # for reproducibility\n(rfo &lt;- randomForest::randomForest(party ~ ., data = rnes96, ntree = 1000))\n\n\n\nCall:\n randomForest(formula = party ~ ., data = rnes96, ntree = 1000) \n               Type of random forest: classification\n                     Number of trees: 1000\nNo. of variables tried at each split: 1\n\n        OOB estimate of  error rate: 55.72%\nConfusion matrix:\n            Democrat Independent Republican class.error\nDemocrat         248          41         91   0.3473684\nIndependent      129          27         83   0.8870293\nRepublican       156          26        143   0.5600000",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-17",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-17",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nRandom forest results for comparison:\n\n\nShow R code\n# Construct the same PD plot as before, but using the RF model\npd &lt;- partial(rfo, pred.var = \"income\", pred.fun = function(object, newdata) {\n  colMeans(predict(object, newdata = newdata, type = \"prob\"))\n})\nggplot(pd, aes(x = income, y = yhat, linetype = yhat.id, color = yhat.id)) +\n  geom_line(size = 2) +\n  xlab(\"Income (midpoint in thousands)\") +\n  ylab(\"Partial dependence\") +\n  geom_rug(data = data.frame(\"income\" = quantile(rnes96$income, prob = 1:9/10)), aes(x = income), inherit.aes = FALSE)",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#ordinal-outcomes-1",
    "href": "slides/05-multinomial-ordinal-regression.html#ordinal-outcomes-1",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "Ordinal outcomes",
    "text": "Ordinal outcomes\n\nIf \\(Z - \\boldsymbol{x}^\\top\\boldsymbol{\\beta}\\) has distribution \\(F\\), then \\[\n\\mathrm{P}\\left(Y \\le j\\right) = \\mathrm{P}\\left(Z \\le \\alpha_j\\right) = F\\left(\\alpha_j - \\boldsymbol{x}^\\top\\boldsymbol{\\beta}\\right)\n\\]\nIf, for example, \\(F\\) is a standard logistic distribution, then \\[\np_j^\\le = \\frac{\\exp\\left(\\alpha_j - \\boldsymbol{x}^\\top\\boldsymbol{\\beta}\\right)}{1 + \\exp\\left(\\alpha_j - \\boldsymbol{x}^\\top\\boldsymbol{\\beta}\\right)}\n\\] which is a logit model for the cumulative probabilities!\nChoosing a standard normal distribution for \\(F\\) would lead to a probit model for the cumulative probabilities, and so on‚Ä¶",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#proportional-odds-po-model",
    "href": "slides/05-multinomial-ordinal-regression.html#proportional-odds-po-model",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "Proportional odds (PO) model",
    "text": "Proportional odds (PO) model\n\nLet \\(p_j^\\top = \\mathrm{P}\\left(Y \\le j|\\boldsymbol{x}\\right)\\)\nThe standard PO model, which uses a logit link, is \\[\n\\log\\left(\\frac{p_j^\\le}{1-p_j^\\le}\\right) = \\alpha_j - \\boldsymbol{x}^\\top\\boldsymbol{\\beta}, \\quad j = 1, 2, \\dots, J-1\n\\]",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#proportional-odds-po-model-1",
    "href": "slides/05-multinomial-ordinal-regression.html#proportional-odds-po-model-1",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "Proportional odds (PO) model",
    "text": "Proportional odds (PO) model\n\nPO assumption, etc‚Ä¶",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-18",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-18",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nThe simplest implementation is polr() from MASS\n\n\nShow R code\nlibrary(MASS)\n\n(pofit &lt;- polr(party ~ age + education + income, data = rnes96))\n\n\nCall:\npolr(formula = party ~ age + education + income, data = rnes96)\n\nCoefficients:\n         age  education.L  education.Q  education.C  education^4  education^5 \n 0.005774902  0.724086814 -0.781360508  0.040168238 -0.019925492 -0.079412657 \n education^6       income \n-0.061103738  0.012738693 \n\nIntercepts:\n  Democrat|Independent Independent|Republican \n             0.6448794              1.7373541 \n\nResidual Deviance: 1984.211 \nAIC: 2004.211",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-19",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-19",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nSimilar to before, we can use AIC-based stepwise procedures:\n\n\nShow R code\n(pofit2 &lt;- stepAIC(pofit, direction = \"both\"))\n\n\nStart:  AIC=2004.21\nparty ~ age + education + income\n\n            Df    AIC\n- education  6 2002.8\n&lt;none&gt;         2004.2\n- age        1 2004.4\n- income     1 2038.6\n\nStep:  AIC=2002.83\nparty ~ age + income\n\n            Df    AIC\n- age        1 2001.4\n&lt;none&gt;         2002.8\n+ education  6 2004.2\n- income     1 2047.2\n\nStep:  AIC=2001.36\nparty ~ income\n\n            Df    AIC\n&lt;none&gt;         2001.4\n+ age        1 2002.8\n+ education  6 2004.4\n- income     1 2045.3\n\n\nCall:\npolr(formula = party ~ income, data = rnes96)\n\nCoefficients:\n    income \n0.01311984 \n\nIntercepts:\n  Democrat|Independent Independent|Republican \n             0.2091045              1.2915566 \n\nResidual Deviance: 1995.363 \nAIC: 2001.363",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-20",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-20",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\n\ncbind(\n  \"deviance\" = c(\"PO\" = deviance(pofit2), \"Multi\" = deviance(mfit2)),\n  \"nparam\" = c(\"PO\" = pofit2$edf, \"Multi\" = mfit2$edf)\n)\n\n      deviance nparam\nPO    1995.363      3\nMulti 1985.424      4",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-21",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-21",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nCompare full and reduced model using LR test\n\nanova(pofit2, pofit, test = \"Chisq\")",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-22",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-22",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nInterpreting the coefficients:\n\nsummary(pofit2)\n\nCall:\npolr(formula = party ~ income, data = rnes96)\n\nCoefficients:\n         Value Std. Error t value\nincome 0.01312   0.001971   6.657\n\nIntercepts:\n                       Value   Std. Error t value\nDemocrat|Independent    0.2091  0.1123     1.8627\nIndependent|Republican  1.2916  0.1201    10.7526\n\nResidual Deviance: 1995.363 \nAIC: 2001.363 \n\n\nWe can say that the odds of moving from Democrat to Independent/Republican (or from Democrat/Independent to Republican) increase by a factor of \\(\\exp\\left(0.013120\\right) = 1.0132\\) per unit increase in income.",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#non-proportional-odds-npo-model",
    "href": "slides/05-multinomial-ordinal-regression.html#non-proportional-odds-npo-model",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "Non-proportional odds (NPO) model",
    "text": "Non-proportional odds (NPO) model\n\nWe can generalize the PO model by allowing the coefficients to vary between categories (similar to the mulinomial logit model form earlier): \\[\n\\log\\left(\\frac{p_j^\\le}{1-p_j^\\le}\\right) = \\alpha_j - \\boldsymbol{x}^\\top\\boldsymbol{\\beta}_j, \\quad j = 1, 2, \\dots, J-1\n\\]\nThis relaxes the PO assumptions but requires more complicated software (e.g., the VGAM package)\nCareful, different packages use different default parameterizations; E.g., see Table 2 here",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-23",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-23",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nPO and NPO fits to election data using VGAM:\n\n\nShow R code\nlibrary(VGAM)\n\n(pofit &lt;- vglm(party ~ income, data = rnes96,\n               family = cumulative(parallel = TRUE, reverse = TRUE)))\n\n\n\nCall:\nvglm(formula = party ~ income, family = cumulative(parallel = TRUE, \n    reverse = TRUE), data = rnes96)\n\n\nCoefficients:\n(Intercept):1 (Intercept):2        income \n  -0.20910243   -1.29155469    0.01311978 \n\nDegrees of Freedom: 1888 Total; 1885 Residual\nResidual deviance: 1995.363 \nLog-likelihood: -997.6813 \n\n\nShow R code\n(npofit &lt;- vglm(party ~ income, data = rnes96,\n                family = cumulative(parallel = FALSE, reverse = TRUE)))\n\n\n\nCall:\nvglm(formula = party ~ income, family = cumulative(parallel = FALSE, \n    reverse = TRUE), data = rnes96)\n\n\nCoefficients:\n(Intercept):1 (Intercept):2      income:1      income:2 \n  -0.32886794   -1.14826963    0.01618611    0.01048588 \n\nDegrees of Freedom: 1888 Total; 1884 Residual\nResidual deviance: 1987.539 \nLog-likelihood: -993.7693",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#assigning-scores",
    "href": "slides/05-multinomial-ordinal-regression.html#assigning-scores",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "Assigning scores",
    "text": "Assigning scores\n\nWhen the ordinal response has a larger number of categories, it may be reasonable to assign scores (i.e., integers) to each level and then model these scores using a standard linear model\nRule of thumb from my old advisor was 10 or more categories",
    "crumbs": [
      "Slides",
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#count-data",
    "href": "slides/06-count-regression.html#count-data",
    "title": "Regression for Counts and Rates",
    "section": "Count data",
    "text": "Count data\n\nA type of data in which the observations take on non-negative integer values \\(\\left\\{0, 1, 2, \\dots\\right\\}\\)\nExamples include:\n\nThe number of patients who come to the ER of Children‚Äôs Hospital between 9PM and 1AM.\nThe number of shoppers in Kenwood Towne Centre on a calendar day.\nThe number of Google searches (in a week) for flights to Shanghai right before Lunar New Year.",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-poisson-distribution",
    "href": "slides/06-count-regression.html#the-poisson-distribution",
    "title": "Regression for Counts and Rates",
    "section": "The Poisson distribution",
    "text": "The Poisson distribution\n\nThe simplest distribution for modeling counts is the Poisson distribution:\n\n\\(Y \\sim \\mathrm{Poi}\\left(\\mu\\right)\\)\n\\(f\\left(y\\right) = \\mathrm{P}\\left(Y = y\\right) = \\frac{\\exp\\left(-\\mu\\right)\\mu^y}{y!}\\), for \\(y = 0, 1, 2, \\dots\\)\n\\(\\mathrm{E}\\left(Y\\right) = \\mathrm{Var}\\left(Y\\right) = \\mu\\), where \\(\\mu \\in \\left(0, \\infty\\right)\\)\nFact: \\(\\sum_i Y_i \\sim \\mathrm{Poi}\\left(\\sum_i \\mu_i\\right)\\) (aggregated data)\n\nAn interesting characteristic of the Poisson is that the mean and variance are equal to each other",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-poisson-distribution-1",
    "href": "slides/06-count-regression.html#the-poisson-distribution-1",
    "title": "Regression for Counts and Rates",
    "section": "The Poisson distribution",
    "text": "The Poisson distribution\n\n\nShow R code\n# Install required package(s)\npkgs &lt;- c(\"faraway\", \"investr\", \"mgcv\", \"performance\", \"pscl\")\nlib &lt;- installed.packages()[, \"Package\"]\ninstall.packages(setdiff(pkgs, lib))\n\n# Y ~ Poisson(lambda = 0.5)\nset.seed(2004)  # for reproducibility\npar(mfrow = c(2, 2))\nfor (lambda in c(0.5, 2, 5, 15)) {\n  y &lt;- dpois(0:35, lambda = lambda)\n  barplot(y, xlab = \"y\", ylab = \"P(Y = y)\", names = 0:35, main = paste(\"E(Y) =\", lambda), \n          col = \"dodgerblue2\", border = \"dodgerblue2\", las = 1)\n}",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-poisson-distribution-2",
    "href": "slides/06-count-regression.html#the-poisson-distribution-2",
    "title": "Regression for Counts and Rates",
    "section": "The Poisson distribution",
    "text": "The Poisson distribution\n\n\nShow R code\ny &lt;- rpois(10000, lambda = 200)\nhist(y, br = 50)",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-poisson-distribution-3",
    "href": "slides/06-count-regression.html#the-poisson-distribution-3",
    "title": "Regression for Counts and Rates",
    "section": "The Poisson distribution",
    "text": "The Poisson distribution\n\nIf the count is some number out of some possible total, then the response would be more appropriately modeled as a binomial r.v.\nHowever, for small \\(p\\) and large \\(n\\), the Poisson distribution provides a reasonable approximation to the binomial; For example, in modeling the incidence of rare forms of cancer, the number of people affected is a small proportion of the population in a given geographical area\n\n\n\nShow R code\nc(\"Binomial\" = pbinom(5, size = 8, p = 0.7),\n  \"Poisson\" = ppois(5, lambda = 8 * 0.7))\n\n\n Binomial   Poisson \n0.4482262 0.5118609 \n\n\nShow R code\nc(\"Binomial\" = pbinom(5, size = 100, p = 0.05),\n  \"Poisson\" = ppois(5, lambda = 100 * 0.05))\n\n\n Binomial   Poisson \n0.6159991 0.6159607",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\nThere are 30 Galapagos islands and 7 variables in the data set. The relationship between the number of plant species (\\(Y\\)) and several geographic variables is of interest. The original data set contained several missing values which have been filled for convenience. See the faraway::galamiss data set for the original version.",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-1",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-1",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-2",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-2",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\nWe‚Äôll remove Endemics since we won‚Äôt be using it!\n\n\nShow R code\n# Load the Galapagos Islands data\ndata(gala, package = \"faraway\")\ngala$Endemics &lt;- NULL\n\n# Print structure of data frame\nstr(gala)\n\n\n'data.frame':   30 obs. of  6 variables:\n $ Species  : num  58 31 3 25 2 18 24 10 8 2 ...\n $ Area     : num  25.09 1.24 0.21 0.1 0.05 ...\n $ Elevation: num  346 109 114 46 77 119 93 168 71 112 ...\n $ Nearest  : num  0.6 0.6 2.8 1.9 1.9 8 6 34.1 0.4 2.6 ...\n $ Scruz    : num  0.6 26.3 58.7 47.4 1.9 ...\n $ Adjacent : num  1.84 572.33 0.78 0.18 903.82 ...",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-3",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-3",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\nSummary of data frame\n\n\nShow R code\nsummary(gala)\n\n\n    Species            Area             Elevation          Nearest     \n Min.   :  2.00   Min.   :   0.0100   Min.   :  25.00   Min.   : 0.20  \n 1st Qu.: 13.00   1st Qu.:   0.2575   1st Qu.:  97.75   1st Qu.: 0.80  \n Median : 42.00   Median :   2.5900   Median : 192.00   Median : 3.05  \n Mean   : 85.23   Mean   : 261.7087   Mean   : 368.03   Mean   :10.06  \n 3rd Qu.: 96.00   3rd Qu.:  59.2375   3rd Qu.: 435.25   3rd Qu.:10.03  \n Max.   :444.00   Max.   :4669.3200   Max.   :1707.00   Max.   :47.40  \n     Scruz           Adjacent      \n Min.   :  0.00   Min.   :   0.03  \n 1st Qu.: 11.03   1st Qu.:   0.52  \n Median : 46.65   Median :   2.59  \n Mean   : 56.98   Mean   : 261.10  \n 3rd Qu.: 81.08   3rd Qu.:  59.24  \n Max.   :290.20   Max.   :4669.32",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-4",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-4",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\n\n\nShow R code\npairs(gala)",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-5",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-5",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\n\n\nShow R code\npairs(~ log(Species) + log(Area) + log(Elevation) + \n  log(Nearest) + log(Scruz + 0.1) + log(Adjacent), data = gala)",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-6",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-6",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\nLet‚Äôs start with a linear model\n\n\nShow R code\ngala.ols &lt;- lm(log(Species) ~ log(Area) + log(Elevation) + log(Nearest) + I(log(Scruz + 0.1)) + log(Adjacent), data = gala)\nsummary(gala.ols)\n\n\n\nCall:\nlm(formula = log(Species) ~ log(Area) + log(Elevation) + log(Nearest) + \n    I(log(Scruz + 0.1)) + log(Adjacent), data = gala)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.4563 -0.5192 -0.1059  0.4632  1.3351 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          5.10569    1.64880   3.097  0.00493 ** \nlog(Area)            0.50350    0.09942   5.064 3.53e-05 ***\nlog(Elevation)      -0.37384    0.32242  -1.159  0.25767    \nlog(Nearest)        -0.06564    0.11475  -0.572  0.57262    \nI(log(Scruz + 0.1)) -0.08255    0.09517  -0.867  0.39433    \nlog(Adjacent)       -0.02488    0.04596  -0.541  0.59327    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7877 on 24 degrees of freedom\nMultiple R-squared:  0.7899,    Adjusted R-squared:  0.7461 \nF-statistic: 18.05 on 5 and 24 DF,  p-value: 1.941e-07",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-7",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-7",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\nResidual analysis\n\n\nShow R code\npar(mfrow = c(2, 2))\nplot(gala.ols, which = 1:4)",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#poisson-regression",
    "href": "slides/06-count-regression.html#poisson-regression",
    "title": "Regression for Counts and Rates",
    "section": "Poisson regression",
    "text": "Poisson regression\n\nNeed a way to link the mean response \\(\\mathrm{E}\\left(Y\\right) = \\mu \\in \\left(0, \\infty\\right)\\) to the linear predictor \\(\\eta = \\boldsymbol{x}^\\top\\boldsymbol{p}\\)\nIn Poisson we regression, we default to \\[\n\\log\\left(\\mu\\right) = \\boldsymbol{x}^\\top\\boldsymbol{p}\n\\]\nMaximum likelihood estimation provides a convenient estimate of \\(\\boldsymbol{\\beta}\\)",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-8",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-8",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\nTry a Poisson regression\n\n\nShow R code\nsummary(gala.poi &lt;- glm(Species ~ ., data = gala, family = poisson))\n\n\n\nCall:\nglm(formula = Species ~ ., family = poisson, data = gala)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.155e+00  5.175e-02  60.963  &lt; 2e-16 ***\nArea        -5.799e-04  2.627e-05 -22.074  &lt; 2e-16 ***\nElevation    3.541e-03  8.741e-05  40.507  &lt; 2e-16 ***\nNearest      8.826e-03  1.821e-03   4.846 1.26e-06 ***\nScruz       -5.709e-03  6.256e-04  -9.126  &lt; 2e-16 ***\nAdjacent    -6.630e-04  2.933e-05 -22.608  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3510.73  on 29  degrees of freedom\nResidual deviance:  716.85  on 24  degrees of freedom\nAIC: 889.68\n\nNumber of Fisher Scoring iterations: 5",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-9",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-9",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\nCheck mean-variance relationship\n\n\nShow R code\nmu &lt;- fitted(gala.poi)  # fitted fitted.values\ny &lt;- gala.poi$y  # observed response values\nplot(log(mu), log((y - mu)^2), xlab = expression(hat(mu)),\n     ylab = expression((y-hat(mu))^2))\nabline(0, 1)",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-10",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-10",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\nCrude check for overdispersion\n\n\nShow R code\nperformance::check_overdispersion(gala.poi)\n\n\n# Overdispersion test\n\n       dispersion ratio =  31.749\n  Pearson's Chi-Squared = 761.979\n                p-value = &lt; 0.001\n\n\nOverdispersion detected.",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-11",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-11",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\nSimilar to before, can use quasipoisson() family to correct fo overdispersion:\n\n\nShow R code\ngala.quasipoi &lt;- glm(Species ~ ., family = quasipoisson(link = \"log\"),  data = gala)\nsummary(gala.quasipoi)\n\n\n\nCall:\nglm(formula = Species ~ ., family = quasipoisson(link = \"log\"), \n    data = gala)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.1548079  0.2915901  10.819 1.03e-10 ***\nArea        -0.0005799  0.0001480  -3.918 0.000649 ***\nElevation    0.0035406  0.0004925   7.189 1.98e-07 ***\nNearest      0.0088256  0.0102622   0.860 0.398292    \nScruz       -0.0057094  0.0035251  -1.620 0.118380    \nAdjacent    -0.0006630  0.0001653  -4.012 0.000511 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 31.74921)\n\n    Null deviance: 3510.73  on 29  degrees of freedom\nResidual deviance:  716.85  on 24  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#rates-and-offsets",
    "href": "slides/06-count-regression.html#rates-and-offsets",
    "title": "Regression for Counts and Rates",
    "section": "Rates and offsets",
    "text": "Rates and offsets\n\nThe number of observed events may depend on a size variable that determines the number of opportunities for the events to occur\n\nFor example, the number of burglaries reported in different cities\n\nIn other cases, the size variable may be time\n\nFor example, the number of customers served by a sales worker (must take account of the differing amounts of time worked)",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#rates-and-offsets-1",
    "href": "slides/06-count-regression.html#rates-and-offsets-1",
    "title": "Regression for Counts and Rates",
    "section": "Rates and offsets",
    "text": "Rates and offsets\nAn experiment was conducted to determine the effect of gamma radiation on the numbers of chromosomal abnormalities (ca) observed. The number (cells), in hundreds of cells exposed in each run, differs. The dose amount (doseamt) and the rate (doserate) at which the dose is applied are the predictors of interest. The hypothesized model is as follows:\n\\[\n\\begin{align}\n\\log\\left(\\mathtt{ca}/\\mathtt{cells}\\right) &= \\boldsymbol{x}^\\top\\boldsymbol{\\beta} \\\\\n\\implies \\log\\left(\\mathtt{ca}\\right) &= \\log\\left(\\mathtt{cells}\\right) + \\boldsymbol{x}^\\top\\boldsymbol{\\beta}\n\\end{align}\n\\]",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#rates-and-offsets-2",
    "href": "slides/06-count-regression.html#rates-and-offsets-2",
    "title": "Regression for Counts and Rates",
    "section": "Rates and offsets",
    "text": "Rates and offsets\n\n\nShow R code\ndicentric &lt;- faraway::dicentric\ndicentric$dosef &lt;- factor(dicentric$doseamt)\nfit &lt;- glm(ca ~ offset(log(cells)) + log(doserate)*dosef, family = poisson, \n           data = dicentric)\nsummary(fit)\n\n\n\nCall:\nglm(formula = ca ~ offset(log(cells)) + log(doserate) * dosef, \n    family = poisson, data = dicentric)\n\nCoefficients:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)            -2.74671    0.03426 -80.165  &lt; 2e-16 ***\nlog(doserate)           0.07178    0.03518   2.041 0.041299 *  \ndosef2.5                1.62542    0.04946  32.863  &lt; 2e-16 ***\ndosef5                  2.76109    0.04349  63.491  &lt; 2e-16 ***\nlog(doserate):dosef2.5  0.16122    0.04830   3.338 0.000844 ***\nlog(doserate):dosef5    0.19350    0.04243   4.561  5.1e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 4753.00  on 26  degrees of freedom\nResidual deviance:   21.75  on 21  degrees of freedom\nAIC: 209.16\n\nNumber of Fisher Scoring iterations: 4",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#zero-inflated-outcomes",
    "href": "slides/06-count-regression.html#zero-inflated-outcomes",
    "title": "Regression for Counts and Rates",
    "section": "zero-inflated outcomes",
    "text": "zero-inflated outcomes\nThe state wildlife biologists want to model how many fish are being caught by fishermen at a state park. Visitors are asked how long they stayed, how many people were in the group, were there children in the group, and how many fish were caught. Some visitors do not fish, but there is no data on whether a person fished or not. Some visitors who did fish did not catch any fish so there are excess zeros in the data because of the people that did not fish.",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#zero-inflated-outcomes-1",
    "href": "slides/06-count-regression.html#zero-inflated-outcomes-1",
    "title": "Regression for Counts and Rates",
    "section": "Zero-inflated outcomes",
    "text": "Zero-inflated outcomes\nOur sample consists of We have data on N=250 groups that went to a park. Each group was questioned about how many fish they caught (count), how many children were in the group (child), how many people were in the group (persons), and whether or not they brought a camper to the park (camper).\nThe data can be read in as follows:\n\nfish &lt;- read.csv(\"https://stats.idre.ucla.edu/stat/data/fish.csv\")",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#zero-inflated-outcomes-2",
    "href": "slides/06-count-regression.html#zero-inflated-outcomes-2",
    "title": "Regression for Counts and Rates",
    "section": "Zero-inflated outcomes",
    "text": "Zero-inflated outcomes\n\n# Retain only variables of interest and print summary\nfish &lt;- fish[, c(\"count\", \"child\", \"persons\", \"camper\")]\nsummary(fish)\n\n     count             child          persons          camper     \n Min.   :  0.000   Min.   :0.000   Min.   :1.000   Min.   :0.000  \n 1st Qu.:  0.000   1st Qu.:0.000   1st Qu.:2.000   1st Qu.:0.000  \n Median :  0.000   Median :0.000   Median :2.000   Median :1.000  \n Mean   :  3.296   Mean   :0.684   Mean   :2.528   Mean   :0.588  \n 3rd Qu.:  2.000   3rd Qu.:1.000   3rd Qu.:4.000   3rd Qu.:1.000  \n Max.   :149.000   Max.   :3.000   Max.   :4.000   Max.   :1.000",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#zero-inflated-outcomes-3",
    "href": "slides/06-count-regression.html#zero-inflated-outcomes-3",
    "title": "Regression for Counts and Rates",
    "section": "Zero-inflated outcomes",
    "text": "Zero-inflated outcomes\n\n\nShow R code\npairs(log(count + 0.1) ~ ., data = fish)",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#zero-inflated-outcomes-4",
    "href": "slides/06-count-regression.html#zero-inflated-outcomes-4",
    "title": "Regression for Counts and Rates",
    "section": "Zero-inflated outcomes",
    "text": "Zero-inflated outcomes\nToo many zeros?\n\n\nShow R code\nbarplot(table(fish$count))",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#zero-inflated-outcomes-5",
    "href": "slides/06-count-regression.html#zero-inflated-outcomes-5",
    "title": "Regression for Counts and Rates",
    "section": "Zero-inflated outcomes",
    "text": "Zero-inflated outcomes\n\nfish.poi &lt;- glm(count ~ ., data = fish, family = poisson)\nsummary(fish.poi)\n\n\nCall:\nglm(formula = count ~ ., family = poisson, data = fish)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.98183    0.15226  -13.02   &lt;2e-16 ***\nchild       -1.68996    0.08099  -20.87   &lt;2e-16 ***\npersons      1.09126    0.03926   27.80   &lt;2e-16 ***\ncamper       0.93094    0.08909   10.45   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2958.4  on 249  degrees of freedom\nResidual deviance: 1337.1  on 246  degrees of freedom\nAIC: 1682.1\n\nNumber of Fisher Scoring iterations: 6\n\nperformance::check_zeroinflation(fish.poi)\n\n# Check for zero-inflation\n\n   Observed zeros: 142\n  Predicted zeros: 95\n            Ratio: 0.67\n\n\nModel is underfitting zeros (probable zero-inflation).",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-hurdle-model",
    "href": "slides/06-count-regression.html#the-hurdle-model",
    "title": "Regression for Counts and Rates",
    "section": "The hurdle model",
    "text": "The hurdle model\nIn addition to predicting the number of fish caught, there is interest in predicting the existence of excess zeros (i.e., the zeroes that were not simply a result of bad luck or lack of fishing skill). In particular, we‚Äôd like to estimate the effect of party size on catching zero fish.\nWe can accomplish this in several ways, but popular choices include:\n\nThe zero-inflated Poisson (or negative binomial) model\nThe hurdle model",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-hurdle-model-1",
    "href": "slides/06-count-regression.html#the-hurdle-model-1",
    "title": "Regression for Counts and Rates",
    "section": "The hurdle model",
    "text": "The hurdle model\nIn this example, we‚Äôll use a simple hurdle model, which essentially fits two separate models:\n\n\\(\\mathrm{P}\\left(Y = 0\\right)\\) via a logistic regression\n\\(\\mathrm{P}\\left(Y = j\\right)\\), \\(j = 1, 2, \\dots\\) via a truncated Poisson regression",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-hurdle-model-2",
    "href": "slides/06-count-regression.html#the-hurdle-model-2",
    "title": "Regression for Counts and Rates",
    "section": "The hurdle model",
    "text": "The hurdle model\nYou can fit hurdle models using the hurdle() function from package pscl:\n\n\nShow R code\nsuppressMessages(library(pscl))\n\nsummary(fish.hur &lt;- hurdle(count ~ child + camper | persons, data = fish))\n\n\n\nCall:\nhurdle(formula = count ~ child + camper | persons, data = fish)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-0.8590 -0.7384 -0.6782 -0.1234 23.9679 \n\nCount model coefficients (truncated poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.64668    0.08278  19.892   &lt;2e-16 ***\nchild       -0.75918    0.09004  -8.432   &lt;2e-16 ***\ncamper       0.75166    0.09112   8.249   &lt;2e-16 ***\nZero hurdle model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -0.7808     0.3240  -2.410   0.0160 *\npersons       0.1993     0.1161   1.716   0.0862 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 14 \nLog-likelihood: -1047 on 5 Df",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-hurdle-model-3",
    "href": "slides/06-count-regression.html#the-hurdle-model-3",
    "title": "Regression for Counts and Rates",
    "section": "The hurdle model",
    "text": "The hurdle model\nCheck the logit part directly:\n\nz &lt;- fish\nz$count &lt;- ifelse(z$count == 0, 0, 1)\nglm(count ~ persons, data = z, family = binomial)\n\n\nCall:  glm(formula = count ~ persons, family = binomial, data = z)\n\nCoefficients:\n(Intercept)      persons  \n    -0.7808       0.1993  \n\nDegrees of Freedom: 249 Total (i.e. Null);  248 Residual\nNull Deviance:      341.9 \nResidual Deviance: 339  AIC: 343",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-hurdle-model-4",
    "href": "slides/06-count-regression.html#the-hurdle-model-4",
    "title": "Regression for Counts and Rates",
    "section": "The hurdle model",
    "text": "The hurdle model\nInterpretation of the previous model\n\nThe expected log number of the fish caught reduces by 0.759 for every additional chile (all else held constant)\nBeing a camper increases the expected log number of fish caught by 0.752 (all else held constant)\nThe log odds of catching at least one fish increases by 0.199 for every additional person",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-hurdle-model-5",
    "href": "slides/06-count-regression.html#the-hurdle-model-5",
    "title": "Regression for Counts and Rates",
    "section": "The hurdle model",
    "text": "The hurdle model\nPredicting new observations\n\nnewobs &lt;- data.frame(\"child\" = 0, \"persons\" = 3, \"camper\" = 1)\nhead(predict(fish.hur, newdata = newobs, type = \"prob\"))\n\n          0            1           2           3           4          5\n1 0.5456108 8.310743e-05 0.000457296 0.001677504 0.004615207 0.01015801\n           6          7          8          9         10         11         12\n1 0.01863138 0.02929101 0.04029322 0.04926936 0.05422062 0.05424495 0.04974685\n          13         14         15         16         17          18         19\n1 0.04211237 0.03310314 0.02428653 0.01670448 0.01081363 0.006611296 0.00382931\n           20          21           22           23           24          25\n1 0.002107066 0.001104196 0.0005523459 0.0002642839 0.0001211845 5.33451e-05\n            26           27           28           29           30          31\n1 2.257921e-05 9.203065e-06 3.617112e-06 1.372624e-06 5.035212e-07 1.78749e-07\n            32           33           34           35           36          37\n1 6.147253e-08 2.050004e-08 6.635342e-09 2.086329e-09 6.377754e-10 1.89694e-10\n            38           39           40           41           42           43\n1 5.493606e-11 1.550174e-11 4.264891e-12 1.144752e-12 2.999507e-13 7.676599e-14\n            44           45           46           47           48           49\n1 1.920011e-14 4.695466e-15 1.123333e-15 2.630256e-16 6.030375e-17 1.354365e-17\n           50           51           52           53           54           55\n1 2.98094e-18 6.432364e-19 1.361303e-19 2.826614e-20 5.760501e-21 1.152617e-21\n            56           57           58           59           60          61\n1 2.265085e-22 4.373178e-23 8.297681e-24 1.547719e-24 2.838759e-25 5.12137e-26\n            62           63           64           65           66           67\n1 9.090379e-27 1.587921e-27 2.730464e-28 4.622859e-29 7.708223e-30 1.266097e-30\n            68           69           70           71           72           73\n1 2.049017e-31 3.268015e-32 5.137756e-33 7.963475e-34 1.217188e-34 1.834942e-35\n           74           75           76           77           78           79\n1 2.72884e-36 4.004095e-37 5.798001e-38 8.286576e-39 1.169144e-39 1.628653e-40\n            80           81           82          83           84          85\n1 2.240402e-41 3.043887e-42 4.085095e-43 5.41641e-44 7.096101e-45 9.18731e-46\n            86           87           88           89           90           91\n1 1.175648e-46 1.487119e-47 1.859733e-48 2.299578e-49 2.811856e-50 3.400472e-51\n            92           93           94           95          96           97\n1 4.067606e-52 4.813306e-53 5.635119e-54 6.527803e-55 7.48313e-56 8.489832e-57\n            98           99          100          101          102          103\n1 9.533679e-58 1.059773e-58 1.166273e-59 1.270769e-60 1.371052e-61 1.464887e-62\n           104          105          106         107          108          109\n1 1.550095e-63 1.624638e-64 1.686701e-65 1.73477e-66 1.767688e-67 1.784706e-68\n           110          111          112          113          114          115\n1 1.785507e-69 1.770215e-70 1.739384e-71 1.693966e-72 1.635262e-73 1.564865e-74\n          116          117          118          119          120          121\n1 1.48459e-75 1.396395e-76 1.302308e-77 1.204354e-78 1.104487e-79 1.004529e-80\n           122          123          124          125          126          127\n1 9.061297e-82 8.107235e-83 7.195129e-84 6.334555e-85 5.532648e-86 4.794208e-87\n           128          129          130          131          132          133\n1 4.121871e-88 3.516351e-89 2.976709e-90 2.500648e-91 2.084809e-92 1.725052e-93\n           134          135          136          137          138          139\n1 1.416722e-94 1.154884e-95 9.345169e-97 7.506787e-98 5.986356e-99 4.73953e-100\n            140           141           142           143           144\n1 3.725587e-101 2.907791e-102 2.253525e-103 1.734258e-104 1.325375e-105\n            145           146           147           148           149\n1 1.005908e-106 7.582161e-108 5.676272e-109 4.220744e-110 3.117383e-111",
    "crumbs": [
      "Slides",
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#western-collaborative-group-study",
    "href": "slides/03-logistic-regression-2.html#western-collaborative-group-study",
    "title": "Logistic Regression (Part II)",
    "section": "Western collaborative group study",
    "text": "Western collaborative group study\n\\(N = 3154\\) healthy young men aged 39‚Äì59 from the San Francisco area were assessed for their personality type. All were free from coronary heart disease at the start of the research. Eight and a half years later change in this situation was recorded. See ?faraway::wcgs in R.\n\n\nShow R code\n# install.packages(\"faraway\")\nhead(wcgs &lt;- faraway::wcgs)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#exploratory-data-analysis",
    "href": "slides/03-logistic-regression-2.html#exploratory-data-analysis",
    "title": "Logistic Regression (Part II)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nTry fitting a full model first!\n\n\nShow R code\nlr.fit.all &lt;- glm(chd ~ ., family = binomial(link = \"logit\"), data = wcgs)#, maxit = 9999)\nsummary(lr.fit.all)\n\n\n\nCall:\nglm(formula = chd ~ ., family = binomial(link = \"logit\"), data = wcgs)\n\nCoefficients: (1 not defined because of singularities)\n                  Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)      2.657e+01  2.228e+05   0.000    1.000\nage             -3.651e-15  1.208e+03   0.000    1.000\nheight          -1.087e-14  3.067e+03   0.000    1.000\nweight           1.687e-15  3.832e+02   0.000    1.000\nsdp              2.409e-15  6.738e+02   0.000    1.000\ndbp             -3.019e-15  1.068e+03   0.000    1.000\nchol             8.301e-17  1.524e+02   0.000    1.000\nbehaveA2        -3.044e-14  2.413e+04   0.000    1.000\nbehaveB3         2.517e-16  2.444e+04   0.000    1.000\nbehaveB4         5.450e-15  2.937e+04   0.000    1.000\ncigs            -9.188e-16  4.526e+02   0.000    1.000\ndibepB                  NA         NA      NA       NA\ntypechdinfdeath -4.405e-06  5.876e+04   0.000    1.000\ntypechdnone     -5.313e+01  5.223e+04  -0.001    0.999\ntypechdsilent   -4.400e-06  6.574e+04   0.000    1.000\ntimechd          1.280e-16  1.080e+01   0.000    1.000\narcuspresent     5.666e-14  1.429e+04   0.000    1.000\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1.7692e+03  on 3139  degrees of freedom\nResidual deviance: 1.8217e-08  on 3124  degrees of freedom\n  (14 observations deleted due to missingness)\nAIC: 32\n\nNumber of Fisher Scoring iterations: 25",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#exploratory-data-analysis-1",
    "href": "slides/03-logistic-regression-2.html#exploratory-data-analysis-1",
    "title": "Logistic Regression (Part II)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nRefit without leakage variables typechd and timechd:\n\n\nShow R code\nwcgs &lt;- subset(wcgs, select = -c(typechd, timechd))\nlr.fit.all &lt;- glm(chd ~ ., family = binomial(link = \"logit\"), data = wcgs)#, maxit = 9999)\nsummary(lr.fit.all)\n\n\n\nCall:\nglm(formula = chd ~ ., family = binomial(link = \"logit\"), data = wcgs)\n\nCoefficients: (1 not defined because of singularities)\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -12.331126   2.350347  -5.247 1.55e-07 ***\nage            0.061812   0.012421   4.977 6.47e-07 ***\nheight         0.006903   0.033335   0.207  0.83594    \nweight         0.008637   0.003892   2.219  0.02647 *  \nsdp            0.018146   0.006435   2.820  0.00481 ** \ndbp           -0.000916   0.010903  -0.084  0.93305    \nchol           0.010726   0.001531   7.006 2.45e-12 ***\nbehaveA2       0.082920   0.222909   0.372  0.70990    \nbehaveB3      -0.618013   0.245032  -2.522  0.01166 *  \nbehaveB4      -0.487224   0.321325  -1.516  0.12944    \ncigs           0.021036   0.004298   4.895 9.84e-07 ***\ndibepB               NA         NA      NA       NA    \narcuspresent   0.212796   0.143915   1.479  0.13924    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1769.2  on 3139  degrees of freedom\nResidual deviance: 1569.1  on 3128  degrees of freedom\n  (14 observations deleted due to missingness)\nAIC: 1593.1\n\nNumber of Fisher Scoring iterations: 6",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#whats-going-on-with-dibep",
    "href": "slides/03-logistic-regression-2.html#whats-going-on-with-dibep",
    "title": "Logistic Regression (Part II)",
    "section": "What‚Äôs going on with dibep?",
    "text": "What‚Äôs going on with dibep?\nLet‚Äôs inspect the data a bit more; we‚Äôll start with a SPLOM\n\n\nShow R code\ny &lt;- ifelse(wcgs$chd == \"yes\", 1, 0)\npairs(wcgs, col = adjustcolor(y + 1, alpha.f = 0.1))",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#check-for-nas",
    "href": "slides/03-logistic-regression-2.html#check-for-nas",
    "title": "Logistic Regression (Part II)",
    "section": "Check for NAs",
    "text": "Check for NAs\nThe lapply() function (and friends) are quite useful!\n\n\nShow R code\n# Which columns contain missing values?\nsapply(wcgs, FUN = function(column) mean(is.na(column)))\n\n\n         age       height       weight          sdp          dbp         chol \n0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0038046925 \n      behave         cigs        dibep          chd        arcus \n0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0006341154",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#measures-of-association-how-to-choose",
    "href": "slides/03-logistic-regression-2.html#measures-of-association-how-to-choose",
    "title": "Logistic Regression (Part II)",
    "section": "Measures of Association: How to Choose?1",
    "text": "Measures of Association: How to Choose?1\n\nNice little paper from Harry Khamis, my old graduate advisor at WSU.",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#check-pairwise-correlations",
    "href": "slides/03-logistic-regression-2.html#check-pairwise-correlations",
    "title": "Logistic Regression (Part II)",
    "section": "Check pairwise correlations",
    "text": "Check pairwise correlations\nOnly looking at numeric columns:\n\n\nShow R code\n# Look at correlations between numeric features\nnum &lt;- sapply(wcgs, FUN = is.numeric)  # identify numeric columns\n(corx &lt;- cor(wcgs[, num], use = \"pairwise.complete.obs\"))  # simple correlation matrix\n\n\n                age      height       weight        sdp         dbp\nage     1.000000000 -0.09537568 -0.034404537 0.16574640  0.13919776\nheight -0.095375682  1.00000000  0.532935466 0.01837357  0.01027555\nweight -0.034404537  0.53293547  1.000000000 0.25324962  0.29592019\nsdp     0.165746397  0.01837357  0.253249623 1.00000000  0.77290641\ndbp     0.139197757  0.01027555  0.295920186 0.77290641  1.00000000\nchol    0.089188510 -0.08893778  0.008537442 0.12306130  0.12959711\ncigs   -0.005033852  0.01491129 -0.081747507 0.02997753 -0.05934232\n               chol         cigs\nage     0.089188510 -0.005033852\nheight -0.088937779  0.014911292\nweight  0.008537442 -0.081747507\nsdp     0.123061297  0.029977529\ndbp     0.129597108 -0.059342317\nchol    1.000000000  0.096031834\ncigs    0.096031834  1.000000000",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#check-pairwise-correlations-1",
    "href": "slides/03-logistic-regression-2.html#check-pairwise-correlations-1",
    "title": "Logistic Regression (Part II)",
    "section": "Check pairwise correlations",
    "text": "Check pairwise correlations\nOnly looking at numeric columns:\n\n\nShow R code\ncorrplot::corrplot(corx, method = \"square\", order = \"FPC\", type = \"lower\", diag = TRUE)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#check-pairwise-correlations-2",
    "href": "slides/03-logistic-regression-2.html#check-pairwise-correlations-2",
    "title": "Logistic Regression (Part II)",
    "section": "Check pairwise correlations",
    "text": "Check pairwise correlations\nPairwise scatterplots with LOWESS smoothers:\n\n\nShow R code\npar(mfrow = c(1, 2))\nplot(weight ~ height, data = wcgs, col = adjustcolor(1, alpha.f = 0.4))\nlines(lowess(x=wcgs$height, y=wcgs$weight), lwd = 2, col = 2)\nplot(dbp ~ sdp, data = wcgs, col = adjustcolor(1, alpha.f = 0.4))\nlines(lowess(wcgs$sdp, y = wcgs$dbp), lwd = 2, col = 2)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#what-about-categorical-variables",
    "href": "slides/03-logistic-regression-2.html#what-about-categorical-variables",
    "title": "Logistic Regression (Part II)",
    "section": "What about categorical variables?",
    "text": "What about categorical variables?\nContingency table cross-classifying dibep and behave:\n\n\nShow R code\n# What about categorical features?\nxtabs(~ behave + dibep, data = wcgs)  # perfect correlation?\n\n\n      dibep\nbehave    A    B\n    A1  264    0\n    A2 1325    0\n    B3    0 1216\n    B4    0  349",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#so-far",
    "href": "slides/03-logistic-regression-2.html#so-far",
    "title": "Logistic Regression (Part II)",
    "section": "So far‚Ä¶",
    "text": "So far‚Ä¶\n\nAs expected, looks like there‚Äôs moderate positive correlation between\n\nsdp and dbp\nheight and weight\n\nNot necessarily a problem (yet). But how could potentially fix any issues?\nAlso looks like some redunancy between the categorical variables dibep and behave",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#looking-more-closely-at-dibep",
    "href": "slides/03-logistic-regression-2.html#looking-more-closely-at-dibep",
    "title": "Logistic Regression (Part II)",
    "section": "Looking more closely at dibep",
    "text": "Looking more closely at dibep\nTry a decision tree:\n\n\nShow R code\n# What about categorical features?\nrpart::rpart(dibep ~ ., data = wcgs)  # perfect correlation?\n\n\nn= 3154 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 3154 1565 A (0.5038047 0.4961953)  \n  2) behave=A1,A2 1589    0 A (1.0000000 0.0000000) *\n  3) behave=B3,B4 1565    0 B (0.0000000 1.0000000) *\n\n\n\nLooks like dipep can be predicted perfectly from behave (i.e., they are redundant)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#redundancy-analysis",
    "href": "slides/03-logistic-regression-2.html#redundancy-analysis",
    "title": "Logistic Regression (Part II)",
    "section": "Redundancy analysis",
    "text": "Redundancy analysis\nRedunancy analysis is a powerful tool available in the Hmisc package:\n\n\nShow R code\n# Notice we're ignoring the response here!\nHmisc::redun(~ . - chd, nk = 0, data = wcgs)\n\n\n\nRedundancy Analysis\n\n~age + height + weight + sdp + dbp + chol + behave + cigs + dibep + \n    arcus\n&lt;environment: 0x12e5c62a8&gt;\n\nn: 3140     p: 10   nk: 0 \n\nNumber of NAs:   0 \n\nTransformation of target variables forced to be linear\n\nR-squared cutoff: 0.9   Type: ordinary \n\nR^2 with which each variable can be predicted from all other variables:\n\n   age height weight    sdp    dbp   chol behave   cigs  dibep  arcus \n 0.083  0.323  0.379  0.606  0.617  0.054  1.000  0.050  1.000  0.055 \n\nRendundant variables:\n\ndibep\n\n\nPredicted from variables:\n\nage height weight sdp dbp chol behave cigs arcus\n\n  Variable Deleted R^2 R^2 after later deletions\n1            dibep   1",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#battery-target",
    "href": "slides/03-logistic-regression-2.html#battery-target",
    "title": "Logistic Regression (Part II)",
    "section": "Battery target",
    "text": "Battery target\nCool idea from Salford Systems back in the day (now part of Minitab). Think of it as VIFs and redundancy analysis on steroids!\n\n\nSource",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#full-model-again",
    "href": "slides/03-logistic-regression-2.html#full-model-again",
    "title": "Logistic Regression (Part II)",
    "section": "Full model (again)",
    "text": "Full model (again)\nThis time we‚Äôve removed both the leakage and redundant predictors:\n\n\nShow R code\n# Refit model without leakage or redundant features\nsummary(lr.fit.all &lt;- glm(chd ~ . - dibep, family = binomial(link = \"logit\"), data = wcgs))\n\n\n\nCall:\nglm(formula = chd ~ . - dibep, family = binomial(link = \"logit\"), \n    data = wcgs)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -12.331126   2.350347  -5.247 1.55e-07 ***\nage            0.061812   0.012421   4.977 6.47e-07 ***\nheight         0.006903   0.033335   0.207  0.83594    \nweight         0.008637   0.003892   2.219  0.02647 *  \nsdp            0.018146   0.006435   2.820  0.00481 ** \ndbp           -0.000916   0.010903  -0.084  0.93305    \nchol           0.010726   0.001531   7.006 2.45e-12 ***\nbehaveA2       0.082920   0.222909   0.372  0.70990    \nbehaveB3      -0.618013   0.245032  -2.522  0.01166 *  \nbehaveB4      -0.487224   0.321325  -1.516  0.12944    \ncigs           0.021036   0.004298   4.895 9.84e-07 ***\narcuspresent   0.212796   0.143915   1.479  0.13924    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1769.2  on 3139  degrees of freedom\nResidual deviance: 1569.1  on 3128  degrees of freedom\n  (14 observations deleted due to missingness)\nAIC: 1593.1\n\nNumber of Fisher Scoring iterations: 6",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#variance-inflation-factors-vifs",
    "href": "slides/03-logistic-regression-2.html#variance-inflation-factors-vifs",
    "title": "Logistic Regression (Part II)",
    "section": "Variance inflation factors (VIFs)",
    "text": "Variance inflation factors (VIFs)\nVIFs aren‚Äôt built into base R, so here we‚Äôll use the car package:\n\n\nShow R code\n# Check (generalized) VIFs\ncar::vif(lr.fit.all)\n\n\n           GVIF Df GVIF^(1/(2*Df))\nage    1.097698  1        1.047711\nheight 1.479064  1        1.216168\nweight 1.603473  1        1.266283\nsdp    2.656210  1        1.629788\ndbp    2.796994  1        1.672421\nchol   1.029458  1        1.014622\nbehave 1.030106  3        1.004956\ncigs   1.054454  1        1.026866\narcus  1.060879  1        1.029990\n\n\n\nDoes anyone recall how VIFs are computed? Try running this with dibep still in the model!",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#body-mass-index-fheight-weight",
    "href": "slides/03-logistic-regression-2.html#body-mass-index-fheight-weight",
    "title": "Logistic Regression (Part II)",
    "section": "Body mass index = f(height, weight)",
    "text": "Body mass index = f(height, weight)\n\nFeature engineering is useful in cases where it makes sense!\nNot sure we can combine sdb and dbp in any useful way?!\nheight and weight can be combined into a single number called body mass index (BMI): \\(\\text{BMI} = \\frac{\\text{mass}_\\text{lb}}{\\text{height}_\\text{in}^2} \\times 703\\)\n\n\n\nShow R code\nwcgs$bmi &lt;- with(wcgs, 703 * weight / (height^2))\nhead(wcgs[, c(\"height\", \"weight\", \"bmi\")])",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#full-model-again-again",
    "href": "slides/03-logistic-regression-2.html#full-model-again-again",
    "title": "Logistic Regression (Part II)",
    "section": "Full model (again again)",
    "text": "Full model (again again)\nThis time, we‚Äôll remove sdb, height, and weight, and include bmi:\n\n\nShow R code\nsummary(lr.fit.all &lt;- update(lr.fit.all, formula = . ~ . + bmi - sdp - height - weight))\n\n\n\nCall:\nglm(formula = chd ~ age + dbp + chol + behave + cigs + arcus + \n    bmi, family = binomial(link = \"logit\"), data = wcgs)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -11.500594   1.026209 -11.207  &lt; 2e-16 ***\nage            0.062802   0.012227   5.136 2.80e-07 ***\ndbp            0.022652   0.007073   3.203  0.00136 ** \nchol           0.010520   0.001507   6.982 2.90e-12 ***\nbehaveA2       0.135021   0.222357   0.607  0.54370    \nbehaveB3      -0.586373   0.244718  -2.396  0.01657 *  \nbehaveB4      -0.469271   0.320962  -1.462  0.14372    \ncigs           0.022668   0.004255   5.327 9.99e-08 ***\narcuspresent   0.223473   0.143358   1.559  0.11903    \nbmi            0.060327   0.027311   2.209  0.02718 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1769.2  on 3139  degrees of freedom\nResidual deviance: 1579.9  on 3130  degrees of freedom\n  (14 observations deleted due to missingness)\nAIC: 1599.9\n\nNumber of Fisher Scoring iterations: 6",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#backward-elimination",
    "href": "slides/03-logistic-regression-2.html#backward-elimination",
    "title": "Logistic Regression (Part II)",
    "section": "Backward elimination",
    "text": "Backward elimination\n\nStepwise procedures work the same here as they did for ordinary linear models\nWhile base R has the step() function, the stepAIC() function from package MASS is a bit better:\n\n\n\nShow R code\nsummary(lr.fit.back &lt;- MASS::stepAIC(lr.fit.all, direction = \"backward\", trace = 0))\n\n\n\nCall:\nglm(formula = chd ~ age + dbp + chol + behave + cigs + arcus + \n    bmi, family = binomial(link = \"logit\"), data = wcgs)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -11.500594   1.026209 -11.207  &lt; 2e-16 ***\nage            0.062802   0.012227   5.136 2.80e-07 ***\ndbp            0.022652   0.007073   3.203  0.00136 ** \nchol           0.010520   0.001507   6.982 2.90e-12 ***\nbehaveA2       0.135021   0.222357   0.607  0.54370    \nbehaveB3      -0.586373   0.244718  -2.396  0.01657 *  \nbehaveB4      -0.469271   0.320962  -1.462  0.14372    \ncigs           0.022668   0.004255   5.327 9.99e-08 ***\narcuspresent   0.223473   0.143358   1.559  0.11903    \nbmi            0.060327   0.027311   2.209  0.02718 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1769.2  on 3139  degrees of freedom\nResidual deviance: 1579.9  on 3130  degrees of freedom\n  (14 observations deleted due to missingness)\nAIC: 1599.9\n\nNumber of Fisher Scoring iterations: 6",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#forward-selection",
    "href": "slides/03-logistic-regression-2.html#forward-selection",
    "title": "Logistic Regression (Part II)",
    "section": "Forward selection",
    "text": "Forward selection\nLet‚Äôs assume we know cigs is relevant for predicting chd (regardless of its statistical significance). So we start with that in the model:\n\n\nShow R code\n# Variable selection using forward selection with AIC; which variables were added?\nm &lt;- glm(chd ~ cigs, data = wcgs, \n         family = binomial(link = \"logit\"))\n(lr.fit.forward &lt;- MASS::stepAIC(m, direction = \"forward\", trace = 0))\n\n\n\nCall:  glm(formula = chd ~ cigs, family = binomial(link = \"logit\"), \n    data = wcgs)\n\nCoefficients:\n(Intercept)         cigs  \n   -2.74216      0.02322  \n\nDegrees of Freedom: 3153 Total (i.e. Null);  3152 Residual\nNull Deviance:      1781 \nResidual Deviance: 1750     AIC: 1754",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#regularized-regression",
    "href": "slides/03-logistic-regression-2.html#regularized-regression",
    "title": "Logistic Regression (Part II)",
    "section": "Regularized regression",
    "text": "Regularized regression\n\nRegression coefficients are estimated under various constraints\nMost common approaches include:\n\nRidge regression, which can be useful when dealing with multicollinearity\nLASSO, which can be useful for variable selection\nElastic net (ENet) \\(\\approx\\) Ridge + LASSO\n\nThe glmnet package in R, among others, can fit the entire regularization path for many kinds of models, including GLMs and the Cox PH model",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#useful-resources",
    "href": "slides/03-logistic-regression-2.html#useful-resources",
    "title": "Logistic Regression (Part II)",
    "section": "Useful resources",
    "text": "Useful resources\n\nSee Section 6.2 of ISL book (FREE!!)\nMy HOMLR book with Brad (FREE!!)\nNice intro video (Python):",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#enet-fit-to-wcgs-data",
    "href": "slides/03-logistic-regression-2.html#enet-fit-to-wcgs-data",
    "title": "Logistic Regression (Part II)",
    "section": "ENet fit to wcgs data",
    "text": "ENet fit to wcgs data\n\n\nShow R code\nlibrary(glmnet)\n\n# Fit an elastic net model (i.e., LASSO and ridge penalties) using 5-fold CV\nwcgs.complete &lt;- na.omit(wcgs)\nX &lt;- model.matrix(~. - chd - dibep - bmi - 1 , data = wcgs.complete)\n#lr.enet &lt;- cv.glmnet(X, y = ifelse(wcgs.complete$chd == \"yes\", 1, 0), \n#                     family = \"binomial\", nfold = 5, keep = TRUE)\nlr.enet &lt;- glmnet(X, y = ifelse(wcgs.complete$chd == \"yes\", 1, 0), \n                  family = \"binomial\")\nplot(lr.enet, label = TRUE, xvar = \"lambda\")",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#enet-fit-to-wcgs-data-1",
    "href": "slides/03-logistic-regression-2.html#enet-fit-to-wcgs-data-1",
    "title": "Logistic Regression (Part II)",
    "section": "ENet fit to wcgs data",
    "text": "ENet fit to wcgs data\n\n\nShow R code\nlibrary(glmnet)\n\n# Fit an elastic net model (i.e., LASSO and ridge penalties) using 5-fold CV\nlr.enet &lt;- cv.glmnet(X, y = ifelse(wcgs.complete$chd == \"yes\", 1, 0), \n                     family = \"binomial\", nfold = 5, keep = TRUE)\nplot(lr.enet)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#enet-fit-to-wcgs-data-2",
    "href": "slides/03-logistic-regression-2.html#enet-fit-to-wcgs-data-2",
    "title": "Logistic Regression (Part II)",
    "section": "ENet fit to wcgs data",
    "text": "ENet fit to wcgs data\n\n\nShow R code\ncoef(lr.enet)\n\n\n13 x 1 sparse Matrix of class \"dgCMatrix\"\n               lambda.1se\n(Intercept)  -6.793996643\nage           0.026985742\nheight        .          \nweight        .          \nsdp           0.011042397\ndbp           .          \nchol          0.006658276\nbehaveA1      .          \nbehaveA2      0.101528310\nbehaveB3     -0.062327909\nbehaveB4      .          \ncigs          0.007890454\narcuspresent  .",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#prediction-accuracy",
    "href": "slides/03-logistic-regression-2.html#prediction-accuracy",
    "title": "Logistic Regression (Part II)",
    "section": "Prediction accuracy",
    "text": "Prediction accuracy\n\nScoring rules are used to evaluate probabilistic predictions\nA scoring rule is proper if it is minimized in expectation by the true probability\nIt is a metric that is optimized when the forecasted probabilities are identical to the true outcome probabilities\nSee Gneiting & Raftery (2007, JASA) and this post for details\nExamples include log loss and the Brier score (or MSE)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#statistical-classification",
    "href": "slides/03-logistic-regression-2.html#statistical-classification",
    "title": "Logistic Regression (Part II)",
    "section": "Statistical classification",
    "text": "Statistical classification\n\nA classifier is a model that outputs a class label, as opposed to a probabilistic prediction\nLogistic regression is NOT a classifier!!\n\nBinary classification via logistic regression represents a forced choice based on a probability threshold\n\nClassification is rarely useful for decision making (think about a weather app that only prodiced classifications and not forecasts!)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#classification-boundary",
    "href": "slides/03-logistic-regression-2.html#classification-boundary",
    "title": "Logistic Regression (Part II)",
    "section": "Classification boundary",
    "text": "Classification boundary\nPerfect seperation (or discrimination):\n\n\nShow R code\n# Simulate some data\nN &lt;- 200\nd1 &lt;- cbind(matrix(MASS::mvrnorm(2*N, mu = c(0, 0), Sigma = diag(2)), ncol = 2), 0)\nd2 &lt;- cbind(matrix(MASS::mvrnorm(2*N, mu = c(8, 8), Sigma = diag(2)), ncol = 2), 1)\nd &lt;- as.data.frame(rbind(d1, d2))\nnames(d) &lt;- c(\"x1\", \"x2\", \"y\")\n\n# Fit a logistic regression\nfit &lt;- glm(y ~ ., data = d, family = binomial)\n\n# Plot decision boundary using 0.5 threshold\npfun &lt;- function(object, newdata) {\n  prob &lt;- predict(object, newdata = newdata, type = \"response\")\n  label &lt;- ifelse(prob &gt; 0.5, 1, 0)  # force into class label\n  label\n}\nplot(x2 ~ x1, data = d, col = d$y + 1)\ntreemisc::decision_boundary(fit, train = d, y = \"y\", x1 = \"x1\", x2 = \"x2\", \n                            pfun = pfun, grid.resolution = 999)\nlegend(\"topleft\", legend = c(\"y = 0\", \"y = 1\"), col = c(1, 2), pch = 1)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#classification-boundary-1",
    "href": "slides/03-logistic-regression-2.html#classification-boundary-1",
    "title": "Logistic Regression (Part II)",
    "section": "Classification boundary",
    "text": "Classification boundary\nClass overlap (four possibilities in terms of classification):\n\n\nShow R code\n# Simulate some data\nN &lt;- 200\nd1 &lt;- cbind(matrix(MASS::mvrnorm(2*N, mu = c(0, 0), Sigma = diag(2)), ncol = 2), 0)\nd2 &lt;- cbind(matrix(MASS::mvrnorm(2*N, mu = c(2, 2), Sigma = diag(2)), ncol = 2), 1)\nd &lt;- as.data.frame(rbind(d1, d2))\nnames(d) &lt;- c(\"x1\", \"x2\", \"y\")\n\n# Fit a logistic regression\nfit &lt;- glm(y ~ ., data = d, family = binomial)\n\n# Plot decision boundary using 0.5 threshold\npfun &lt;- function(object, newdata) {\n  prob &lt;- predict(object, newdata = newdata, type = \"response\")\n  label &lt;- ifelse(prob &gt; 0.5, 1, 0)  # force into class label\n  label\n}\nplot(x2 ~ x1, data = d, col = d$y + 1)\ntreemisc::decision_boundary(fit, train = d, y = \"y\", x1 = \"x1\", x2 = \"x2\", \n                            pfun = pfun, grid.resolution = 999)\nlegend(\"topleft\", legend = c(\"y = 0\", \"y = 1\"), col = c(1, 2), pch = 1)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#confusion-matrix",
    "href": "slides/03-logistic-regression-2.html#confusion-matrix",
    "title": "Logistic Regression (Part II)",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\nA confusion matrix is a special contingency table that describes the performance of a binary classifier\nMore of a matrix of confusion üò±\nLots of statistics can be computed from a given confusion matrix\n\nThey are all improper scoring rules and can be optimized by a bogus model\nMost are not useful for decision making IMO",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#example-with-wcgs-data",
    "href": "slides/03-logistic-regression-2.html#example-with-wcgs-data",
    "title": "Logistic Regression (Part II)",
    "section": "Example with wcgs data",
    "text": "Example with wcgs data\n\n\nShow R code\n# Confusion matrix (i.e., 2x2 contingency table of classification results)\ny &lt;- na.omit(wcgs)$chd  # observed classes\nprob &lt;- predict(lr.fit.all, type = \"response\")  # predicted probabilities\nclasses &lt;- ifelse(prob &gt; 0.5, \"yes\", \"no\")  # classification based on 0.5 threshold\n(cm &lt;- table(\"actual\" = y, \"predicted\" = classes))  # confusion matrix\n\n\n      predicted\nactual   no  yes\n   no  2883    2\n   yes  253    2",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#confusion-matrix-1",
    "href": "slides/03-logistic-regression-2.html#confusion-matrix-1",
    "title": "Logistic Regression (Part II)",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n Source",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#roc-curves",
    "href": "slides/03-logistic-regression-2.html#roc-curves",
    "title": "Logistic Regression (Part II)",
    "section": "ROC curves",
    "text": "ROC curves\n\nReceiver operating characteristic (ROC) curves display the tradeoff between the true positive rate (TPR or sensitivity) and false positive rate (FPR or 1 - specificity) for a range of probability thresholds\nInvariant to monotone transformations of \\(\\hat{p}\\)\nPrecision-recall plots can be more informative when dealing with class imbalance (really not a problem with logistic regression or when dealing with probabilities)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#transposed-conditionals",
    "href": "slides/03-logistic-regression-2.html#transposed-conditionals",
    "title": "Logistic Regression (Part II)",
    "section": "Transposed conditionals",
    "text": "Transposed conditionals\n\nConfusion of the inverse: \\(P\\left(A|B\\right) \\ne P\\left(B|A\\right)\\)\nThe error of the transposed conditional is rampant in research:\n\n‚ÄúConditioning on what is unknowable to predict what is already known leads to a host of complexities and interpretation problems.‚Äù\n\nTPR and FPR (and others) are transposed conditionals \\[\n\\begin{align}\nTPR &= P\\left(\\hat{Y} = 1 | Y = 1\\right) \\\\\n&= P\\left(\\text{known} | \\text{unknown}\\right)\n\\end{align}\n\\]",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#roc-curve-by-hand",
    "href": "slides/03-logistic-regression-2.html#roc-curve-by-hand",
    "title": "Logistic Regression (Part II)",
    "section": "ROC curve (by hand)",
    "text": "ROC curve (by hand)\n\n\nShow R code\nthreshold &lt;- seq(from = 0, to = 1, length = 999)\ntp &lt;- tn &lt;- fp &lt;- fn &lt;- numeric(length(threshold))\nfor (i in seq_len(length(threshold))) {\n  classes &lt;- ifelse(prob &gt; threshold[i], \"yes\", \"no\")\n  tp[i] &lt;- sum(classes == \"yes\" & y == \"yes\")  # true positives\n  tn[i] &lt;- sum(classes == \"no\"  & y == \"no\")  # true negatives\n  fp[i] &lt;- sum(classes == \"yes\" & y == \"no\")  # false positives\n  fn[i] &lt;- sum(classes == \"no\"  & y == \"yes\")  # false negatives\n}\ntpr &lt;- tp / (tp + fn)  # sensitivity\ntnr &lt;- tn / (tn + fp)  # specificity\n\n# Plot ROC curve\nplot(tnr, y = tpr, type = \"l\", col = 2, lwd = 2, xlab = \"TNR (or specificity)\", \n     ylab = \"TPR (or sensitivity)\")\nabline(1, -1, lty = 2)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#roc-curve-proc-package",
    "href": "slides/03-logistic-regression-2.html#roc-curve-proc-package",
    "title": "Logistic Regression (Part II)",
    "section": "ROC curve (pROC package)",
    "text": "ROC curve (pROC package)\nCan be useful to use a package sometimes (e.g., for computing are under the ROC curve; AKA AUROC or AUC)\n\n\nShow R code\nplot(roc &lt;- pROC::roc(y, predictor = prob))\n\n\n\nShow R code\nroc\n\n\n\nCall:\nroc.default(response = y, predictor = prob)\n\nData: prob in 2885 controls (y no) &lt; 255 cases (y yes).\nArea under the curve: 0.751",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#leave-one-covariate-out-loco-importance",
    "href": "slides/03-logistic-regression-2.html#leave-one-covariate-out-loco-importance",
    "title": "Logistic Regression (Part II)",
    "section": "Leave-one-covariate-out (LOCO) importance",
    "text": "Leave-one-covariate-out (LOCO) importance\nA simple and intuitive way to measure the ‚Äúimportance‚Äù of each covariate in a model. In the simplest terms:\n\nEstimate baseline performance (e.g., AUROC or Brier score)\nFor \\(j = 1, 2, \\dots p\\), refit the model without feature \\(x_j\\) and compute the degredation to the baseline performance.\nSort these values in a table or plot them.\n\nSee here for more details.",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#loco-scores-for-wcgs-example",
    "href": "slides/03-logistic-regression-2.html#loco-scores-for-wcgs-example",
    "title": "Logistic Regression (Part II)",
    "section": "LOCO scores for wcgs example",
    "text": "LOCO scores for wcgs example\n\n\nShow R code\nwcgs &lt;- faraway::wcgs\nwcgs$bmi &lt;- with(wcgs, weight / (height^2) * 703)\nomit &lt;- c(\"typechd\", \"timechd\", \"dibep\", \"height\", \"weight\")\nkeep &lt;- setdiff(names(wcgs), y = omit)\nwcgs &lt;- wcgs[, keep]\nfit &lt;- glm(chd ~ ., data = wcgs, family = binomial)\nsummary(fit)\n\n\n\nCall:\nglm(formula = chd ~ ., family = binomial, data = wcgs)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.168e+01  1.026e+00 -11.387  &lt; 2e-16 ***\nage           5.922e-02  1.233e-02   4.804 1.56e-06 ***\nsdp           1.806e-02  6.435e-03   2.807   0.0050 ** \ndbp          -3.193e-04  1.087e-02  -0.029   0.9766    \nchol          1.047e-02  1.520e-03   6.889 5.64e-12 ***\nbehaveA2      8.498e-02  2.229e-01   0.381   0.7030    \nbehaveB3     -6.238e-01  2.449e-01  -2.547   0.0109 *  \nbehaveB4     -4.994e-01  3.211e-01  -1.555   0.1199    \ncigs          2.139e-02  4.294e-03   4.982 6.31e-07 ***\narcuspresent  2.238e-01  1.437e-01   1.558   0.1193    \nbmi           5.880e-02  2.717e-02   2.164   0.0305 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1769.2  on 3139  degrees of freedom\nResidual deviance: 1572.3  on 3129  degrees of freedom\n  (14 observations deleted due to missingness)\nAIC: 1594.3\n\nNumber of Fisher Scoring iterations: 6\n\n\nShow R code\n# Leave-one-covariate-out (LOCO) method\n#\n# Note: Would be better to incorporate some form of cross-validation (or bootstrap)\nx.names &lt;- attr(fit$terms, \"term.labels\")\nloco &lt;- numeric(length(x.names))\nbaseline &lt;- deviance(fit)  # smaller is better; could also use AUROC, Brier score, etc.\nloco &lt;- sapply(x.names, FUN = function(x.name) {\n  wcgs.copy &lt;- wcgs\n  wcgs.copy[[x.name]] &lt;- NULL\n  fit.new &lt;- glm(chd ~ ., data = wcgs.copy, family = binomial(link = \"logit\"))\n  deviance(fit.new) - baseline  # measure drop in performance\n})\nnames(loco) &lt;- x.names\nsort(loco, decreasing = TRUE)\n\n\n        chol         cigs          age       behave        arcus          sdp \n5.201189e+01 2.385372e+01 2.298085e+01 2.233397e+01 1.203822e+01 7.647463e+00 \n         bmi          dbp \n4.630518e+00 8.624341e-04 \n\n\nShow R code\ndotchart(sort(loco, decreasing = TRUE), pch = 19)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#lift-charts",
    "href": "slides/03-logistic-regression-2.html#lift-charts",
    "title": "Logistic Regression (Part II)",
    "section": "Lift charts",
    "text": "Lift charts\n\nClassification is a forced choice!\nIn marketing, analysts generally know better than to try to classify a potential customer as someone to ignore or someone to spend resources on\n\nInstead, potential customers are sorted in decreasing order of estimated probability of purchasing a product\nThe marketer who can afford to advertise to \\(n\\) persons then picks the \\(n\\) highest-probability customers as targets!\n\nI like the idea of cumulative gain charts for this!",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#lift-charts-1",
    "href": "slides/03-logistic-regression-2.html#lift-charts-1",
    "title": "Logistic Regression (Part II)",
    "section": "Lift charts",
    "text": "Lift charts\nCumulative gains chart applied to wcgs example (lr.fit.all):\n\n\nShow R code\nres &lt;- treemisc::lift(prob, y = y, pos.class = \"yes\")\nplot(res)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#probability-calibration",
    "href": "slides/03-logistic-regression-2.html#probability-calibration",
    "title": "Logistic Regression (Part II)",
    "section": "Probability calibration",
    "text": "Probability calibration\n\nA probability \\(p\\) is well calibrated if a fraction of about p of the events we predict with probability \\(p\\) actually occur\nCalibration curves are the ü•á gold standard ü•á\nCompared to general machine learning models, logistic regression tends to return well calibrated probabilities\nFor details, see Niculescu-Mizil & Caruana (2005) and Kull et al.¬†(2017) and",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#probability-calibration-1",
    "href": "slides/03-logistic-regression-2.html#probability-calibration-1",
    "title": "Logistic Regression (Part II)",
    "section": "Probability calibration",
    "text": "Probability calibration\nThe rms function val.prob() can be used for this:\n\n\nShow R code\nwcgs2 &lt;- na.omit(faraway::wcgs)\ny &lt;- ifelse(wcgs2$chd == \"yes\", 1, 0)\nfit &lt;- glm(y ~ cigs + height, data = wcgs2, family = binomial)\nprob &lt;- predict(fit, newdata = wcgs2, type = \"response\")\nrms::val.prob(prob, y = y)",
    "crumbs": [
      "Slides",
      "Logistic Regression (Part II)"
    ]
  }
]