[
  {
    "objectID": "slides/01-linear-regression.html#about-me",
    "href": "slides/01-linear-regression.html#about-me",
    "title": "Linear Regression",
    "section": "About me",
    "text": "About me\n\n\n\nüë®‚Äçüéì B.S. & M.S. in Applied Statistics (WSU)\nüë®‚Äçüéì Ph.D.¬†in Applied Matehmatics (AFIT)\nüé¨ Director, Data Science at 84.51Àö\nüë®‚Äçüè´ UC LCB adjunct (~8 years)\nSome R packages üì¶ :\n\npdp (partial dependence plots)\nvip (variable importance plots)\nfastshap (faster SHAP values)\n\nSome books üìö :\n\nHands-On Machine Learning with R\nTree-Based Methods for Statistical Learning",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#ames-housing-data",
    "href": "slides/01-linear-regression.html#ames-housing-data",
    "title": "Linear Regression",
    "section": "Ames housing data",
    "text": "Ames housing data\n\nData describing the sale of individual residential property in Ames, Iowa from 2006 to 2010\nThere are 2930 observations on 81 variables involved in assessing home values:\n\n23 nominal\n23 ordinal\n14 discrete\n20 continuous\n\nPaper: https://jse.amstat.org/v19n3/decock.pdf",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#ames-housing-data-1",
    "href": "slides/01-linear-regression.html#ames-housing-data-1",
    "title": "Linear Regression",
    "section": "Ames housing data",
    "text": "Ames housing data\n\n\nShow R code\names &lt;- AmesHousing::make_ames()  # install.packages(\"AmesHousing\")\n\names$Sale_Price &lt;- ames$Sale_Price / 10000\n\nhead(ames)",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#ames-housing-data-2",
    "href": "slides/01-linear-regression.html#ames-housing-data-2",
    "title": "Linear Regression",
    "section": "Ames housing data",
    "text": "Ames housing data\nWe‚Äôll focus on a handful of variables:\n\nSale_Price - Sale price of the house / $10K (response variable)\nGr_Liv_Area - Above grade (ground) living area square feet\nOverall_Qual - Rates the overall material and finish of the house",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#statistical-relationships",
    "href": "slides/01-linear-regression.html#statistical-relationships",
    "title": "Linear Regression",
    "section": "Statistical relationships",
    "text": "Statistical relationships\n\n\nShow R code\n# Simulate data from different SLR models\nset.seed(101)  # for reproducibility\nx &lt;- seq(from = 0, to = 4, length = 100)\ny &lt;- cbind(\n  1 + x + rnorm(length(x)),  # linear\n  1 + (x - 2)^2 + rnorm(length(x)),  # quadratic\n  1 + log(x + 0.1) + rnorm(length(x), sd = 0.3),  # logarithmic\n  1 + rnorm(length(x))  # no association\n)\n\n# Scatterplot of X vs. each Y in a 2-by-2 grid\npar(mfrow = c(2, 2))\nfor (i in 1:4) {\n  plot(x, y[, i], col = adjustcolor(\"cornflowerblue\", alpha.f = 0.7),\n       pch = 19, xlab = \"X\", ylab = \"Y\")\n}",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#are-x-and-y-correlated",
    "href": "slides/01-linear-regression.html#are-x-and-y-correlated",
    "title": "Linear Regression",
    "section": "Are \\(x\\) and \\(y\\) correlated?",
    "text": "Are \\(x\\) and \\(y\\) correlated?\n\n\nShow R code\nset.seed(1051)  # for reproducibility\nn &lt;- 1000 \nx &lt;- rnorm(n)\ny &lt;- 1 + 0.001*x + rnorm(n)\nplot(x, y, asp = 1, col = adjustcolor(\"black\", alpha.f = 0.3))",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#correlation-is-not-causation",
    "href": "slides/01-linear-regression.html#correlation-is-not-causation",
    "title": "Linear Regression",
    "section": "Correlation is not causation",
    "text": "Correlation is not causation",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#all-models-are-wrong",
    "href": "slides/01-linear-regression.html#all-models-are-wrong",
    "title": "Linear Regression",
    "section": "All models are wrong!",
    "text": "All models are wrong!\n\n\n\nAlso, see this talk by my old adviser, Thad Tarpey: ‚ÄúAll Models are Right‚Ä¶ most are useless.‚Äù",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#modeling-the-mean-response",
    "href": "slides/01-linear-regression.html#modeling-the-mean-response",
    "title": "Linear Regression",
    "section": "Modeling the mean response",
    "text": "Modeling the mean response\n\nAssume that \\(Y \\sim N\\left(\\mu, \\sigma^2\\right)\\), where\n\n\\[\\mu = \\mu\\left(x\\right) = \\beta_0 + \\beta_1 x = E\\left(Y|x\\right)\\]\n\n\nIn other words: \\(Y \\sim N\\left(\\beta_0 + \\beta_1 x, \\sigma^2\\right)\\)\nAlternatively, we could write \\(Y = \\beta_0 + \\beta_1 x + \\epsilon\\), where \\(\\epsilon \\sim N\\left(0, \\sigma^2\\right)\\)\nThis is called the simple linear regression (SLR) model",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#the-idea-behind-slr",
    "href": "slides/01-linear-regression.html#the-idea-behind-slr",
    "title": "Linear Regression",
    "section": "The idea behind SLR",
    "text": "The idea behind SLR\n\nImage source",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#simple-linear-regression-slr-1",
    "href": "slides/01-linear-regression.html#simple-linear-regression-slr-1",
    "title": "Linear Regression",
    "section": "Simple linear regression (SLR)",
    "text": "Simple linear regression (SLR)\n\nData: \\(\\left\\{(X_i, Y_i)\\right\\}_{i=1}^n\\)\nModel: \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\)\n\n\\(Y_i\\) is a continuous response\n\\(X_i\\) is a continuous predictor\n\\(\\beta_0\\) is the intercept of the regression line (also called the bias term)\n\\(\\beta_1\\) is the slope of the regression line\n\\(\\epsilon_i \\stackrel{iid}{\\sim} D\\left(0, \\sigma^2\\right)\\)",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#assumptions-about-the-errors-epsilon_i",
    "href": "slides/01-linear-regression.html#assumptions-about-the-errors-epsilon_i",
    "title": "Linear Regression",
    "section": "Assumptions about the errors \\(\\epsilon_i\\)",
    "text": "Assumptions about the errors \\(\\epsilon_i\\)\nFor \\(i\\) and \\(j\\) in \\(\\left\\{1, 2, \\dots, n\\right\\}\\) and \\(i \\ne j\\)\n\n\\(\\quad E\\left(\\epsilon_i\\right) = 0\\)\n\\(\\quad Var\\left(\\epsilon_i\\right) = \\sigma^2\\) (homoscedacticity üò±)\n\\(\\quad Cov\\left(\\epsilon_i, \\epsilon_j\\right) = 0\\) (independence)\n\nAssumptions 1‚Äì3 can be summarized as \\(\\epsilon_i \\stackrel{iid}{\\sim} D\\left(0, \\sigma^2\\right)\\), where \\(iid\\) refers to independent and identically distributed.",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#properties-of-slr",
    "href": "slides/01-linear-regression.html#properties-of-slr",
    "title": "Linear Regression",
    "section": "Properties of SLR",
    "text": "Properties of SLR\n\nSimple linear regression: \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\)\n\nAssumes the model is linear in the regression coefficients \\(\\beta_0\\) and \\(\\beta_1\\)\n\nThe error term is a random variable; hence, \\(Y_i\\) is also a random variable (Why? ü§î)\n\nWhat is \\(E\\left(Y_i|X_i\\right)\\) and \\(Var\\left(Y_i|X_i\\right)\\)?\n\n\\(Cor\\left(Y_i, Y_j\\right) = 0\\) \\(\\forall i \\ne j\\) (Why? ü§î)",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#inference-for-a-single-variable",
    "href": "slides/01-linear-regression.html#inference-for-a-single-variable",
    "title": "Linear Regression",
    "section": "Inference for a single variable",
    "text": "Inference for a single variable\n\nIs it useful to test the hypothesis that Sale_Price = \\(160K\\)?\n\n\n\nNo! Because Sale_Price is not a constant, but a random variable whose value varies from home to home (and year to year, etc.)\n\n\n\n\nWe‚Äôre more interested in questions such as:\n\nWhat is the chance that Sale_Price &gt; \\(160K\\)? (above median sale price)\nWhat is the chance that Sale_Price &lt; \\(105K\\)? (lowest decile)\nWhat is the chance that $129,500 &lt; Sale_Price &lt; \\(213,500\\)? (within IQR)",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#distribution-of-sale_price",
    "href": "slides/01-linear-regression.html#distribution-of-sale_price",
    "title": "Linear Regression",
    "section": "Distribution of Sale_Price",
    "text": "Distribution of Sale_Price\nCan look at historgram and empirical CDF:\n\n\nShow R code\npar(mfrow = c(1, 2), las = 1)\nhist(ames$Sale_Price, br = 50, xlab = \"Sale price ($)\", freq = FALSE, main = \"\")\nplot(ecdf(ames$Sale_Price), xlab = \"Sale price ($)\", main = \"\",\n     col = adjustcolor(1, alpha.f = 0.1))",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#normality-tests",
    "href": "slides/01-linear-regression.html#normality-tests",
    "title": "Linear Regression",
    "section": "Normality tests ü§Æ",
    "text": "Normality tests ü§Æ\n\nNormality tests, like the Shapiro-Wilk1 and Anderson-Darling tests, can also be used to assess normality\n\nI STRONGLY ADVISE AGAINST USING THEM!\n\nNo data is normally distributed, what we care about is whether a normal approximation is close enough!\nNormality tests provide a \\(p\\)-value, which only gives a yes/no conclusion and is heavily dependent on sample size.\n\nIn R, see ?shapiro.test for details.",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#what-can-we-do-if-the-normality-assumption-isnt-justified",
    "href": "slides/01-linear-regression.html#what-can-we-do-if-the-normality-assumption-isnt-justified",
    "title": "Linear Regression",
    "section": "What can we do if the normality assumption isn‚Äôt justified?",
    "text": "What can we do if the normality assumption isn‚Äôt justified?\n\nTry transformations\n\nLogarithm or square root for positive data\nPower transformation (like the well-known Box-Cox procedure)\n\nTry a more appropriate distribution (e.g., Poisson or gamma distribution)\nTry more advanced approaches, like the nonparametric bootstrapping!",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#least-squares-ls-estimation",
    "href": "slides/01-linear-regression.html#least-squares-ls-estimation",
    "title": "Linear Regression",
    "section": "Least squares (LS) estimation",
    "text": "Least squares (LS) estimation\nIdea of LS is to find \\(\\beta_0\\) and \\(\\beta_1\\) so that the sum of squared residuals (i.e., errors) is minimized: \\[SSE = \\sum_{i=1}^n\\left(y_i - \\beta_0 - \\beta_1x_i\\right)^2\\]\n\nPretty straightforward optimization problem that leads to closed-form solution (but no point in memorizing the formulas!)",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#concept-of-ls-estimation",
    "href": "slides/01-linear-regression.html#concept-of-ls-estimation",
    "title": "Linear Regression",
    "section": "Concept of LS estimation",
    "text": "Concept of LS estimation\n\nImage source",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#sale_price-and-gr_liv_area",
    "href": "slides/01-linear-regression.html#sale_price-and-gr_liv_area",
    "title": "Linear Regression",
    "section": "Sale_Price and Gr_Liv_Area",
    "text": "Sale_Price and Gr_Liv_Area\n\n\nShow R code\nplot(Sale_Price ~ Gr_Liv_Area, data = ames, las = 1,\n     col = adjustcolor(1, alpha.f = 0.3))",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#slr-fit",
    "href": "slides/01-linear-regression.html#slr-fit",
    "title": "Linear Regression",
    "section": "SLR fit",
    "text": "SLR fit\n\n\nShow R code\nsummary(fit &lt;- lm(Sale_Price ~ Gr_Liv_Area, data = ames))\n\n\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area, data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.347  -3.022  -0.197   2.273  33.432 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.3289634  0.3269703   4.064 4.94e-05 ***\nGr_Liv_Area 0.0111694  0.0002066  54.061  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.652 on 2928 degrees of freedom\nMultiple R-squared:  0.4995,    Adjusted R-squared:  0.4994 \nF-statistic:  2923 on 1 and 2928 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#is-this-a-good-fit",
    "href": "slides/01-linear-regression.html#is-this-a-good-fit",
    "title": "Linear Regression",
    "section": "Is this a good fit?",
    "text": "Is this a good fit?\n\n\nShow R code\nplot(Sale_Price ~ Gr_Liv_Area, data = ames,\n     col = adjustcolor(1, alpha.f = 0.3))\nabline(fit, lwd = 2, col = 2)  # add SLR fit\n\n\n\nWhich assumptions seem violated to some degree?",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#residual-diagnostics",
    "href": "slides/01-linear-regression.html#residual-diagnostics",
    "title": "Linear Regression",
    "section": "Residual diagnostics",
    "text": "Residual diagnostics\n\nThe standard residual is defined as \\(e_i = y_i - \\hat{y}_i\\) and can be regarded as the observed error\nThe residuals hold a lot of properties that make them useful for diagnosing potential issues with the model (e.g., suggesting potential transformations to try)\nMany other kinds of residuals exist for different purposes (e.g., standardized, studentized, jackknife or PRESS residuals, etc.)",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#what-can-residual-plots-tell-us",
    "href": "slides/01-linear-regression.html#what-can-residual-plots-tell-us",
    "title": "Linear Regression",
    "section": "What can residual plots tell us?",
    "text": "What can residual plots tell us?\n\nResiduals vs.¬†predictor values (checking non-linearity).\nResiduals vs.¬†fitted values (non-constant variance, non-linearity, and outliers)\nResiduals vs.¬†time or another sequence (checking independence)\nResiduals vs.¬†omitted predictor values (missing potentially important predictors)\nNormal QQ plot of residuals (non-normality).\nAnd much, much more!",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#sale_price-gr_liv_area",
    "href": "slides/01-linear-regression.html#sale_price-gr_liv_area",
    "title": "Linear Regression",
    "section": "Sale_Price ~ Gr_Liv_Area",
    "text": "Sale_Price ~ Gr_Liv_Area\nResidual analysis:\n\n\nShow R code\npar(mfrow = c(2, 3), las = 1)\nplot(fit, which = 1:6)\n\n\n\nWhat assumptions appear to be in violation?",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#lets-try-a-log-transformation",
    "href": "slides/01-linear-regression.html#lets-try-a-log-transformation",
    "title": "Linear Regression",
    "section": "Let‚Äôs try a log transformation",
    "text": "Let‚Äôs try a log transformation\n\n\nShow R code\nfit2 &lt;- lm(log(Sale_Price) ~ Gr_Liv_Area, data = ames)\nplot(log(Sale_Price) ~ Gr_Liv_Area, data = ames,\n     col = adjustcolor(1, alpha.f = 0.3))\nabline(fit2, lwd = 2, col = 2)  # add SLR fit",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#logsale_price-gr_liv_area",
    "href": "slides/01-linear-regression.html#logsale_price-gr_liv_area",
    "title": "Linear Regression",
    "section": "log(Sale_Price) ~ Gr_Liv_Area",
    "text": "log(Sale_Price) ~ Gr_Liv_Area\nResidual analysis:\n\n\nShow R code\npar(mfrow = c(2, 3), las = 1)\nplot(fit2, which = 1:6)\n\n\n\nAny better?",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#mlr-in-a-nutshell",
    "href": "slides/01-linear-regression.html#mlr-in-a-nutshell",
    "title": "Linear Regression",
    "section": "MLR in a nutshell ü•ú",
    "text": "MLR in a nutshell ü•ú\n\nThe (normal) multiple linear regression model assumes \\(Y \\sim N\\left(\\mu\\left(\\boldsymbol{x}\\right), \\sigma^2\\right)\\), where \\[\\mu\\left(\\boldsymbol{x}\\right) = \\beta_0 + \\sum_{i=1}^p \\beta_i x_i = \\boldsymbol{x}^\\top\\boldsymbol{\\beta}\\]\nLS estimation still provides unbiased estimate of \\(\\boldsymbol{\\beta} = \\left(\\beta_0, \\beta_1, \\dots, \\beta_p\\right)^\\top\\): \\(\\hat{\\boldsymbol{\\beta}} = \\left(\\boldsymbol{X}^\\top\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y}\\)\nFitted values: \\(\\hat{\\boldsymbol{y}} = \\boldsymbol{X}\\left(\\boldsymbol{X}^\\top\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y} = \\boldsymbol{H}\\boldsymbol{y}\\)\n\\(\\boldsymbol{H}\\) is the well-known ‚Äúhat matrix‚Äù",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#polynomial-regression",
    "href": "slides/01-linear-regression.html#polynomial-regression",
    "title": "Linear Regression",
    "section": "Polynomial regression",
    "text": "Polynomial regression\n\nPolynomial regression is just a special case of the MLR model\nA second order model in a single predictor: \\[Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon\\]\nA k-th order model in a single predictor (Typically \\(k \\le 3\\)): \\[Y = \\beta_0 + \\sum_{j=1}^k\\beta_j X^j + \\epsilon\\]",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#example-paper-strength-data",
    "href": "slides/01-linear-regression.html#example-paper-strength-data",
    "title": "Linear Regression",
    "section": "Example: paper strength data",
    "text": "Example: paper strength data\nData concerning the strength of kraft paper and the percentage of hardwood in the batch of pulp from which the paper was produced.\n\n\nShow R code\n# Load the hardwood conentration data\nurl &lt;- \"https://bgreenwell.github.io/uc-bana7052/data/hardwood.csv\"\nhardwood &lt;- read.csv(url)\n\n\n\n\nShow R code\nfit1 &lt;- lm(TsStr ~ HwdCon, data = hardwood)\nfit2 &lt;- lm(TsStr ~ HwdCon + I(HwdCon^2), data = hardwood)\n\npar(mfrow = c(1, 2), las = 1)\ninvestr::plotFit(fit1, pch = 19, col.fit = \"red2\", main = \"Linear Fit\")\ninvestr::plotFit(fit2, pch = 19, col.fit = \"red2\", main = \"Quadratic Fit\")",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#categorical-variables",
    "href": "slides/01-linear-regression.html#categorical-variables",
    "title": "Linear Regression",
    "section": "Categorical variables",
    "text": "Categorical variables\n\nCategorical variables can be handled in a number of ways in linear models, including\n\nDummy encoding (nominal)\nOrthogonal polynomials (ordinal)\n\n\nR dummy encodes nominal factors by default:\n\n\nShow R code\nfit3 &lt;- lm(log(Sale_Price) ~ Gr_Liv_Area + Central_Air + Paved_Drive, \n           data = ames)\nsummary(fit3)\n\n\n\nCall:\nlm(formula = log(Sale_Price) ~ Gr_Liv_Area + Central_Air + Paved_Drive, \n    data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2344 -0.1345  0.0073  0.1453  0.8502 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 1.428e+00  2.418e-02  59.059  &lt; 2e-16 ***\nGr_Liv_Area                 5.183e-04  9.520e-06  54.440  &lt; 2e-16 ***\nCentral_AirY                3.464e-01  2.068e-02  16.747  &lt; 2e-16 ***\nPaved_DrivePartial_Pavement 1.334e-01  3.733e-02   3.574 0.000358 ***\nPaved_DrivePaved            3.085e-01  1.975e-02  15.618  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2574 on 2925 degrees of freedom\nMultiple R-squared:  0.6018,    Adjusted R-squared:  0.6013 \nF-statistic:  1105 on 4 and 2925 DF,  p-value: &lt; 2.2e-16\n\n\nHow do you interpret the coefficients here?",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#coefficient-of-determination",
    "href": "slides/01-linear-regression.html#coefficient-of-determination",
    "title": "Linear Regression",
    "section": "Coefficient of determination",
    "text": "Coefficient of determination\nThe coefficient of determination is the proportion of the variance in the dependent variable that is predictable from the independent variables in the model.\n\n\nR-squared (\\(R^2\\))\n\n\\(R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\\)\n\\(R^2\\) will always increase as more terms are added to the model!\n\n\nAdjusted R-squared (\\(R_{adj}^2\\))\n\n\n\\(R_{adj}^2 = 1 - \\frac{MSE}{SST/\\left(n - 1\\right)}\\)\nPenalizes \\(R^2\\) if there are ‚Äútoo many‚Äù terms in the model\n\\(R_{adj}^2\\) and \\(MSE\\) provide equivalent information",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#variablemodel-selection",
    "href": "slides/01-linear-regression.html#variablemodel-selection",
    "title": "Linear Regression",
    "section": "Variable/model selection",
    "text": "Variable/model selection\n\nVariable/model selection is a very noisy problem! (Often best to avoid, if feasible)\nAsk the domain experts about important variables (don‚Äôt just rely on algorithms)\nP(selecting the ‚Äúright‚Äù variables) = 0 (source)\n‚ÄúAll models subsets of variables are wrong, but some are useful!‚Äù\nIn regression settings, regularization (e.g., ridge regression and the LASSO) is often more useful! (Think about the impact of multicollinearity on variable selection)",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#data-splitting",
    "href": "slides/01-linear-regression.html#data-splitting",
    "title": "Linear Regression",
    "section": "Data splitting",
    "text": "Data splitting\n\nIf prediction is the goal (e.g., compared to inference and hypothesis testing), the model performance should be assessed rigorously\nData splitting techniques ae key and the type of data splitting to use often depends on the situation (e.g., cross-sectional vs.¬†time-series data)\nIn simplest terms, random split the data into two parts: A and B. Build a model on part A and see how well it does with predicting the response in part B.\nLeakage is a huge concern here, so data splitting ALWAYS has to be done carefully!",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#linear-regression-as-a-glm",
    "href": "slides/01-linear-regression.html#linear-regression-as-a-glm",
    "title": "Linear Regression",
    "section": "Linear Regression as a GLM",
    "text": "Linear Regression as a GLM\nWe can define the (normal) linear regression model as a specific type of Generalized Linear Model (GLM):\n\nRandom Component: The response variable \\(Y\\) follows a normal distribution: \\(Y \\sim N(\\mu, \\sigma^2)\\).\nSystematic Component: The linear predictor \\(\\alpha\\) is a linear function of the regressors: \\(\\eta = \\boldsymbol{x}^\\top\\boldsymbol{\\beta}\\).\nLink Function: The link function \\(g(\\cdot)\\) connects the mean response \\(\\mu\\) to the linear predictor \\(\\eta\\). For linear regression, we use the identity link: \\[g(\\mu) = \\mu = \\eta\\]\n\nThis framework allows us to generalize to other types of response variables (e.g., binary, count) by changing the random component and the link function!",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression.html#questions",
    "href": "slides/01-linear-regression.html#questions",
    "title": "Linear Regression",
    "section": "Questions?",
    "text": "Questions?\n```",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/01-linear-regression-old.html#about-me",
    "href": "slides/01-linear-regression-old.html#about-me",
    "title": "Linear Regression",
    "section": "About me",
    "text": "About me\n\n\n\nüë®‚Äçüéì B.S. & M.S. in Applied Statistics (WSU)\nüë®‚Äçüéì Ph.D.¬†in Applied Matehmatics (AFIT)\nüé¨ Director, Data Science at 84.51Àö\nüë®‚Äçüè´ UC LCB adjunct (~8 years)\nSome R packages üì¶ :\n\npdp (partial dependence plots)\nvip (variable importance plots)\nfastshap (faster SHAP values)\n\nSome books üìö :\n\nHands-On Machine Learning with R\nTree-Based Methods for Statistical Learning"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#ames-housing-data",
    "href": "slides/01-linear-regression-old.html#ames-housing-data",
    "title": "Linear Regression",
    "section": "Ames housing data",
    "text": "Ames housing data\n\nData describing the sale of individual residential property in Ames, Iowa from 2006 to 2010\nThere are 2930 observations on 81 variables involved in assessing home values:\n\n23 nominal\n23 ordinal\n14 discrete\n20 continuous\n\nPaper: https://jse.amstat.org/v19n3/decock.pdf"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#ames-housing-data-1",
    "href": "slides/01-linear-regression-old.html#ames-housing-data-1",
    "title": "Linear Regression",
    "section": "Ames housing data",
    "text": "Ames housing data\n\n\nShow R code\names &lt;- AmesHousing::make_ames()  # install.packages(\"AmesHousing\")\names$Sale_Price &lt;- ames$Sale_Price / 10000\nhead(ames)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#ames-housing-data-2",
    "href": "slides/01-linear-regression-old.html#ames-housing-data-2",
    "title": "Linear Regression",
    "section": "Ames housing data",
    "text": "Ames housing data\nWe‚Äôll focus on a handful of variables:\n\nSale_Price - Sale price of the house / $10K (response variable)\nGr_Liv_Area - Above grade (ground) living area square feet\nOverall_Qual‚Å† - Rates the overall material and finish of the house"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#statistical-relationships",
    "href": "slides/01-linear-regression-old.html#statistical-relationships",
    "title": "Linear Regression",
    "section": "Statistical relationships",
    "text": "Statistical relationships\n\n\nShow R code\n# Simulate data from different SLR models\nset.seed(101)  # for reproducibility\nx &lt;- seq(from = 0, to = 4, length = 100)\ny &lt;- cbind(\n  1 + x + rnorm(length(x)),  # linear\n  1 + (x - 2)^2 + rnorm(length(x)),  # quadratic\n  1 + log(x + 0.1) + rnorm(length(x), sd = 0.3),  # logarithmic\n  1 + rnorm(length(x))  # no association\n)\n\n# Scatterplot of X vs. each Y in a 2-by-2 grid\npar(mfrow = c(2, 2))\nfor (i in 1:4) {\n  plot(x, y[, i], col = adjustcolor(\"cornflowerblue\", alpha.f = 0.7),\n       pch = 19, xlab = \"X\", ylab = \"Y\")\n}"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#are-x-and-y-correlated",
    "href": "slides/01-linear-regression-old.html#are-x-and-y-correlated",
    "title": "Linear Regression",
    "section": "Are \\(X\\) and \\(Y\\) correlated?",
    "text": "Are \\(X\\) and \\(Y\\) correlated?\n\n\nShow R code\nplot(x, y[, 3], col = adjustcolor(\"cornflowerblue\", alpha.f = 0.7),\n     pch = 19, xlab = \"X\", ylab = \"Y\")\nr &lt;- round(cor(x, y[, 3]), digits = 3)\nlegend(\"bottomright\", legend = paste0(\"r = \", r), bty = \"n\", inset = 0.01)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#pearsons-correlation-coefficient",
    "href": "slides/01-linear-regression-old.html#pearsons-correlation-coefficient",
    "title": "Linear Regression",
    "section": "Pearson‚Äôs correlation coefficient",
    "text": "Pearson‚Äôs correlation coefficient\n\nThe (Pearson) correlation between two random variables \\(X\\) and \\(Y\\) is given by\n\n\\[Cor\\left(X, Y\\right) = \\rho = \\frac{Cov\\left(X,Y\\right)}{\\sigma_X\\sigma_Y}\\]\n\nGiven a sample of \\(n\\) pairs \\(\\left\\{\\left(x_i, y_i\\right)\\right\\}_{i=1}^n\\), we estimate \\(\\rho\\) with \\(r = S_{xy} / \\sqrt{S_{xx}S_{yy}}\\), where, for example, \\[S_{xx} = \\sum_{i=1}^n\\left(x_i - \\bar{x}\\right)^2 \\text{ and } S_{xy} = \\sum_{i=1}^n\\left(x_i - \\bar{x}\\right)\\left(y_i - \\bar{y}\\right)\\]"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#pearsons-correlation-coefficient-1",
    "href": "slides/01-linear-regression-old.html#pearsons-correlation-coefficient-1",
    "title": "Linear Regression",
    "section": "Pearson‚Äôs correlation coefficient",
    "text": "Pearson‚Äôs correlation coefficient\n\nRange: \\(-1 \\le r \\le 1\\)\nWhat does it measure?\n\nPearson‚Äôs correlation coefficient is a unitless measure of the strength of the linear relationship between two variables\n\nOther useful correlation measures also exist:\n\nSpearman‚Äôs rank correlation (or Spearman‚Äôs \\(\\rho\\)) only assumes a monotonic relationship between \\(X\\) and \\(Y\\)\n\nEquivalent to computing \\(r\\) on the ranks of \\(X\\) and \\(Y\\)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#pearsons-correlation-coefficient-2",
    "href": "slides/01-linear-regression-old.html#pearsons-correlation-coefficient-2",
    "title": "Linear Regression",
    "section": "Pearson‚Äôs correlation coefficient",
    "text": "Pearson‚Äôs correlation coefficient\n\nIt is common to test the hypothesis \\(H_0: \\rho = 0\\) vs.¬†\\(H_1: \\rho \\ne 0\\)\n\nRejecting \\(H_0\\) is only evidence that \\(\\rho\\) is not exactly zero (NOT VERY USEFUL, OR INTERESTING)\nA \\(p\\)-value does not measure the magnitude/strength of the (linear) association\nSample size affects the \\(p\\)-value! üò±"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#are-x-and-y-correlated-1",
    "href": "slides/01-linear-regression-old.html#are-x-and-y-correlated-1",
    "title": "Linear Regression",
    "section": "Are \\(x\\) and \\(y\\) correlated?",
    "text": "Are \\(x\\) and \\(y\\) correlated?\n\n\nShow R code\nset.seed(1051)  # for reproducibility\nn &lt;- 1000 \nx &lt;- rnorm(n)\ny &lt;- 1 + 0.001*x + rnorm(n)\nplot(x, y, asp = 1, col = adjustcolor(\"black\", alpha.f = 0.3))"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#pearsons-correlation-coefficient-3",
    "href": "slides/01-linear-regression-old.html#pearsons-correlation-coefficient-3",
    "title": "Linear Regression",
    "section": "Pearson‚Äôs correlation coefficient",
    "text": "Pearson‚Äôs correlation coefficient\n\n\n\n\nShow R code\nset.seed(1050)  # for reproducibility\nn &lt;- 100\nx &lt;- rnorm(n)\ny &lt;- 1 + 0.001*x + rnorm(n)\ncor.test(x, y)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y\nt = -0.0028012, df = 98, p-value = 0.9978\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1966901  0.1961461\nsample estimates:\n          cor \n-0.0002829617 \n\n\n\n\n\nShow R code\nset.seed(1051)  # for reproducibility\nn &lt;- 10000000  # n = ten million\nx &lt;- rnorm(n)\ny &lt;- 1 + 0.001*x + rnorm(n)\ncor.test(x, y)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y\nt = 3.731, df = 9999998, p-value = 0.0001907\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.0005600528 0.0017996412\nsample estimates:\n        cor \n0.001179847 \n\n\n\nThe real question is, are \\(X\\) and \\(Y\\) practically uncorrelated?"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#correlation-is-not-causation",
    "href": "slides/01-linear-regression-old.html#correlation-is-not-causation",
    "title": "Linear Regression",
    "section": "Correlation is not causation",
    "text": "Correlation is not causation"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#all-models-are-wrong",
    "href": "slides/01-linear-regression-old.html#all-models-are-wrong",
    "title": "Linear Regression",
    "section": "All models are wrong!",
    "text": "All models are wrong!\n\n\n\nAlso, see this talk by my old adviser, Thad Tarpey: ‚ÄúAll Models are Right‚Ä¶ most are useless.‚Äù"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#pearsons-correlation-vs.-slr",
    "href": "slides/01-linear-regression-old.html#pearsons-correlation-vs.-slr",
    "title": "Linear Regression",
    "section": "Pearson‚Äôs correlation vs.¬†SLR",
    "text": "Pearson‚Äôs correlation vs.¬†SLR\n\nThere‚Äôs a formal relationship between Pearson‚Äôs correlation coefficient (\\(\\rho\\)) and the SLR model\n‚ÄúSimple‚Äù linear relationships can be described by an intercept and slope:\n\n\\(y = mx + b\\) (algebra)\n\\(\\mu = \\beta_0 + \\beta_1x\\) (statistics)\n\n‚ÄúSimple‚Äù here means two variables, \\(x\\) and \\(y\\) (but \\(y\\) can be linearly related to several variables)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#example-ames-housing",
    "href": "slides/01-linear-regression-old.html#example-ames-housing",
    "title": "Linear Regression",
    "section": "Example: Ames housing",
    "text": "Example: Ames housing\nCheck out this paper for useful background on the Ames housing data in regression"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#example-ames-housing-1",
    "href": "slides/01-linear-regression-old.html#example-ames-housing-1",
    "title": "Linear Regression",
    "section": "Example: Ames housing",
    "text": "Example: Ames housing\n\n\nShow R code\nhead(cbind(ames$Sale_Price, ames$Gr_Liv_Area))\n\n\n      [,1] [,2]\n[1,] 21.50 1656\n[2,] 10.50  896\n[3,] 17.20 1329\n[4,] 24.40 2110\n[5,] 18.99 1629\n[6,] 19.55 1604\n\n\nShow R code\ncor.test(ames$Sale_Price, y = ames$Gr_Liv_Area)  # see ?cor.test\n\n\n\n    Pearson's product-moment correlation\n\ndata:  ames$Sale_Price and ames$Gr_Liv_Area\nt = 54.061, df = 2928, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6881814 0.7244502\nsample estimates:\n      cor \n0.7067799 \n\n\nThis doesn‚Äôt tell us much about the nature of the linear relationship between Gr_Liv_Area and Sale_Price"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#statistical-relationships-1",
    "href": "slides/01-linear-regression-old.html#statistical-relationships-1",
    "title": "Linear Regression",
    "section": "Statistical relationships",
    "text": "Statistical relationships\n\n\nShow R code\nlibrary(ggplot2) \n\np1 &lt;- ggplot(investr::crystal, aes(x = time, y = weight)) +\n  geom_point() +\n  labs(x = \"Time (hours)\", \n       y = \"Weight (grams)\", \n       title = \"Crystal weight data\")\np2 &lt;- ggplot(investr::arsenic, aes(x = actual, y = measured)) +\n  geom_point() +\n  labs(x = \"True amount of arsenic\", \n       y = \"Measured amount of arsenic\",\n       title = \"Arsenic concentration data\")\ngridExtra::grid.arrange(p1, p2, nrow = 1)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#examples-of-statistical-relationships",
    "href": "slides/01-linear-regression-old.html#examples-of-statistical-relationships",
    "title": "Linear Regression",
    "section": "Examples of statistical relationships",
    "text": "Examples of statistical relationships\n\nSimple linear regression: \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)\nMultiple linear regression: \\(Y = \\beta_0 + \\sum_{i=1}^p \\beta_p X_p + \\epsilon\\)\nPolynomial regression: \\(Y = \\beta_0 + \\sum_{i=1}^p \\beta_p X^p + \\epsilon\\)\nLogarithmic: \\(Y = \\beta_0 + \\beta_1 \\log\\left(X + 0.1\\right) + \\epsilon\\)\nNonlinear regression: \\(Y = \\frac{\\beta_1 X}{\\left(\\beta_2 + X\\right)} + \\epsilon\\)\nMultiplicative: \\(Y = \\beta X \\epsilon\\)\n\n\\(\\log\\left(Y\\right) = \\alpha + \\log\\left(X\\right) + \\log\\left(\\epsilon\\right)\\)\n\n\nAssuming \\(\\epsilon \\sim D\\left(\\mu, \\sigma\\right)\\)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#simple-linear-regression-slr-1",
    "href": "slides/01-linear-regression-old.html#simple-linear-regression-slr-1",
    "title": "Linear Regression",
    "section": "Simple linear regression (SLR)",
    "text": "Simple linear regression (SLR)\n\nData: \\(\\left\\{\\left(X_i, Y_i\\right)\\right\\}_{i=1}^n\\)\nModel: \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\)\n\n\\(Y_i\\) is a continuous response\n\\(X_i\\) is a continuous predictor\n\\(\\beta_0\\) is the intercept of the regression line (also called the bias term)\n\\(\\beta_1\\) is the slope of the regression line\n\\(\\epsilon_i \\stackrel{iid}{\\sim} D\\left(0, \\sigma^2\\right)\\)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#assumptions-about-the-errors-epsilon_i",
    "href": "slides/01-linear-regression-old.html#assumptions-about-the-errors-epsilon_i",
    "title": "Linear Regression",
    "section": "Assumptions about the errors \\(\\epsilon_i\\)",
    "text": "Assumptions about the errors \\(\\epsilon_i\\)\nFor \\(i\\) and \\(j\\) in \\(\\left\\{1, 2, \\dots, n\\right\\}\\) and \\(i \\ne j\\)\n\n\\(\\quad E\\left(\\epsilon_i\\right) = 0\\)\n\\(\\quad Var\\left(\\epsilon_i\\right) = \\sigma^2\\) (homoscedacticity üò±)\n\\(\\quad Cov\\left(\\epsilon_i, \\epsilon_j\\right) = 0\\) (independence)\n\nAssumptions 1‚Äì3 can be summarized as \\(\\epsilon_i \\stackrel{iid}{\\sim} D\\left(0, \\sigma^2\\right)\\), where \\(iid\\) refers to independent and identically distributed."
  },
  {
    "objectID": "slides/01-linear-regression-old.html#properties-of-slr",
    "href": "slides/01-linear-regression-old.html#properties-of-slr",
    "title": "Linear Regression",
    "section": "Properties of SLR",
    "text": "Properties of SLR\n\nSimple linear regression: \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\)\n\nAssumes the model is linear in the regression coefficients \\(\\beta_0\\) and \\(\\beta_1\\)\n\nThe error term is a random variable; hence, \\(Y_i\\) is also a random variable (Why? ü§î)\n\nWhat is \\(E\\left(Y_i|X_i\\right)\\) and \\(Var\\left(Y_i|X_i\\right)\\)?\n\n\\(Cor\\left(Y_i, Y_j\\right) = 0\\) \\(\\forall i \\ne j\\) (Why? ü§î)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#inference-for-a-single-variable",
    "href": "slides/01-linear-regression-old.html#inference-for-a-single-variable",
    "title": "Linear Regression",
    "section": "Inference for a single variable",
    "text": "Inference for a single variable\n\nIs it useful to test the hypothesis that Sale_Price = $160K?\n\n\n\nNo! Because Sale_Price is not a constant, but a random variable whose value varies from home to home (and year to year, etc.)\n\n\n\n\nWe‚Äôre more interested in questions such as:\n\nWhat is the chance that Sale_Price &gt; $160K? (above median sale price)\nWhat is the chance that Sale_Price &lt; $105K? (lowest decile)\nWhat is the chance that $129,500 &lt; Sale_Price &lt; $213,500? (within IQR)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#distribution-of-sale_price",
    "href": "slides/01-linear-regression-old.html#distribution-of-sale_price",
    "title": "Linear Regression",
    "section": "Distribution of Sale_Price",
    "text": "Distribution of Sale_Price\nCan look at historgram and empirical CDF:\n\n\nShow R code\npar(mfrow = c(1, 2), las = 1)\nhist(ames$Sale_Price, br = 50, xlab = \"Sale price ($)\", freq = FALSE, main = \"\")\nplot(ecdf(ames$Sale_Price), xlab = \"Sale price ($)\", main = \"\",\n     col = adjustcolor(1, alpha.f = 0.1))"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#distribution-of-sale_price-1",
    "href": "slides/01-linear-regression-old.html#distribution-of-sale_price-1",
    "title": "Linear Regression",
    "section": "Distribution of Sale_Price",
    "text": "Distribution of Sale_Price\n\nHistograms and ECDFs are nonparammetric in nature\nA simple parametric approach might assume a particular distribution for Sale_Price\nFor instance, we might assume Sale_Price \\(\\sim N\\left(\\mu, \\sigma^2\\right)\\)\nHow can we estimate \\(\\mu\\) and \\(\\sigma^2\\)?\n\n\n\n\nShow R code\n# Maximum likelihiid estimates\nc(\"sample mean\" = mean(ames$Sale_Price), \"sample stdev\" = sd(ames$Sale_Price))\n\n\n sample mean sample stdev \n   18.079606     7.988669 \n\n\n\n\n\nIs the normal distribution a reasonable assumption here?"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#normal-qq-plot",
    "href": "slides/01-linear-regression-old.html#normal-qq-plot",
    "title": "Linear Regression",
    "section": "Normal QQ plot",
    "text": "Normal QQ plot\n\nNormal quantile-quantile (Q-Q) plot* can be used to asses the ‚Äúnormalityness‚Äù of a set of observations\nQ-Q plots can, in general, be used to compare data with any distribution!\n\n\n\nShow R code\nqqnorm(ames$Sale_Price, col = 2, las = 1)\nqqline(ames$Sale_Price)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#normality-tests",
    "href": "slides/01-linear-regression-old.html#normality-tests",
    "title": "Linear Regression",
    "section": "Normality tests ü§Æ",
    "text": "Normality tests ü§Æ\n\nNormality tests, like the Shapiro-Wilk1 and Anderson-Darling tests, can also be used to assess normality\n\nI STRONGLY ADVISE AGAINST USING THEM!\n\nNo data is normally distributes, what we care about is whether enough a normal approximation is close enough!\nNormality tests provide a \\(p\\)-value, which only gives a yes/no conclusion\n\nIn R, see ?shapiro.test for details."
  },
  {
    "objectID": "slides/01-linear-regression-old.html#normality-tests-1",
    "href": "slides/01-linear-regression-old.html#normality-tests-1",
    "title": "Linear Regression",
    "section": "Normality tests ü§Æ",
    "text": "Normality tests ü§Æ\nRecall that \\(p\\)-values are a function of sample size!\n\n\nShow R code\n# Shapiro-Wilk test results vs. sample size\nset.seed(101)  # for reproducibility\nx &lt;- replicate(100, c(\n  shapiro.test(rt(10, df = 40))$p.value,\n  shapiro.test(rt(100, df = 40))$p.value,\n  shapiro.test(rt(500, df = 40))$p.value,\n  shapiro.test(rt(1000, df = 40))$p.value,\n  shapiro.test(rt(2500, df = 40))$p.value,\n  shapiro.test(rt(5000, df = 40))$p.value\n))\nrownames(x) &lt;- paste0(\"n=\", c(10, 100, 500, 1000, 2500, 5000))\nrowMeans(x &lt; 0.05)\n\n\n  n=10  n=100  n=500 n=1000 n=2500 n=5000 \n  0.01   0.08   0.11   0.22   0.24   0.39"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#normality-tests-2",
    "href": "slides/01-linear-regression-old.html#normality-tests-2",
    "title": "Linear Regression",
    "section": "Normality tests ü§Æ",
    "text": "Normality tests ü§Æ\n\n\nAre these two distributions significantly different?\nAre these two distributions practically different?"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#what-can-we-do-if-the-normality-assumption-isnt-justified",
    "href": "slides/01-linear-regression-old.html#what-can-we-do-if-the-normality-assumption-isnt-justified",
    "title": "Linear Regression",
    "section": "What can we do if the normality assumption isn‚Äôt justified?",
    "text": "What can we do if the normality assumption isn‚Äôt justified?\n\nTry transformations\n\nLogarithm or square root for positive data\nPower transformation (like the well-known Box-Cox procedure)\n\nTry a more appropriate distribution (e.g., Poisson or gamma distribution)\nTry more advanced approaches, like the nonparametric bootstrapping!"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#modeling-the-mean-response",
    "href": "slides/01-linear-regression-old.html#modeling-the-mean-response",
    "title": "Linear Regression",
    "section": "Modeling the mean response",
    "text": "Modeling the mean response\n\nAssume that \\(Y \\sim N\\left(\\mu, \\sigma^2\\right)\\), where\n\n\\[\\mu = \\mu\\left(x\\right) = \\beta_0 + \\beta_1 x = E\\left(Y|x\\right)\\]\n\n\nIn other words: \\(Y \\sim N\\left(\\beta_0 + \\beta_1 x, \\sigma^2\\right)\\)\nAlternatively, we could write \\(Y = \\beta_0 + \\beta_1 x + \\epsilon\\), where \\(\\epsilon \\sim N\\left(0, \\sigma^2\\right)\\)\nThis is called the simple linear regression (SLR) model"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#the-idea-behind-slr",
    "href": "slides/01-linear-regression-old.html#the-idea-behind-slr",
    "title": "Linear Regression",
    "section": "The idea behind SLR",
    "text": "The idea behind SLR\n\nImage source"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#arsenic-experiment-example",
    "href": "slides/01-linear-regression-old.html#arsenic-experiment-example",
    "title": "Linear Regression",
    "section": "Arsenic experiment example",
    "text": "Arsenic experiment example\n\n\nShow R code\nplot(investr::arsenic, las = 1)  # see ?investr::arsenic for details"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#is-linear-regression-reasonable-here",
    "href": "slides/01-linear-regression-old.html#is-linear-regression-reasonable-here",
    "title": "Linear Regression",
    "section": "Is linear regression reasonable here?",
    "text": "Is linear regression reasonable here?\n\n\nShow R code\nx &lt;- rep(c(1:10 / 10, 1.5, 2), each = 30)\ny &lt;- 1 + 2*x^2 + rnorm(length(x), sd = 1)\npar(mfrow = c(1, 2), las = 1)\nplot(x, y, col = \"dodgerblue2\")\n#abline(lm(y ~ x), lwd = 2)\nhist(y, main = \"\", col = \"dodgerblue2\", border = \"white\")"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#is-linear-regression-reasonable-here-1",
    "href": "slides/01-linear-regression-old.html#is-linear-regression-reasonable-here-1",
    "title": "Linear Regression",
    "section": "Is linear regression reasonable here?",
    "text": "Is linear regression reasonable here?\n\n\nShow R code\nfit &lt;- lm(y ~ x)\nres &lt;- residuals(fit)\nqqnorm(res, las = 1)\nqqline(res, col = 2)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#least-squares-ls-estimation",
    "href": "slides/01-linear-regression-old.html#least-squares-ls-estimation",
    "title": "Linear Regression",
    "section": "Least squares (LS) estimation",
    "text": "Least squares (LS) estimation\nIdea of LS is to find \\(\\beta_0\\) and \\(\\beta_1\\) so that the sum of squared residuals (i.e., errors) is minimized: \\[SSE = \\sum_{i=1}^n\\left(y_i - \\beta_0 - \\beta_1x_i\\right)^2\\]\n\nPretty straightforward optimization problem that leads to closed-form solution (but no point in memorizing the formulas!)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#concept-of-ls-estimation",
    "href": "slides/01-linear-regression-old.html#concept-of-ls-estimation",
    "title": "Linear Regression",
    "section": "Concept of LS estimation",
    "text": "Concept of LS estimation\n\nImage source"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#sale_price-and-gr_liv_area",
    "href": "slides/01-linear-regression-old.html#sale_price-and-gr_liv_area",
    "title": "Linear Regression",
    "section": "Sale_Price and Gr_Liv_Area",
    "text": "Sale_Price and Gr_Liv_Area\n\n\nShow R code\nplot(Sale_Price ~ Gr_Liv_Area, data = ames, las = 1,\n     col = adjustcolor(1, alpha.f = 0.3))"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#slr-fit",
    "href": "slides/01-linear-regression-old.html#slr-fit",
    "title": "Linear Regression",
    "section": "SLR fit",
    "text": "SLR fit\n\n\nShow R code\nsummary(fit &lt;- lm(Sale_Price ~ Gr_Liv_Area, data = ames))\n\n\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area, data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.347  -3.022  -0.197   2.273  33.432 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.3289634  0.3269703   4.064 4.94e-05 ***\nGr_Liv_Area 0.0111694  0.0002066  54.061  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.652 on 2928 degrees of freedom\nMultiple R-squared:  0.4995,    Adjusted R-squared:  0.4994 \nF-statistic:  2923 on 1 and 2928 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#is-this-a-good-fit",
    "href": "slides/01-linear-regression-old.html#is-this-a-good-fit",
    "title": "Linear Regression",
    "section": "Is this a good fit?",
    "text": "Is this a good fit?\n\n\nShow R code\nplot(Sale_Price ~ Gr_Liv_Area, data = ames,\n     col = adjustcolor(1, alpha.f = 0.3))\nabline(fit, lwd = 2, col = 2)  # add SLR fit\n\n\n\nWhich assumptions seem violated to some degree?"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#residual-diagnostics",
    "href": "slides/01-linear-regression-old.html#residual-diagnostics",
    "title": "Linear Regression",
    "section": "Residual diagnostics",
    "text": "Residual diagnostics\n\nThe standard residual is defined as \\(e_i = y_i - \\hat{y}_i\\) and can be regarded as the observed error\nThe residuals hold a lot of properties that make them useful for diagnosing potential issues with the model (e.g., suggesting potential transformations to try)\nMany other kinds of residuals exist for different purposes (e.g., standardized, studentized, jackknife or PRESS residuals, etc.)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#properties-of-the-residuals",
    "href": "slides/01-linear-regression-old.html#properties-of-the-residuals",
    "title": "Linear Regression",
    "section": "Properties of the residuals",
    "text": "Properties of the residuals\n\n\\(\\sum_{i=1}^n e_i = 0\\)\n\\(\\sum_{i=1}^n e_i^2\\) is a minimum\n\\(\\sum_{i=1}^n X_ie_i = 0\\)\n\\(\\sum_{i=1}^n \\hat{Y}_ie_i = 0\\)\nThe LS regression line passes through the point \\(\\left(\\bar{X}, \\bar{Y}\\right)\\) (i.e., the center of the training data)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#what-can-residual-plots-tell-us",
    "href": "slides/01-linear-regression-old.html#what-can-residual-plots-tell-us",
    "title": "Linear Regression",
    "section": "What can residual plots tell us?",
    "text": "What can residual plots tell us?\n\nResiduals vs.¬†predictor values (checking non-linearity).\nResiduals vs.¬†fitted values (non-constant variance, non-linearity, and outliers)\nResiduals vs.¬†time or another sequence (checking independence)\nResiduals vs.¬†omitted predictor values (missing potentially important predictors)\nNormal QQ plot of residuals (non-normality).\nAnd much, much more!"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#sale_price-gr_liv_area",
    "href": "slides/01-linear-regression-old.html#sale_price-gr_liv_area",
    "title": "Linear Regression",
    "section": "Sale_Price ~ Gr_Liv_Area",
    "text": "Sale_Price ~ Gr_Liv_Area\nResidual analysis:\n\n\nShow R code\npar(mfrow = c(2, 3), las = 1)\nplot(fit, which = 1:6)\n\n\n\nWhat assumptions appear to be in violation?"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#lets-try-a-log-transformation",
    "href": "slides/01-linear-regression-old.html#lets-try-a-log-transformation",
    "title": "Linear Regression",
    "section": "Let‚Äôs try a log transformation",
    "text": "Let‚Äôs try a log transformation\n\n\nShow R code\nfit2 &lt;- lm(log(Sale_Price) ~ Gr_Liv_Area, data = ames)\nplot(log(Sale_Price) ~ Gr_Liv_Area, data = ames,\n     col = adjustcolor(1, alpha.f = 0.3))\nabline(fit2, lwd = 2, col = 2)  # add SLR fit"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#lets-try-a-log-transformation-1",
    "href": "slides/01-linear-regression-old.html#lets-try-a-log-transformation-1",
    "title": "Linear Regression",
    "section": "Let‚Äôs try a log transformation",
    "text": "Let‚Äôs try a log transformation\n\n\nShow R code\nsummary(fit2)\n\n\n\nCall:\nlm(formula = log(Sale_Price) ~ Gr_Liv_Area, data = ames)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.36215 -0.15145  0.03091  0.16583  0.90332 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.9692014  0.0169355  116.28   &lt;2e-16 ***\nGr_Liv_Area 0.0005611  0.0000107   52.43   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2928 on 2928 degrees of freedom\nMultiple R-squared:  0.4842,    Adjusted R-squared:  0.484 \nF-statistic:  2749 on 1 and 2928 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#logsale_price-gr_liv_area",
    "href": "slides/01-linear-regression-old.html#logsale_price-gr_liv_area",
    "title": "Linear Regression",
    "section": "log(Sale_Price) ~ Gr_Liv_Area",
    "text": "log(Sale_Price) ~ Gr_Liv_Area\nResidual analysis:\n\n\nShow R code\npar(mfrow = c(2, 3), las = 1)\nplot(fit2, which = 1:6)\n\n\n\nAny better?"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#mlr-in-a-nutshell",
    "href": "slides/01-linear-regression-old.html#mlr-in-a-nutshell",
    "title": "Linear Regression",
    "section": "MLR in a nutshell ü•ú",
    "text": "MLR in a nutshell ü•ú\n\nThe (normal) multiple linear regression model assumes \\(Y \\sim N\\left(\\mu\\left(\\boldsymbol{x}\\right), \\sigma^2\\right)\\), where \\[\\mu\\left(\\boldsymbol{x}\\right) = \\beta_0 + \\sum_{i=1}^p \\beta_i x_i = \\boldsymbol{x}^\\top\\boldsymbol{\\beta}\\]\nLS estimation still provides unbiased estimate of \\(\\boldsymbol{\\beta} = \\left(\\beta_0, \\beta_1, \\dots, \\beta_p\\right)^\\top\\): \\(\\hat{\\boldsymbol{\\beta}} = \\left(\\boldsymbol{X}^\\top\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y}\\)\nFitted values: \\(\\hat{\\boldsymbol{y}} = \\boldsymbol{X}\\left(\\boldsymbol{X}^\\top\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y} = \\boldsymbol{H}\\boldsymbol{y}\\)\n\\(\\boldsymbol{H}\\) is the well-known ‚Äúhat matrix‚Äù"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#polynomial-regression",
    "href": "slides/01-linear-regression-old.html#polynomial-regression",
    "title": "Linear Regression",
    "section": "Polynomial regression",
    "text": "Polynomial regression\n\nPolynomial regression is just a special case of the MLR model\nA second order model in a single predictor: \\[Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon\\]\nA k-th order model in a single predictor (Typically \\(k \\le 3\\)): \\[Y = \\beta_0 + \\sum_{j=1}^k\\beta_j X^j + \\epsilon\\]"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#example-paper-strength-data",
    "href": "slides/01-linear-regression-old.html#example-paper-strength-data",
    "title": "Linear Regression",
    "section": "Example: paper strength data",
    "text": "Example: paper strength data\nData concerning the strength of kraft paper and the percentage of hardwood in the batch of pulp from which the paper was produced.\n\n\nShow R code\n# Load the hardwood conentration data\nurl &lt;- \"https://bgreenwell.github.io/uc-bana7052/data/hardwood.csv\"\nhardwood &lt;- read.csv(url)\n\n# Print first few observations\nhead(hardwood)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#example-paper-strength-data-1",
    "href": "slides/01-linear-regression-old.html#example-paper-strength-data-1",
    "title": "Linear Regression",
    "section": "Example: paper strength data",
    "text": "Example: paper strength data\n\n\nShow R code\nplot(hardwood, pch = 19)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#example-paper-strength-data-2",
    "href": "slides/01-linear-regression-old.html#example-paper-strength-data-2",
    "title": "Linear Regression",
    "section": "Example: paper strength data",
    "text": "Example: paper strength data\n\n\nShow R code\nfit1 &lt;- lm(TsStr ~ HwdCon, data = hardwood)\ninvestr::plotFit(fit1, pch = 19, col.fit = \"red2\")"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#example-paper-strength-data-3",
    "href": "slides/01-linear-regression-old.html#example-paper-strength-data-3",
    "title": "Linear Regression",
    "section": "Example: paper strength data",
    "text": "Example: paper strength data\n\n\nShow R code\npar(mfrow = c(1, 2), las = 1)\n\n# Plot residuals vs HwdCon (i.e., X)\npar(mfrow = c(1, 2))\nplot(x = hardwood$HwdCon, y = residuals(fit1), xlab = \"HwdCon\",\n     ylab = \"Residuals\", main = \"Residuals vs HwdCon\")\nabline(h = 0, lty = \"dotted\")\nplot(fit1, which = 1, caption = \"\", main = \"Residuals vs Fitted\")"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#example-paper-strength-data-4",
    "href": "slides/01-linear-regression-old.html#example-paper-strength-data-4",
    "title": "Linear Regression",
    "section": "Example: paper strength data",
    "text": "Example: paper strength data\n\n\nShow R code\nfit2 &lt;- lm(TsStr ~ HwdCon + I(HwdCon^2), data = hardwood)\ninvestr::plotFit(fit2, pch = 19, col.fit = \"red2\")"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#example-paper-strength-data-5",
    "href": "slides/01-linear-regression-old.html#example-paper-strength-data-5",
    "title": "Linear Regression",
    "section": "Example: paper strength data",
    "text": "Example: paper strength data\n\n\nShow R code\npar(mfrow = c(2, 3), las = 1)\nfor (i in 1:6) {  # try higher-order models\n  fit &lt;- lm(TsStr ~ poly(HwdCon, degree = i), data = hardwood)\n  investr::plotFit(fit, main = paste(\"Degree =\", i))\n}"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#example-paper-strength-data-6",
    "href": "slides/01-linear-regression-old.html#example-paper-strength-data-6",
    "title": "Linear Regression",
    "section": "Example: paper strength data",
    "text": "Example: paper strength data\n\n\nShow R code\npar(mfrow = c(2, 3))\nfor (i in 1:6) {  # try higher-order models\n  fit &lt;- lm(TsStr ~poly(HwdCon, degree = i), data = hardwood)\n  investr::plotFit(fit, main = paste(\"Degree =\", i), \n                   interval = \"confidence\", shade = TRUE,\n                   xlim = c(-10, 30))\n}"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#polynomial-regression-1",
    "href": "slides/01-linear-regression-old.html#polynomial-regression-1",
    "title": "Linear Regression",
    "section": "Polynomial regression",
    "text": "Polynomial regression\nSome cautions ‚ö†Ô∏è\nKeep the order of the model as low as possible\n\nAvoid interpolating the data or over fitting\nUse the simplest model possible to explain the data, but no simpler (parsimony)\nAn \\(n - 1\\) order model can perfectly fit a data set with \\(n\\) observations (Why is this bad ü§î)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#polynomial-regression-2",
    "href": "slides/01-linear-regression-old.html#polynomial-regression-2",
    "title": "Linear Regression",
    "section": "Polynomial regression",
    "text": "Polynomial regression\nTwo model-building strategies:\n\nFit the lowest order polynomial possible and build up (forward selection)\nFit the highest order polynomial of interest, and remove terms one at a time (backward elimination)\n\nThese two procedures may not result in the same final model\nIncreasing the order can result in an ill-conditioned \\(\\boldsymbol{X}^\\top\\boldsymbol{X}\\) and multicollinearity"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#categorical-variables",
    "href": "slides/01-linear-regression-old.html#categorical-variables",
    "title": "Linear Regression",
    "section": "Categorical variables",
    "text": "Categorical variables\n\nCategorical variables can be handled in a number of ways in linear models, including\n\nDummy encoding (nominal)\nOrthogonal polynomials (ordinal)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#categorical-variables-1",
    "href": "slides/01-linear-regression-old.html#categorical-variables-1",
    "title": "Linear Regression",
    "section": "Categorical variables",
    "text": "Categorical variables\nLet‚Äôs look at two (nominal) categorical variables:\n\n\nShow R code\ntable(ames$Central_Air)\n\n\n\n   N    Y \n 196 2734 \n\n\nShow R code\ntable(ames$Paved_Drive)\n\n\n\n     Dirt_Gravel Partial_Pavement            Paved \n             216               62             2652"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#categorical-variables-2",
    "href": "slides/01-linear-regression-old.html#categorical-variables-2",
    "title": "Linear Regression",
    "section": "Categorical variables",
    "text": "Categorical variables\n\n\nShow R code\nplot(log(Sale_Price) ~ Central_Air, data = ames, las = 1, col = c(2, 3))"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#categorical-variables-3",
    "href": "slides/01-linear-regression-old.html#categorical-variables-3",
    "title": "Linear Regression",
    "section": "Categorical variables",
    "text": "Categorical variables\n\n\nShow R code\nplot(log(Sale_Price) ~ Paved_Drive, data = ames, las = 1, col = c(2, 3, 4))\n\n\n\nIf one of these homes downgraded from a paved driveway to a gravel driveway, would that cause the sale price to decrease? (Think very carefully here!)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#categorical-variables-4",
    "href": "slides/01-linear-regression-old.html#categorical-variables-4",
    "title": "Linear Regression",
    "section": "Categorical variables",
    "text": "Categorical variables\nR dummy encodes nominal factors by default:\n\n\nShow R code\nfit3 &lt;- lm(log(Sale_Price) ~ Gr_Liv_Area + Central_Air + Paved_Drive, \n           data = ames)\nsummary(fit3)\n\n\n\nCall:\nlm(formula = log(Sale_Price) ~ Gr_Liv_Area + Central_Air + Paved_Drive, \n    data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2344 -0.1345  0.0073  0.1453  0.8502 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 1.428e+00  2.418e-02  59.059  &lt; 2e-16 ***\nGr_Liv_Area                 5.183e-04  9.520e-06  54.440  &lt; 2e-16 ***\nCentral_AirY                3.464e-01  2.068e-02  16.747  &lt; 2e-16 ***\nPaved_DrivePartial_Pavement 1.334e-01  3.733e-02   3.574 0.000358 ***\nPaved_DrivePaved            3.085e-01  1.975e-02  15.618  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2574 on 2925 degrees of freedom\nMultiple R-squared:  0.6018,    Adjusted R-squared:  0.6013 \nF-statistic:  1105 on 4 and 2925 DF,  p-value: &lt; 2.2e-16\n\n\nHow do you interpret the coefficients here?"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#coefficient-of-determination",
    "href": "slides/01-linear-regression-old.html#coefficient-of-determination",
    "title": "Linear Regression",
    "section": "Coefficient of determination",
    "text": "Coefficient of determination\nThe coefficient of determination is the proportion of the variance in the dependent variable that is predictable from the independent variables in the model.\n\n\nR-squared (\\(R^2\\))\n\n\\(R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\\)\n\\(R^2\\) will always increase as more terms are added to the model!\n\n\nAdjusted R-squared (\\(R_{adj}^2\\))\n\n\n\\(R_{adj}^2 = 1 - \\frac{MSE}{SST/\\left(n - 1\\right)}\\)\nPenalizes \\(R^2\\) if there are ‚Äútoo many‚Äù terms in the model\n\\(R_{adj}^2\\) and \\(MSE\\) provide equivalent information"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#variablemodel-selection",
    "href": "slides/01-linear-regression-old.html#variablemodel-selection",
    "title": "Linear Regression",
    "section": "Variable/model selection",
    "text": "Variable/model selection\n\nVariable/model selection is a very noisy problem! (Often best to avoid, if feasible)\nAsk the domain experts about important variables (don‚Äôt just rely on algorithms)\nP(selecting the ‚Äúright‚Äù variables) = 0 (source)\n‚ÄúAll models subsets of variables are wrong, but some are useful!‚Äù\nIn regression settings, regularization (e.g., ridge regression and the LASSO) is often more useful! (Think about the impact of multicollinearity on variable selection)"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#data-splitting",
    "href": "slides/01-linear-regression-old.html#data-splitting",
    "title": "Linear Regression",
    "section": "Data splitting",
    "text": "Data splitting\n\nIf prediction is the goal (e.g., compared to inference and hypothesis testing), the model performance should be assessed rigorously\nData splitting techniques ae key and the type of data splitting to use often depends on the situation (e.g., cross-sectional vs.¬†time-series data)\nIn simplest terms, random split the data into two parts: A and B. Build a model on part A and see how well it does with predicting the response in part B.\nLeakage is a huge concern here, so data splitting ALWAYS has to be done carefully!"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#data-splitting-k-fold-cross-validation",
    "href": "slides/01-linear-regression-old.html#data-splitting-k-fold-cross-validation",
    "title": "Linear Regression",
    "section": "Data splitting: \\(k\\)-fold cross-validation",
    "text": "Data splitting: \\(k\\)-fold cross-validation\n\nThe PRESS statistic in linear regression is a special case (\\(k = n\\)) we get for free!"
  },
  {
    "objectID": "slides/01-linear-regression-old.html#questions",
    "href": "slides/01-linear-regression-old.html#questions",
    "title": "Linear Regression",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#multinomial-data",
    "href": "slides/05-multinomial-ordinal-regression.html#multinomial-data",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "Multinomial data",
    "text": "Multinomial data\n\nThe multinomial distribution is an extension of the binomial where the outcome can take on more than two values\nThe categories can be nominal (e.g., have no natural ordering) or ordinal in nature (e.g., low &lt; medium &lt; high)\nWe‚Äôll start with the case of a nominal outcome having more than two categories\nLet \\(Y\\) be a discrete r.v. that can take on one of \\(J\\) categories with \\(\\left\\{\\mathrm{P}\\left(Y = j\\right) = p_j\\right\\}_{j=1}^J\\), where \\(\\sum_{j=1}^Jp_j=1\\)",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#multinomial-logit-model",
    "href": "slides/05-multinomial-ordinal-regression.html#multinomial-logit-model",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "Multinomial logit model",
    "text": "Multinomial logit model\n\nSimilar to binary regression, we need a way to link the probabilities \\(p_j\\) to the predictors \\(\\boldsymbol{x} = \\left(x_1, x_2, \\dots, x_p\\right)^\\top\\)\nNeed to ensure each \\(0 \\le p_j \\le 1\\) and that \\(\\sum_{j=1}^Jp_j = 1\\)\nIdea is similar to fitting several logistic regressions using one of the categories as the reference or baseline (say, \\(j = 1\\))\nTo that end, we define the \\(J-1\\) logits \\(\\eta_j = \\boldsymbol{x}^\\top\\boldsymbol{\\beta}_j = \\log\\left(p_j / p_1\\right)\\), where \\(j \\ne 1\\)\nNotice we have a set of coefficients for each comparison!",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#multinomial-logit-model-1",
    "href": "slides/05-multinomial-ordinal-regression.html#multinomial-logit-model-1",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "Multinomial logit model",
    "text": "Multinomial logit model\n\nTo ensure \\(\\sum_{j=1}^Jp_j = 1\\), we have \\[\np_i = \\frac{\\exp\\left(\\eta_i\\right)}{1 + \\sum_{j=2}^J\\exp\\left(\\eta_j\\right)}\n\\]\nThis implies that \\(p_1 = 1 - \\sum_{j=2}^Jp_j\\)",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-1",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-1",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\n\nWe‚Äôll consider a sample taken from a subset of the 1996 American National Election Study\nContained in the nes96 data frame in the faraway üì¶\n\n\n\nShow R code\nstr(nes96 &lt;- faraway::nes96)\n\n\n'data.frame':   944 obs. of  10 variables:\n $ popul : int  0 190 31 83 640 110 100 31 180 2800 ...\n $ TVnews: int  7 1 7 4 7 3 7 1 7 0 ...\n $ selfLR: Ord.factor w/ 7 levels \"extLib\"&lt;\"Lib\"&lt;..: 7 3 2 3 5 3 5 5 4 3 ...\n $ ClinLR: Ord.factor w/ 7 levels \"extLib\"&lt;\"Lib\"&lt;..: 1 3 2 4 6 4 6 4 6 3 ...\n $ DoleLR: Ord.factor w/ 7 levels \"extLib\"&lt;\"Lib\"&lt;..: 6 5 6 5 4 6 4 5 3 7 ...\n $ PID   : Ord.factor w/ 7 levels \"strDem\"&lt;\"weakDem\"&lt;..: 7 2 2 2 1 2 2 5 4 1 ...\n $ age   : int  36 20 24 28 68 21 77 21 31 39 ...\n $ educ  : Ord.factor w/ 7 levels \"MS\"&lt;\"HSdrop\"&lt;..: 3 4 6 6 6 4 4 4 4 3 ...\n $ income: Ord.factor w/ 24 levels \"$3Kminus\"&lt;\"$3K-$5K\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ vote  : Factor w/ 2 levels \"Clinton\",\"Dole\": 2 1 1 1 1 1 1 1 1 1 ...",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-2",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-2",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\n\nFor simplicity, we‚Äôll consider three variables: age, education level, and income groups of each respondent\nSome of the factors are ‚Äúordered‚Äù by default; E.g., income (respondent‚Äôs education level):\n\n\nlevels(nes96$income)\n\n [1] \"$3Kminus\"   \"$3K-$5K\"    \"$5K-$7K\"    \"$7K-$9K\"    \"$9K-$10K\"  \n [6] \"$10K-$11K\"  \"$11K-$12K\"  \"$12K-$13K\"  \"$13K-$14K\"  \"$14K-$15K\" \n[11] \"$15K-$17K\"  \"$17K-$20K\"  \"$20K-$22K\"  \"$22K-$25K\"  \"$25K-$30K\" \n[16] \"$30K-$35K\"  \"$35K-$40K\"  \"$40K-$45K\"  \"$45K-$50K\"  \"$50K-$60K\" \n[21] \"$60K-$75K\"  \"$75K-$90K\"  \"$90K-$105K\" \"$105Kplus\" \n\n\n\nSame goes for PID and educ (respondent‚Äôs party identification and education, respectively)",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-3",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-3",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nHere‚Äôs a cleaned up version of the data we‚Äôll work with:\n\n\nShow R code\n# Condense party identification (PID) column into three categories\nparty &lt;- nes96$PID\nlevels(party) &lt;- c(\n  \"Democrat\", \"Democrat\",\n  \"Independent\", \"Independent\", \"Independent\", \n  \"Republican\", \"Republican\"\n)\n\n# Convert income to numeric\ninca &lt;- c(1.5, 4, 6, 8, 9.5, 10.5, 11.5, 12.5, 13.5, 14.5, 16, 18.5, 21, 23.5,\n          27.5, 32.5, 37.5, 42.5, 47.5, 55, 67.5, 82.5, 97.5, 115)\nincome &lt;- inca[unclass(nes96$income)]\n\n# Construct new data set for analysis\nrnes96 &lt;- data.frame(\n  \"party\" = party, \n  \"income\" = income, \n  \"education\" = nes96$educ, \n  \"age\" = nes96$age\n)\n\n# Print summary of data set\nsummary(rnes96)\n\n\n         party         income        education        age       \n Democrat   :380   Min.   :  1.50   MS    : 13   Min.   :19.00  \n Independent:239   1st Qu.: 23.50   HSdrop: 52   1st Qu.:34.00  \n Republican :325   Median : 37.50   HS    :248   Median :44.00  \n                   Mean   : 46.58   Coll  :187   Mean   :47.04  \n                   3rd Qu.: 67.50   CCdeg : 90   3rd Qu.:58.00  \n                   Max.   :115.00   BAdeg :227   Max.   :91.00  \n                                    MAdeg :127",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-4",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-4",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nLet‚Äôs continue with some visual exploration:\n\n\nShow R code\nlibrary(dplyr)\nlibrary(ggplot2)\n\ntheme_set(theme_bw())\n\n# Aggregate data; what's happening here?\negp &lt;- group_by(rnes96, education, party) %&gt;% \n  summarise(count = n()) %&gt;%\n  group_by(education) %&gt;% \n  mutate(etotal = sum(count), proportion = count/etotal)\n\n# Plot results\nggplot(egp, aes(x = education, y = proportion, group = party, \n                linetype = party, color = party)) + \n  geom_line(size = 2)",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-5",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-5",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nLet‚Äôs continue with some visual exploration:\n\n\nShow R code\n# Aggregate data; what's happening here?\nigp &lt;- mutate(rnes96, income_group = cut_number(income, 7)) %&gt;% \n  group_by(income_group, party) %&gt;% \n  summarise(count = n()) %&gt;% \n  group_by(income_group) %&gt;% \n  mutate(etotal = sum(count), proportion = count / etotal)\n\n# Plot results\nggplot(igp, aes(x = income_group, y = proportion, group = party, \n                linetype = party, color = party)) +\n  scale_x_discrete(guide = guide_axis(angle = 45)) +\n  geom_line(size = 2)",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-6",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-6",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nLet‚Äôs continue with some visual exploration:\n\n\nShow R code\n# Aggregate data; what's happening here?\nagp &lt;- rnes96 %&gt;% \n  group_by(age, party) %&gt;% \n  summarise(count = n()) %&gt;% \n  group_by(age) %&gt;% \n  mutate(etotal = sum(count), proportion = count / etotal)\n\n# Plot results\nggplot(agp, aes(x = age, y = proportion, group = party, \n                linetype = party, color = party)) +\n  geom_line(size = 1, alpha = 0.5)",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-7",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-7",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nDefine the following probabilities:\n\n\\(p_{d} = P\\left(\\text{voting democrat}\\right)\\);\n\\(p_{i} = P\\left(\\text{voting independent}\\right)\\);\n\\(p_{r} = P\\left(\\text{voting republican}\\right)\\),\n\nwhere \\(p_d + p_i + p_r = 1\\). Assume for now that income is the only independent variable of interest.",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-8",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-8",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nThe multinomial logit model effectively fits several logits (one for every class except the baseline, which is arbitrary; here, it‚Äôs democrat):\n\n\\(\\log\\left(p_{i} / p_{d}\\right) = \\beta_0 + \\beta_1 \\mathtt{income}\\quad\\) (log odds of voting independent vs.¬†democrat);\n\\(\\log\\left(p_{r} / p_{d}\\right) = \\alpha_0 + \\alpha_1 \\mathtt{income}\\quad\\) (log odds of voting republican vs.¬†democrat).\n\nHere we use \\(\\beta_i\\) and \\(\\alpha_i\\) to remind us that the estimated coefficients between the two logits will be different.",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-9",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-9",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nMultinomial logit model using all three predictors:\n\n\nShow R code\nlibrary(nnet)  # install.packages(\"nnet\")\n\n(mfit &lt;- multinom(party ~ age + education + income, data = rnes96, trace = FALSE))\n\n\nCall:\nmultinom(formula = party ~ age + education + income, data = rnes96, \n    trace = FALSE)\n\nCoefficients:\n            (Intercept)          age education.L education.Q education.C\nIndependent   -1.197260 0.0001534525  0.06351451  -0.1217038   0.1119542\nRepublican    -1.642656 0.0081943691  1.19413345  -1.2292869   0.1544575\n            education^4 education^5 education^6     income\nIndependent -0.07657336   0.1360851  0.15427826 0.01623911\nRepublican  -0.02827297  -0.1221176 -0.03741389 0.01724679\n\nResidual Deviance: 1968.333 \nAIC: 2004.333",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#brief-digression",
    "href": "slides/05-multinomial-ordinal-regression.html#brief-digression",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "Brief digression‚Ä¶",
    "text": "Brief digression‚Ä¶\n\nBy default, R encodes ordered factors using orthogonal polynomials\nAmes Housing example:\n\n\n\nShow R code\names &lt;- AmesHousing::make_ames()\nggplot(ames, aes(x = Overall_Qual, y = Sale_Price)) +#log(Sale_Price))) +\n  geom_boxplot(aes(color = Overall_Qual)) +\n  scale_x_discrete(guide = guide_axis(angle = 45))",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-10",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-10",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nNo \\(p\\)-values here!\n\n\nShow R code\nsummary(mfit)\n\n\nCall:\nmultinom(formula = party ~ age + education + income, data = rnes96, \n    trace = FALSE)\n\nCoefficients:\n            (Intercept)          age education.L education.Q education.C\nIndependent   -1.197260 0.0001534525  0.06351451  -0.1217038   0.1119542\nRepublican    -1.642656 0.0081943691  1.19413345  -1.2292869   0.1544575\n            education^4 education^5 education^6     income\nIndependent -0.07657336   0.1360851  0.15427826 0.01623911\nRepublican  -0.02827297  -0.1221176 -0.03741389 0.01724679\n\nStd. Errors:\n            (Intercept)         age education.L education.Q education.C\nIndependent   0.3265951 0.005374592   0.4571884   0.4142859   0.3498491\nRepublican    0.3312877 0.004902668   0.6502670   0.6041924   0.4866432\n            education^4 education^5 education^6      income\nIndependent   0.2883031   0.2494706   0.2171578 0.003108585\nRepublican    0.3605620   0.2696036   0.2031859 0.002881745\n\nResidual Deviance: 1968.333 \nAIC: 2004.333",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-11",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-11",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\n\nHow do we interpret the coefficients? How about for income?\nAll else held constant, for every ‚ö†Ô∏èone-unit increase in income‚ö†Ô∏è, the multinomial log odds of voting republican, relative to democrat increase by 0.017.\nGross‚Ä¶ ü§¢\nEffect plots to the rescue!",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-12",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-12",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nLook at predicted probabilities:\n\nhead(probs &lt;- predict(mfit, type = \"probs\"))\n\n   Democrat Independent Republican\n1 0.5923052   0.1975326  0.2101622\n2 0.5919378   0.1687055  0.2393567\n3 0.5970789   0.1732058  0.2297154\n4 0.5924809   0.1719775  0.2355417\n5 0.5423563   0.1583973  0.2992464\n6 0.5907590   0.1683954  0.2408456\n\n# Sanity check\nhead(rowSums(probs))\n\n1 2 3 4 5 6 \n1 1 1 1 1 1",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-13",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-13",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\n\n\nShow R code\nlibrary(pdp)  # for partial dependence (PD) plots\n\n# Compute partial dependence of party identification on income\npfun &lt;- function(object, newdata) {\n  probs &lt;- predict(object, newdata = newdata, type = \"probs\")\n  colMeans(probs)  # return average probability for each class\n}\npd.inc &lt;- partial(mfit, pred.var = \"income\", pred.fun = pfun)\nggplot(pd.inc, aes(x = income, y = yhat, linetype = yhat.id, color = yhat.id)) +\n  geom_line(size = 2) +\n  xlab(\"Income group midpoint (in thousands)\") +\n  ylab(\"Partial dependence\")",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-14",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-14",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nCan perform classification, if desired‚Ä¶üôÑ\n\n\nShow R code\ntable(\"Predicted\" = predict(mfit), \"Actual\" = rnes96$party)\n\n\n             Actual\nPredicted     Democrat Independent Republican\n  Democrat         277         130        169\n  Independent        4           7          5\n  Republican        99         102        151\n\n\n\nUmmm‚Ä¶majority of actual Republicans are classified as Democrats?!",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-15",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-15",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nFor kicks, try stepwise selection; since the model is based on a (multinomial) likelihood, the AIC/BIC are well-defined and the usual stepwise procedures are still valid:\n\n\nShow R code\n(mfit2 &lt;- MASS::stepAIC(mfit, direction = \"both\", scope = list(\"upper\" = ~.^2)))\n\n\nStart:  AIC=2004.33\nparty ~ age + education + income\n\n                   Df    AIC\n- education        12 1996.5\n- age               2 2003.6\n&lt;none&gt;                2004.3\n+ age:income        2 2006.7\n+ age:education    12 2009.3\n+ education:income 12 2013.2\n- income            2 2045.9\n\nStep:  AIC=1996.54\nparty ~ age + income\n\n             Df    AIC\n- age         2 1993.4\n&lt;none&gt;          1996.5\n+ age:income  2 1998.8\n+ education  12 2004.3\n- income      2 2048.8\n\nStep:  AIC=1993.42\nparty ~ income\n\n            Df    AIC\n&lt;none&gt;         1993.4\n+ age        2 1996.5\n+ education 12 2003.6\n- income     2 2045.3\n\n\nCall:\nmultinom(formula = party ~ income, data = rnes96, trace = FALSE)\n\nCoefficients:\n            (Intercept)     income\nIndependent  -1.1749331 0.01608683\nRepublican   -0.9503591 0.01766457\n\nResidual Deviance: 1985.424 \nAIC: 1993.424",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-16",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-16",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nFor comparison, fit a (default) random forest:\n\n\nShow R code\nset.seed(2008)  # for reproducibility\n(rfo &lt;- randomForest::randomForest(party ~ ., data = rnes96, ntree = 1000))\n\n\n\nCall:\n randomForest(formula = party ~ ., data = rnes96, ntree = 1000) \n               Type of random forest: classification\n                     Number of trees: 1000\nNo. of variables tried at each split: 1\n\n        OOB estimate of  error rate: 55.72%\nConfusion matrix:\n            Democrat Independent Republican class.error\nDemocrat         248          41         91   0.3473684\nIndependent      129          27         83   0.8870293\nRepublican       156          26        143   0.5600000",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-17",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-17",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nRandom forest results for comparison:\n\n\nShow R code\n# Construct the same PD plot as before, but using the RF model\npd &lt;- partial(rfo, pred.var = \"income\", pred.fun = function(object, newdata) {\n  colMeans(predict(object, newdata = newdata, type = \"prob\"))\n})\nggplot(pd, aes(x = income, y = yhat, linetype = yhat.id, color = yhat.id)) +\n  geom_line(size = 2) +\n  xlab(\"Income (midpoint in thousands)\") +\n  ylab(\"Partial dependence\") +\n  geom_rug(data = data.frame(\"income\" = quantile(rnes96$income, prob = 1:9/10)), aes(x = income), inherit.aes = FALSE)",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#ordinal-outcomes-1",
    "href": "slides/05-multinomial-ordinal-regression.html#ordinal-outcomes-1",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "Ordinal outcomes",
    "text": "Ordinal outcomes\n\nIf \\(Z - \\boldsymbol{x}^\\top\\boldsymbol{\\beta}\\) has distribution \\(F\\), then \\[\n\\mathrm{P}\\left(Y \\le j\\right) = \\mathrm{P}\\left(Z \\le \\alpha_j\\right) = F\\left(\\alpha_j - \\boldsymbol{x}^\\top\\boldsymbol{\\beta}\\right)\n\\]\nIf, for example, \\(F\\) is a standard logistic distribution, then \\[\np_j^\\le = \\frac{\\exp\\left(\\alpha_j - \\boldsymbol{x}^\\top\\boldsymbol{\\beta}\\right)}{1 + \\exp\\left(\\alpha_j - \\boldsymbol{x}^\\top\\boldsymbol{\\beta}\\right)}\n\\] which is a logit model for the cumulative probabilities!\nChoosing a standard normal distribution for \\(F\\) would lead to a probit model for the cumulative probabilities, and so on‚Ä¶",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#proportional-odds-po-model",
    "href": "slides/05-multinomial-ordinal-regression.html#proportional-odds-po-model",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "Proportional odds (PO) model",
    "text": "Proportional odds (PO) model\n\nLet \\(p_j^\\top = \\mathrm{P}\\left(Y \\le j|\\boldsymbol{x}\\right)\\)\nThe standard PO model, which uses a logit link, is \\[\n\\log\\left(\\frac{p_j^\\le}{1-p_j^\\le}\\right) = \\alpha_j - \\boldsymbol{x}^\\top\\boldsymbol{\\beta}, \\quad j = 1, 2, \\dots, J-1\n\\]",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#proportional-odds-po-model-1",
    "href": "slides/05-multinomial-ordinal-regression.html#proportional-odds-po-model-1",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "Proportional odds (PO) model",
    "text": "Proportional odds (PO) model\n\nPO assumption, etc‚Ä¶",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-18",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-18",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nThe simplest implementation is polr() from MASS\n\n\nShow R code\nlibrary(MASS)\n\n(pofit &lt;- polr(party ~ age + education + income, data = rnes96))\n\n\nCall:\npolr(formula = party ~ age + education + income, data = rnes96)\n\nCoefficients:\n         age  education.L  education.Q  education.C  education^4  education^5 \n 0.005774902  0.724086814 -0.781360508  0.040168238 -0.019925492 -0.079412657 \n education^6       income \n-0.061103738  0.012738693 \n\nIntercepts:\n  Democrat|Independent Independent|Republican \n             0.6448794              1.7373541 \n\nResidual Deviance: 1984.211 \nAIC: 2004.211",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-19",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-19",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nSimilar to before, we can use AIC-based stepwise procedures:\n\n\nShow R code\n(pofit2 &lt;- stepAIC(pofit, direction = \"both\"))\n\n\nStart:  AIC=2004.21\nparty ~ age + education + income\n\n            Df    AIC\n- education  6 2002.8\n&lt;none&gt;         2004.2\n- age        1 2004.4\n- income     1 2038.6\n\nStep:  AIC=2002.83\nparty ~ age + income\n\n            Df    AIC\n- age        1 2001.4\n&lt;none&gt;         2002.8\n+ education  6 2004.2\n- income     1 2047.2\n\nStep:  AIC=2001.36\nparty ~ income\n\n            Df    AIC\n&lt;none&gt;         2001.4\n+ age        1 2002.8\n+ education  6 2004.4\n- income     1 2045.3\n\n\nCall:\npolr(formula = party ~ income, data = rnes96)\n\nCoefficients:\n    income \n0.01311984 \n\nIntercepts:\n  Democrat|Independent Independent|Republican \n             0.2091045              1.2915566 \n\nResidual Deviance: 1995.363 \nAIC: 2001.363",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-20",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-20",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\n\ncbind(\n  \"deviance\" = c(\"PO\" = deviance(pofit2), \"Multi\" = deviance(mfit2)),\n  \"nparam\" = c(\"PO\" = pofit2$edf, \"Multi\" = mfit2$edf)\n)\n\n      deviance nparam\nPO    1995.363      3\nMulti 1985.424      4",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-21",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-21",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nCompare full and reduced model using LR test\n\nanova(pofit2, pofit, test = \"Chisq\")",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-22",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-22",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nInterpreting the coefficients:\n\nsummary(pofit2)\n\nCall:\npolr(formula = party ~ income, data = rnes96)\n\nCoefficients:\n         Value Std. Error t value\nincome 0.01312   0.001971   6.657\n\nIntercepts:\n                       Value   Std. Error t value\nDemocrat|Independent    0.2091  0.1123     1.8627\nIndependent|Republican  1.2916  0.1201    10.7526\n\nResidual Deviance: 1995.363 \nAIC: 2001.363 \n\n\nWe can say that the odds of moving from Democrat to Independent/Republican (or from Democrat/Independent to Republican) increase by a factor of \\(\\exp\\left(0.013120\\right) = 1.0132\\) per unit increase in income.",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#non-proportional-odds-npo-model",
    "href": "slides/05-multinomial-ordinal-regression.html#non-proportional-odds-npo-model",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "Non-proportional odds (NPO) model",
    "text": "Non-proportional odds (NPO) model\n\nWe can generalize the PO model by allowing the coefficients to vary between categories (similar to the mulinomial logit model form earlier): \\[\n\\log\\left(\\frac{p_j^\\le}{1-p_j^\\le}\\right) = \\alpha_j - \\boldsymbol{x}^\\top\\boldsymbol{\\beta}_j, \\quad j = 1, 2, \\dots, J-1\n\\]\nThis relaxes the PO assumptions but requires more complicated software (e.g., the VGAM package)\nCareful, different packages use different default parameterizations; E.g., see Table 2 here",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#national-election-study-23",
    "href": "slides/05-multinomial-ordinal-regression.html#national-election-study-23",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "1996 National Election Study",
    "text": "1996 National Election Study\nPO and NPO fits to election data using VGAM:\n\n\nShow R code\nlibrary(VGAM)\n\n(pofit &lt;- vglm(party ~ income, data = rnes96,\n               family = cumulative(parallel = TRUE, reverse = TRUE)))\n\n\n\nCall:\nvglm(formula = party ~ income, family = cumulative(parallel = TRUE, \n    reverse = TRUE), data = rnes96)\n\n\nCoefficients:\n(Intercept):1 (Intercept):2        income \n  -0.20910243   -1.29155469    0.01311978 \n\nDegrees of Freedom: 1888 Total; 1885 Residual\nResidual deviance: 1995.363 \nLog-likelihood: -997.6813 \n\n\nShow R code\n(npofit &lt;- vglm(party ~ income, data = rnes96,\n                family = cumulative(parallel = FALSE, reverse = TRUE)))\n\n\n\nCall:\nvglm(formula = party ~ income, family = cumulative(parallel = FALSE, \n    reverse = TRUE), data = rnes96)\n\n\nCoefficients:\n(Intercept):1 (Intercept):2      income:1      income:2 \n  -0.32886794   -1.14826963    0.01618611    0.01048588 \n\nDegrees of Freedom: 1888 Total; 1884 Residual\nResidual deviance: 1987.539 \nLog-likelihood: -993.7693",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "slides/05-multinomial-ordinal-regression.html#assigning-scores",
    "href": "slides/05-multinomial-ordinal-regression.html#assigning-scores",
    "title": "Regression for Multinomial and Ordinal Outcomes",
    "section": "Assigning scores",
    "text": "Assigning scores\n\nWhen the ordinal response has a larger number of categories, it may be reasonable to assign scores (i.e., integers) to each level and then model these scores using a standard linear model\nRule of thumb from my old advisor was 10 or more categories",
    "crumbs": [
      "Regression for Multinomial and Ordinal Outcomes"
    ]
  },
  {
    "objectID": "code/lecture-5.html",
    "href": "code/lecture-5.html",
    "title": "Install required packages",
    "section": "",
    "text": "# Install required package(s)\npkgs &lt;- c(\"AmesHousing\", \"dplyr\", \"faraway\", \"ggplot2\", \"MASS\", \"nnet\", \"ordinal\", \"rms\", \"VGAM\")\nlib &lt;- installed.packages()[, \"Package\"]\ninstall.packages(setdiff(pkgs, lib))"
  },
  {
    "objectID": "code/lecture-5.html#load-some-useful-packages",
    "href": "code/lecture-5.html#load-some-useful-packages",
    "title": "Install required packages",
    "section": "Load some useful packages",
    "text": "Load some useful packages\n\nlibrary(dplyr)    # for data wrangling\nlibrary(ggplot2)  # for plotting\n\n# Set ggplot2 theme for the notebook\ntheme_set(theme_bw())"
  },
  {
    "objectID": "code/lecture-5.html#data-exploration",
    "href": "code/lecture-5.html#data-exploration",
    "title": "Install required packages",
    "section": "Data exploration",
    "text": "Data exploration\nIn the code chunk below, we‚Äôll load data from the US 1996 national election study and take a peek at the first few rows:\n\n# Load 10 variable subset of US 1996 National Election Study\ndata(nes96, package = \"faraway\")\n\n# Print first few rows\nhead(nes96, n = 10)\n\n\nA data.frame: 10 √ó 10\n\n\n\npopul\nTVnews\nselfLR\nClinLR\nDoleLR\nPID\nage\neduc\nincome\nvote\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;ord&gt;\n&lt;ord&gt;\n&lt;ord&gt;\n&lt;ord&gt;\n&lt;int&gt;\n&lt;ord&gt;\n&lt;ord&gt;\n&lt;fct&gt;\n\n\n\n\n1\n0\n7\nextCon\nextLib\nCon\nstrRep\n36\nHS\n$3Kminus\nDole\n\n\n2\n190\n1\nsliLib\nsliLib\nsliCon\nweakDem\n20\nColl\n$3Kminus\nClinton\n\n\n3\n31\n7\nLib\nLib\nCon\nweakDem\n24\nBAdeg\n$3Kminus\nClinton\n\n\n4\n83\n4\nsliLib\nMod\nsliCon\nweakDem\n28\nBAdeg\n$3Kminus\nClinton\n\n\n5\n640\n7\nsliCon\nCon\nMod\nstrDem\n68\nBAdeg\n$3Kminus\nClinton\n\n\n6\n110\n3\nsliLib\nMod\nCon\nweakDem\n21\nColl\n$3Kminus\nClinton\n\n\n7\n100\n7\nsliCon\nCon\nMod\nweakDem\n77\nColl\n$3Kminus\nClinton\n\n\n8\n31\n1\nsliCon\nMod\nsliCon\nindRep\n21\nColl\n$3Kminus\nClinton\n\n\n9\n180\n7\nMod\nCon\nsliLib\nindind\n31\nColl\n$3Kminus\nClinton\n\n\n10\n2800\n0\nsliLib\nsliLib\nextCon\nstrDem\n39\nHS\n$3Kminus\nClinton\n\n\n\n\n\nLet‚Äôs use the str() function to print a more useful summary of the str()ucture of this data set:\n\nstr(nes96)\n\n'data.frame':   944 obs. of  10 variables:\n $ popul : int  0 190 31 83 640 110 100 31 180 2800 ...\n $ TVnews: int  7 1 7 4 7 3 7 1 7 0 ...\n $ selfLR: Ord.factor w/ 7 levels \"extLib\"&lt;\"Lib\"&lt;..: 7 3 2 3 5 3 5 5 4 3 ...\n $ ClinLR: Ord.factor w/ 7 levels \"extLib\"&lt;\"Lib\"&lt;..: 1 3 2 4 6 4 6 4 6 3 ...\n $ DoleLR: Ord.factor w/ 7 levels \"extLib\"&lt;\"Lib\"&lt;..: 6 5 6 5 4 6 4 5 3 7 ...\n $ PID   : Ord.factor w/ 7 levels \"strDem\"&lt;\"weakDem\"&lt;..: 7 2 2 2 1 2 2 5 4 1 ...\n $ age   : int  36 20 24 28 68 21 77 21 31 39 ...\n $ educ  : Ord.factor w/ 7 levels \"MS\"&lt;\"HSdrop\"&lt;..: 3 4 6 6 6 4 4 4 4 3 ...\n $ income: Ord.factor w/ 24 levels \"$3Kminus\"&lt;\"$3K-$5K\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ vote  : Factor w/ 2 levels \"Clinton\",\"Dole\": 2 1 1 1 1 1 1 1 1 1 ...\n\n\nNote that some of the factors are ‚Äúordered‚Äù by default; for example, take a look at the educ variable (respondent‚Äôs education level):\n\ntable(nes96$educ)\n\n\n    MS HSdrop     HS   Coll  CCdeg  BAdeg  MAdeg \n    13     52    248    187     90    227    127 \n\n\nSame goes for PID (respondent‚Äôs party identification) and income. Does it make sense for these variables to be treated as ordinal?\n\ntable(nes96$PID)\ntable(nes96$income)\n\n\n strDem weakDem  indDem  indind  indRep weakRep  strRep \n    200     180     108      37      94     150     175 \n\n\n\n  $3Kminus    $3K-$5K    $5K-$7K    $7K-$9K   $9K-$10K  $10K-$11K  $11K-$12K \n        19         12         17         19         18         13         11 \n $12K-$13K  $13K-$14K  $14K-$15K  $15K-$17K  $17K-$20K  $20K-$22K  $22K-$25K \n        17         10         15         23         35         26         39 \n $25K-$30K  $30K-$35K  $35K-$40K  $40K-$45K  $45K-$50K  $50K-$60K  $60K-$75K \n        68         70         62         48         51        100        103 \n $75K-$90K $90K-$105K  $105Kplus \n        53         47         68 \n\n\nIn the next code chunk, we‚Äôll clean up the data a bit by:\n\ncondensing PID into a new variable (party) iwht only three categories;\nconvert income to numeric by taking the midpoint of each bin (is this a reasonable thing to do?);\ncreate a new data frame with the variables of interest.\n\n\n# Condense party identification (PID) column into three categories\nparty &lt;- nes96$PID\nlevels(party) &lt;- c(\n  \"Democrat\", \"Democrat\",\n  \"Independent\", \"Independent\", \"Independent\", \n  \"Republican\", \"Republican\"\n)\n\n# Convert income to numeric\ninca &lt;- c(1.5, 4, 6, 8, 9.5, 10.5, 11.5, 12.5, 13.5, 14.5, 16, 18.5, 21, 23.5,\n          27.5, 32.5, 37.5, 42.5, 47.5, 55, 67.5, 82.5, 97.5, 115)\nincome &lt;- inca[unclass(nes96$income)]\n\n# Construct new data set for analysis\nrnes96 &lt;- data.frame(\n  \"party\" = party, \n  \"income\" = income, \n  \"education\" = nes96$educ, \n  \"age\" = nes96$age\n)\n\n# Print summary of data set\nstr(rnes96)\n\n'data.frame':   944 obs. of  4 variables:\n $ party    : Ord.factor w/ 3 levels \"Democrat\"&lt;\"Independent\"&lt;..: 3 1 1 1 1 1 1 2 2 1 ...\n $ income   : num  1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 ...\n $ education: Ord.factor w/ 7 levels \"MS\"&lt;\"HSdrop\"&lt;..: 3 4 6 6 6 4 4 4 4 3 ...\n $ age      : int  36 20 24 28 68 21 77 21 31 39 ...\n\n\nThe next couple of code blocks try to visualize party affiliation by education level and income.\n\n# Aggregate data; what's happening here?\negp &lt;- group_by(rnes96, education, party) %&gt;% \n  summarise(count = n()) %&gt;%\n  group_by(education) %&gt;% \n  mutate(etotal = sum(count), proportion = count/etotal)\n\n# Plot results\nggplot(egp, aes(x = education, y = proportion, group = party, \n                linetype = party, color = party)) + \n  geom_line(size = 2)\n\n\n`summarise()` has grouped output by 'education'. You can override using the\n\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\n\n\n# Aggregate data; what's happening here?\nigp &lt;- mutate(rnes96, incomegp = cut_number(income, 7)) %&gt;% \n  group_by(incomegp, party) %&gt;% \n  summarise(count = n()) %&gt;% \n  group_by(incomegp) %&gt;% \n  mutate(etotal = sum(count), proportion = count / etotal)\n\n# Plot results\nggplot(igp, aes(x = incomegp, y  =proportion, group  =party, \n                linetype = party, color = party)) +\n  geom_line(size = 2)\n\n\n`summarise()` has grouped output by 'incomegp'. You can override using the\n\n`.groups` argument."
  },
  {
    "objectID": "code/lecture-5.html#multinomial-logistic-model",
    "href": "code/lecture-5.html#multinomial-logistic-model",
    "title": "Install required packages",
    "section": "Multinomial logistic model",
    "text": "Multinomial logistic model\nDefine the following probabilities:\n\n\\(p_{d} = P\\left(\\text{voting democrat}\\right)\\);\n\\(p_{i} = P\\left(\\text{voting independent}\\right)\\);\n\\(p_{r} = P\\left(\\text{voting republican}\\right)\\),\n\nwhere \\(p_d + p_i + p_r = 1\\). Assume for now that income is the only independent variable of interest. The multinomial logit model effectively fits several logits (one for every class except the baseline, which is arbitrary; here, it‚Äôs democrat):\n\n\\(\\log\\left(p_{i} / p_{d}\\right) = \\beta_0 + \\beta_1 \\mathtt{income}\\quad\\) (log odds of voting independent vs.¬†democrat);\n\\(\\log\\left(p_{r} / p_{d}\\right) = \\alpha_0 + \\alpha_1 \\mathtt{income}\\quad\\) (log odds of voting republican vs.¬†democrat).\n\nHere we use \\(\\beta_i\\) and \\(\\alpha_i\\) to remind us that the estimated coefficients between the two models will be different.\nThis is somewhat similar to fitting two seperate logit (i.e., logistic regresison) models; akin to a one-vs-one aproach. Hence, seperate coefficient estimates for each. For. \\(J\\) classes, we would effectively have \\(J - 1\\) seperate logit models (i.e., sets of different coefficients).\nBelow, we use the nnet package in R to fit a multinomial logit model to the rnes96 data.\n\nlibrary(nnet)  # for multinom() and polr() functions\n\n# Fit multinomial log-linear model; see ?nnet::multinom for details\n(fit.multi &lt;- multinom(party ~ age + education + income, data = rnes96))\n\n# weights:  30 (18 variable)\ninitial  value 1037.090001 \niter  10 value 990.568608\niter  20 value 984.319052\nfinal  value 984.166272 \nconverged\n\n\nCall:\nmultinom(formula = party ~ age + education + income, data = rnes96)\n\nCoefficients:\n            (Intercept)          age education.L education.Q education.C\nIndependent   -1.197260 0.0001534525  0.06351451  -0.1217038   0.1119542\nRepublican    -1.642656 0.0081943691  1.19413345  -1.2292869   0.1544575\n            education^4 education^5 education^6     income\nIndependent -0.07657336   0.1360851  0.15427826 0.01623911\nRepublican  -0.02827297  -0.1221176 -0.03741389 0.01724679\n\nResidual Deviance: 1968.333 \nAIC: 2004.333 \n\n\n\n# Fit multinomial log-linear model with education as a nominal factor\nrnes96_2 &lt;- rnes96\nrnes96_2$education &lt;- factor(rnes96_2$education, ordered = FALSE)\nmultinom(party ~ age + education + income, data = rnes96_2)\n\n# weights:  30 (18 variable)\ninitial  value 1037.090001 \niter  10 value 990.364722\niter  20 value 984.508641\nfinal  value 984.166272 \nconverged\n\n\nCall:\nmultinom(formula = party ~ age + education + income, data = rnes96_2)\n\nCoefficients:\n            (Intercept)          age educationHSdrop educationHS educationColl\nIndependent   -1.373895 0.0001539014       0.2704482   0.2458744    0.09119446\nRepublican    -3.048576 0.0081945031       0.9876547   1.6915600    1.95336096\n            educationCCdeg educationBAdeg educationMAdeg     income\nIndependent      0.3269554      0.1082654      0.1933497 0.01623914\nRepublican       1.8835335      1.8708213      1.4539589 0.01724696\n\nResidual Deviance: 1968.333 \nAIC: 2004.333 \n\n\nWhat happened to educucation, and why did it not get dummy encoded?"
  },
  {
    "objectID": "code/lecture-5.html#brief-non-math-digression-into-orthogonal-polynomial-encoding",
    "href": "code/lecture-5.html#brief-non-math-digression-into-orthogonal-polynomial-encoding",
    "title": "Install required packages",
    "section": "Brief (non-math) digression into orthogonal polynomial encoding",
    "text": "Brief (non-math) digression into orthogonal polynomial encoding\nOrthogonal polynomial (OP) encoding for factors helps look for the linear, quadratic, and cubic trends in the categorical variable of interest (see plots below). Note that OP encoding should only be used with an ordinal variable in which the levels are equally spaced (e.g., income or education).\n\names &lt;- AmesHousing::make_ames()\nggplot(ames, aes(x = Overall_Qual, y = Sale_Price)) +#log(Sale_Price))) +\n  geom_boxplot(aes(color = Overall_Qual))\n\n\n\n\n\n\n\n\n\nsummary(lm(log(Sale_Price) ~ Overall_Qual, data = ames))\n\n\names.ord &lt;- AmesHousing::make_ordinal_ames()\nsummary(lm(log(Sale_Price) ~ Overall_Qual, data = ames.ord))\n\n\n# What if we ignore the log scale?\nggplot(ames, aes(x = Overall_Qual, y = Sale_Price)) +\n  geom_boxplot(aes(color = Overall_Qual))\nsummary(lm(Sale_Price ~ Overall_Qual, data = ames))\nsummary(lm(Sale_Price ~ Overall_Qual, data = ames.ord))"
  },
  {
    "objectID": "code/lecture-5.html#end-digression",
    "href": "code/lecture-5.html#end-digression",
    "title": "Install required packages",
    "section": "End digression‚Ä¶",
    "text": "End digression‚Ä¶\n\nsummary(fit.multi)  # no p-values here\n\n\n# How do we interpret the coefficients? For example, (all else held constant) for every one-unit increase\n# in income, the multinomial log odds of voting republican, relative to democrat,\n# increase by 0.003.\n#\n# Gross...\n#\n# Effect plots to the rescue!\n\n\nlibrary(pdp)  # for partial dependence (PD) plots\n\n# Compute partial dependence of party identification on income\npfun &lt;- function(object, newdata) {\n  probs &lt;- predict(object, newdata = newdata, type = \"probs\")\n  colMeans(probs)  # return \n}\npd.inc &lt;- partial(fit.multi, pred.var = \"income\", pred.fun = pfun)\nggplot(pd.inc, aes(x = income, y = yhat, linetype = yhat.id, color = yhat.id)) +\n  geom_line(size = 2) +\n  xlab(\"Income (midpoint in thousands)\") +\n  ylab(\"Partial dependence\")\n\n\n\n\n\n\n\n\n\n# Try stepwise; since the model is based on a (multinomial) likelihood, the AIC/BIC \n# are well-defined and the usual stepwise procedures are still valid \nMASS::stepAIC(fit.multi, direction = \"both\", scope = list(\"upper\" = ~.^2))\n\nStart:  AIC=2004.33\nparty ~ age + education + income\n\n# weights:  27 (16 variable)\ninitial  value 1037.090001 \niter  10 value 988.896864\niter  20 value 985.822223\nfinal  value 985.812737 \nconverged\n# weights:  12 (6 variable)\ninitial  value 1037.090001 \niter  10 value 992.269502\nfinal  value 992.269484 \nconverged\n# weights:  27 (16 variable)\ninitial  value 1037.090001 \niter  10 value 1009.025560\niter  20 value 1006.961593\nfinal  value 1006.955275 \nconverged\n# weights:  48 (30 variable)\ninitial  value 1037.090001 \niter  10 value 1004.992102\niter  20 value 981.097880\niter  30 value 974.672766\niter  40 value 974.632373\niter  40 value 974.632372\niter  40 value 974.632372\nfinal  value 974.632372 \nconverged\n# weights:  33 (20 variable)\ninitial  value 1037.090001 \niter  10 value 989.133986\niter  20 value 983.452423\nfinal  value 983.369209 \nconverged\n# weights:  48 (30 variable)\ninitial  value 1037.090001 \niter  10 value 1002.858283\niter  20 value 980.366035\niter  30 value 976.643913\nfinal  value 976.619912 \nconverged\n                   Df    AIC\n- education        12 1996.5\n- age               2 2003.6\n&lt;none&gt;                2004.3\n+ age:income        2 2006.7\n+ age:education    12 2009.3\n+ education:income 12 2013.2\n- income            2 2045.9\n# weights:  12 (6 variable)\ninitial  value 1037.090001 \niter  10 value 992.269502\nfinal  value 992.269484 \nconverged\n\nStep:  AIC=1996.54\nparty ~ age + income\n\n# weights:  9 (4 variable)\ninitial  value 1037.090001 \nfinal  value 992.712152 \nconverged\n# weights:  9 (4 variable)\ninitial  value 1037.090001 \nfinal  value 1020.425203 \nconverged\n# weights:  30 (18 variable)\ninitial  value 1037.090001 \niter  10 value 990.568608\niter  20 value 984.319052\nfinal  value 984.166272 \nconverged\n# weights:  15 (8 variable)\ninitial  value 1037.090001 \niter  10 value 992.613103\nfinal  value 991.418362 \nconverged\n             Df    AIC\n- age         2 1993.4\n&lt;none&gt;          1996.5\n+ age:income  2 1998.8\n+ education  12 2004.3\n- income      2 2048.8\n# weights:  9 (4 variable)\ninitial  value 1037.090001 \nfinal  value 992.712152 \nconverged\n\nStep:  AIC=1993.42\nparty ~ income\n\n# weights:  6 (2 variable)\ninitial  value 1037.090001 \nfinal  value 1020.636052 \nconverged\n# weights:  12 (6 variable)\ninitial  value 1037.090001 \niter  10 value 992.269502\nfinal  value 992.269484 \nconverged\n# weights:  27 (16 variable)\ninitial  value 1037.090001 \niter  10 value 988.896864\niter  20 value 985.822223\nfinal  value 985.812737 \nconverged\n            Df    AIC\n&lt;none&gt;         1993.4\n+ age        2 1996.5\n+ education 12 2003.6\n- income     2 2045.3\n\n\nCall:\nmultinom(formula = party ~ income, data = rnes96)\n\nCoefficients:\n            (Intercept)     income\nIndependent  -1.1749331 0.01608683\nRepublican   -0.9503591 0.01766457\n\nResidual Deviance: 1985.424 \nAIC: 1993.424 \n\n\n\n# Look at predicted probabilities\nhead(predict(fit.multi, type = \"probs\"))\n\n\nA matrix: 6 √ó 3 of type dbl\n\n\n\nDemocrat\nIndependent\nRepublican\n\n\n\n\n1\n0.5923052\n0.1975326\n0.2101622\n\n\n2\n0.5919378\n0.1687055\n0.2393567\n\n\n3\n0.5970789\n0.1732058\n0.2297154\n\n\n4\n0.5924809\n0.1719775\n0.2355417\n\n\n5\n0.5423563\n0.1583973\n0.2992464\n\n\n6\n0.5907590\n0.1683954\n0.2408456\n\n\n\n\n\n\nproportions(table(rnes96$party))\n\n\n# Construct partial dependence of party affiliation on education\npd.edu &lt;- partial(fit.multi, pred.var = \"education\", pred.fun = pfun)\nhead(pd.edu)\nggplot(pd.edu, aes(x = education, y = yhat, linetype = yhat.id, color = yhat.id)) +\n  geom_point(size = 3, alpha = 0.5) +\n  xlab(\"Education\") +\n  ylab(\"Partial dependence\") \n\n\nA ice: 6 √ó 3\n\n\n\neducation\nyhat\nyhat.id\n\n\n\n&lt;ord&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\nMS\n0.5847048\nDemocrat\n\n\n2\nHSdrop\n0.4725179\nDemocrat\n\n\n3\nHS\n0.4009031\nDemocrat\n\n\n4\nColl\n0.3796768\nDemocrat\n\n\n5\nCCdeg\n0.3688061\nDemocrat\n\n\n6\nBAdeg\n0.3900266\nDemocrat\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Can perform classification, if desired (insert eyeroll emoji...)\ntable(\"Predicted\" = predict(fit.multi), \"Actual\" = rnes96$party)\n# Ummm...majority of actual Republicans are classified as Democrats\n\n             Actual\nPredicted     Democrat Independent Republican\n  Democrat         277         130        169\n  Independent        4           7          5\n  Republican        99         102        151\n\n\n\n# For comparison, fit a (default) random forest (RF); see \n# ?randomForest::randomForest for additional details\nset.seed(2008)  # for reproducibility\n(fit.rfo &lt;- randomForest::randomForest(party ~ ., data = rnes96, ntree = 1000))\n\n\n# Construct the same PD plot as before, but using the RF model\npd.rfo &lt;- partial(fit.rfo, pred.var = \"income\", pred.fun = function(object, newdata) {\n  colMeans(predict(object, newdata = newdata, type = \"prob\"))\n})\nggplot(pd.rfo, aes(x = income, y = yhat, linetype = yhat.id, color = yhat.id)) +\n  geom_line(size = 2) +\n  xlab(\"Income (midpoint in thousands)\") +\n  ylab(\"Partial dependence\") +\n  geom_rug(data = data.frame(\"income\" = quantile(rnes96$income, prob = 1:9/10)),\n           aes(x = income), inherit.aes = FALSE)"
  },
  {
    "objectID": "code/lecture-5.html#proportional-odds-cummulative-logit-model",
    "href": "code/lecture-5.html#proportional-odds-cummulative-logit-model",
    "title": "Install required packages",
    "section": "Proportional odds cummulative logit model",
    "text": "Proportional odds cummulative logit model\nFor an ordered outcome with \\(J\\) categories \\(1 &lt; 2 &lt; 3 &lt; \\cdots &lt; J\\), the proportional odds cummulative logit (PO, for short) models fits a logit-type model using the cumulative probabilities. For brevity, let\n\n\\(p_{\\le j} = P\\left(Y \\le j | \\boldsymbol{x}\\right)\\);\n\\(p_{&gt; j} = P\\left(Y &gt; j | \\boldsymbol{x}\\right)\\).\n\nThe PO model uses the following logit:\n\\(\\log\\left(\\frac{p_{\\le j}}{p_{&gt;j}}\\right) = \\alpha_j + \\boldsymbol{x}^\\top\\boldsymbol{\\beta}\\).\nIt‚Äôs worth noting that for the non-proportional odds cummulative logit (NPO, for short) model:\n\\(\\log\\left(\\frac{p_{\\le j}}{p_{&gt;j}}\\right) = \\alpha_j + \\boldsymbol{x}^\\top\\boldsymbol{\\beta}_j\\).\nThat is, in the NPO model, the \\(\\beta\\) coefficients depend on \\(j\\). In short, the PO model has a simpler form since only the inetrcepts are allowed to vary (hence, proportional odds assumption).\n\n###############################################################################\n#\n# Modeling ordinal data using the proportional odds (PO) model\n#\n# Reference: https://journal.r-project.org/archive/2018/RJ-2018-004/RJ-2018-004.pdf\n#\n###############################################################################\n\n\nlibrary(MASS)  # for polr() function; already loaded but listed here again\n\n# The following packages offer similar functionality\n#library(rms)\n#library(ordinal)\n#library(VGAM)  # used in Dr. Liu's slides and videos\n\n# This is the log odds of category k or less, and since these are log odds \n# which differ only by a constant for different k, the odds are proportional. \n# Hence the term proportional odds logistic regression.\n\n# Fit a proportional odds logistic regression\n(fit.polr &lt;- polr(party ~ age + education + income, data = rnes96))\n\n\nAttaching package: ‚ÄòMASS‚Äô\n\n\nThe following object is masked from ‚Äòpackage:dplyr‚Äô:\n\n    select\n\n\n\n\nCall:\npolr(formula = party ~ age + education + income, data = rnes96)\n\nCoefficients:\n         age  education.L  education.Q  education.C  education^4  education^5 \n 0.005774902  0.724086814 -0.781360508  0.040168238 -0.019925492 -0.079412657 \n education^6       income \n-0.061103738  0.012738693 \n\nIntercepts:\n  Democrat|Independent Independent|Republican \n             0.6448794              1.7373541 \n\nResidual Deviance: 1984.211 \nAIC: 2004.211 \n\n\n\n# Predicted (non-cumulative) probabilities\nhead(predict(fit.polr, newdata = rnes96, type = \"probs\"))\n\n\nA matrix: 6 √ó 3 of type dbl\n\n\n\nDemocrat\nIndependent\nRepublican\n\n\n\n\n1\n0.5673361\n0.2289862\n0.2036777\n\n\n2\n0.5346171\n0.2394053\n0.2259776\n\n\n3\n0.5428780\n0.2369062\n0.2202158\n\n\n4\n0.5371401\n0.2386517\n0.2242082\n\n\n5\n0.4794712\n0.2536103\n0.2669185\n\n\n6\n0.5331800\n0.2398307\n0.2269893"
  },
  {
    "objectID": "code/lecture-5.html#interpreting-the-coefficients",
    "href": "code/lecture-5.html#interpreting-the-coefficients",
    "title": "Install required packages",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\n\nTake a look at Table 2 in this paper. Once you know which parameterization your software uses for the PO/NPO model, then you should be able to interpret the coefficient estimates in the right direction.\n\n# One interprets the effects in the PO model using ordinary odds ratios. The \n# difference is that a single odds ratio is assumed to apply equally to *all*\n# events Y&lt;=j (or Y&gt;=j, depending on software), j=1,2,...,k. If linearity and additivity hold, exp(beta_m) is \n# the odds of Y&lt;=j (regardless of j) for every one-unit increase in X_m.\n\n\n# Just look at effect plots...\npfun &lt;- function(object, newdata) {\n  probs &lt;- predict(object, newdata = newdata, type = \"probs\")  # individual probs\n  cprobs &lt;- t(apply(probs[, 1:2], MARGIN = 1, FUN = cumsum))  # cumulative probs\n  colMeans(cprobs)  # return averaged predicted probs for each class\n}\npd.inc &lt;- partial(fit.polr, pred.var = \"income\", pred.fun = pfun)\nggplot(pd.inc, aes(x = income, y = yhat, linetype = yhat.id, color = yhat.id)) +\n  geom_line(size = 2) +\n  xlab(\"Income (midpoint in thousands)\") +\n  ylab(\"Partial dependence\")\n\n\n\n\n\n\n\n\n\n# Try stepwise\nstepAIC(fit.polr, direction = \"both\")\n\nStart:  AIC=2004.21\nparty ~ age + education + income\n\n            Df    AIC\n- education  6 2002.8\n&lt;none&gt;         2004.2\n- age        1 2004.4\n- income     1 2038.6\n\nStep:  AIC=2002.83\nparty ~ age + income\n\n            Df    AIC\n- age        1 2001.4\n&lt;none&gt;         2002.8\n+ education  6 2004.2\n- income     1 2047.2\n\nStep:  AIC=2001.36\nparty ~ income\n\n            Df    AIC\n&lt;none&gt;         2001.4\n+ age        1 2002.8\n+ education  6 2004.4\n- income     1 2045.3\n\n\nCall:\npolr(formula = party ~ income, data = rnes96)\n\nCoefficients:\n    income \n0.01311984 \n\nIntercepts:\n  Democrat|Independent Independent|Republican \n             0.2091045              1.2915566 \n\nResidual Deviance: 1995.363 \nAIC: 2001.363 \n\n\n\nlibrary(VGAM)  # for vglm() function\n\n# Fit different models; some of these are the same, but use a different \n# parameterization...\n(fit1 &lt;- vglm(party ~ age + education + income,  # uses P(Y &lt;= j),\n              family = propodds(reverse = FALSE), data = rnes96))  \n(fit2 &lt;- vglm(formula = party ~ age + education + income,  # same as above, but uses P(Y &gt;= j)\n              family = propodds(reverse = TRUE), data = rnes96))\n(fit3 &lt;- vglm(formula = party ~ age + education + income, \n              family = cumulative(parallel = TRUE), data = rnes96))\n(fit4 &lt;- vglm(formula = party ~ age + education + income,  # same as above (and with polr())\n              family = cumulative(parallel = FALSE), data = rnes96))\n\n\n# A basic assumption of all commonly used ordinal regression models is that \n# the response variable behaves in an ordinal fashion. \n\n\n# ?rms::plot.xmean.ordinaly\n#\n# Separately for each predictor variable X in a formula, plots the mean of X \n# vs. levels of Y. Then under the proportional odds assumption, the expected \n# value of the predictor for each Y value is also plotted (as a dotted line). \n# This plot is useful for assessing the ordinality assumption for Y separately \n# for each X, and for assessing the proportional odds assumption in a simple \n# univariable way. If several predictors do not distinguish adjacent categories \n# of Y, those levels may need to be pooled. This display assumes that each \n# predictor is linearly related to the log odds of each event in the \n# proportional odds model. There is also an option to plot the expected means \n# assuming a forward continuation ratio model.\nlibrary(rms)\n\n# plot mean of each x vs. ordinal y (assumes each x is linearly related to y)\nrms::plot.xmean.ordinaly(party ~ age + education + income, data = rnes96)\n\nLoading required package: Hmisc\n\n\nAttaching package: ‚ÄòHmisc‚Äô\n\n\nThe following objects are masked from ‚Äòpackage:dplyr‚Äô:\n\n    src, summarize\n\n\nThe following objects are masked from ‚Äòpackage:base‚Äô:\n\n    format.pval, units\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Fit model the PO model using rms package\n#options()$contrasts\n(fit.orm &lt;- orm(party ~ age + education + income, data = rnes96))\n\nERROR: Error in Design(X, formula = formula): Variable education is an ordered factor with non-numeric levels.\n You should set options(contrasts=c(\"contr.treatment\", \"contr.treatment\"))\nor rms will not work properly.\n\nError in Design(X, formula = formula): Variable education is an ordered factor with non-numeric levels.\n You should set options(contrasts=c(\"contr.treatment\", \"contr.treatment\"))\nor rms will not work properly.\nTraceback:\n\n1. orm(party ~ age + education + income, data = rnes96)\n2. Design(X, formula = formula)\n3. stop(paste(\"Variable\", nam, \"is an ordered factor with non-numeric levels.\\n\", \n .     \"You should set options(contrasts=c(\\\"contr.treatment\\\", \\\"contr.treatment\\\"))\\nor rms will not work properly.\"))\n\n\n\n# WTF?\n#\n# Fine...\noptions(contrasts = c(\"contr.treatment\", \"contr.treatment\"))  # I guess rms does not like ordered factors...\n\n# Refit the model and treat all categorical variables as nominal\n(fit.orm &lt;- orm(party ~ age + education + income, data = rnes96))\n\nLogistic (Proportional Odds) Ordinal Regression Model\n\norm(formula = party ~ age + education + income, data = rnes96)\n\n                        Model Likelihood               Discrimination    Rank Discrim.    \n                              Ratio Test                      Indexes          Indexes    \nObs            944    LR chi2      57.06    R2                  0.066    rho     0.245    \n Democrat      380    d.f.             8    R2(8,944)           0.051                     \n Independent   239    Pr(&gt; chi2) &lt;0.0001    R2(8,828.6)         0.057                     \n Republican    325    Score chi2   56.58    |Pr(Y&gt;=median)-0.5| 0.116                     \nDistinct Y       3    Pr(&gt; chi2) &lt;0.0001                                                  \nMedian Y         2                                                                        \nmax |deriv| 0.0002                                                                        \n\n                 Coef    S.E.   Wald Z Pr(&gt;|Z|)\ny&gt;=Independent   -1.4963 0.6503 -2.30  0.0214  \ny&gt;=Republican    -2.5887 0.6537 -3.96  &lt;0.0001 \nage               0.0058 0.0039  1.49  0.1372  \neducation=HSdrop  0.5828 0.6464  0.90  0.3673  \neducation=HS      0.9983 0.6079  1.64  0.1005  \neducation=Coll    1.2230 0.6138  1.99  0.0463  \neducation=CCdeg   1.1525 0.6312  1.83  0.0679  \neducation=BAdeg   1.1666 0.6155  1.90  0.0580  \neducation=MAdeg   0.8365 0.6255  1.34  0.1811  \nincome            0.0127 0.0021  5.95  &lt;0.0001 \n\n\n\n# Exponentiating gives the multiplicative increase to P(Y&gt;j|x), for j=Dem&lt;Mod&lt;Rep\nexp(coef(fit.orm))\n\nFor example, for every ‚Äúmidpoint increase‚Äù in income, the odds of voting republican or independent over democrat increase by 1.3%.\n\n# More options when requesting predictions from an ordinal regression model\nhead(p1 &lt;- predict(fit.orm, type = \"fitted\"))  # cumulative probabilities\nhead(p2 &lt;- predict(fit.orm, type = \"fitted.ind\"))  # individual probabilities\n\n\nA matrix: 6 √ó 2 of type dbl\n\n\n\ny&gt;=Independent\ny&gt;=Republican\n\n\n\n\n1\n0.4326639\n0.2036775\n\n\n2\n0.4653843\n0.2259784\n\n\n3\n0.4571215\n0.2202153\n\n\n4\n0.4628592\n0.2242075\n\n\n5\n0.5205267\n0.2669167\n\n\n6\n0.4668214\n0.2269900\n\n\n\n\n\n\nA matrix: 6 √ó 3 of type dbl\n\n\n\nparty=Democrat\nparty=Independent\nparty=Republican\n\n\n\n\n1\n0.5673361\n0.2289864\n0.2036775\n\n\n2\n0.5346157\n0.2394059\n0.2259784\n\n\n3\n0.5428785\n0.2369062\n0.2202153\n\n\n4\n0.5371408\n0.2386517\n0.2242075\n\n\n5\n0.4794733\n0.2536101\n0.2669167\n\n\n6\n0.5331786\n0.2398313\n0.2269900\n\n\n\n\n\n\n# Let's make a simple feature effect (e.g., a partial dependence) plot\npfun.orm &lt;- function(object, newdata) {\n  colMeans(predict(object, newdata = newdata, type = \"fitted\"))\n}\npd.income &lt;- partial(fit.orm, pred.var = \"income\", pred.fun = pfun.orm)\nggplot(pd.income, aes(x = income, y = yhat, linetype = yhat.id, color = yhat.id)) +\n  geom_line(size = 2) +\n  xlab(\"Income (midpoint in thousands)\") +\n  ylab(\"Partial dependence\") +\n  geom_rug(data = data.frame(\"income\" = quantile(rnes96$income, prob = 1:9/10)),\n           aes(x = income), inherit.aes = FALSE)"
  },
  {
    "objectID": "code/lecture-1.html",
    "href": "code/lecture-1.html",
    "title": "BANA 7042",
    "section": "",
    "text": "faa &lt;- read.csv(\"../homework/homework-1/FAA1.csv\", header = TRUE, stringsAsFactors = TRUE)  # read in data\n\n\nhead(faa, n = 5)  # print fisrt five rows\n\n\nA data.frame: 5 √ó 8\n\n\n\naircraft\nduration\nno_pasg\nspeed_ground\nspeed_air\nheight\npitch\ndistance\n\n\n\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\nboeing\n98.47909\n53\n107.91568\n109.3284\n27.41892\n4.043515\n3369.836\n\n\n2\nboeing\n125.73330\n69\n101.65559\n102.8514\n27.80472\n4.117432\n2987.804\n\n\n3\nboeing\n112.01700\n61\n71.05196\nNA\n18.58939\n4.434043\n1144.922\n\n\n4\nboeing\n196.82569\n56\n85.81333\nNA\n30.74460\n3.884236\n1664.218\n\n\n5\nboeing\n90.09538\n70\n59.88853\nNA\n32.39769\n4.026096\n1050.264\n\n\n\n\n\n\ntable(sapply(faa, class))\n\n\n factor integer numeric \n      1       1       6 \n\n\n\nstr(faa)  # display (str)ucture of an arbitrary object\n\n'data.frame':   800 obs. of  8 variables:\n $ aircraft    : Factor w/ 2 levels \"airbus\",\"boeing\": 2 2 2 2 2 2 2 2 2 2 ...\n $ duration    : num  98.5 125.7 112 196.8 90.1 ...\n $ no_pasg     : int  53 69 61 56 70 55 54 57 61 56 ...\n $ speed_ground: num  107.9 101.7 71.1 85.8 59.9 ...\n $ speed_air   : num  109 103 NA NA NA ...\n $ height      : num  27.4 27.8 18.6 30.7 32.4 ...\n $ pitch       : num  4.04 4.12 4.43 3.88 4.03 ...\n $ distance    : num  3370 2988 1145 1664 1050 ...\n\n\n\ndim(faa)\n\n\n8008\n\n\n\nsummary(faa)  # print summary of data frame\n\n   aircraft      duration         no_pasg       speed_ground   \n airbus:400   Min.   : 14.76   Min.   :29.00   Min.   : 27.74  \n boeing:400   1st Qu.:119.49   1st Qu.:55.00   1st Qu.: 65.87  \n              Median :153.95   Median :60.00   Median : 79.64  \n              Mean   :154.01   Mean   :60.13   Mean   : 79.54  \n              3rd Qu.:188.91   3rd Qu.:65.00   3rd Qu.: 92.33  \n              Max.   :305.62   Max.   :87.00   Max.   :141.22  \n                                                               \n   speed_air          height           pitch          distance      \n Min.   : 90.00   Min.   :-3.546   Min.   :2.284   Min.   :  34.08  \n 1st Qu.: 96.16   1st Qu.:23.338   1st Qu.:3.658   1st Qu.: 900.95  \n Median :100.99   Median :30.147   Median :4.020   Median :1267.44  \n Mean   :103.83   Mean   :30.122   Mean   :4.018   Mean   :1544.52  \n 3rd Qu.:109.48   3rd Qu.:36.981   3rd Qu.:4.388   3rd Qu.:1960.44  \n Max.   :141.72   Max.   :59.946   Max.   :5.927   Max.   :6533.05  \n NA's   :600                                                        \n\n\n\n(edf &lt;- ecdf(faa[, \"distance\"]))  \n\nstructure(function (v) \n.approxfun(x, y, v, method, yleft, yright, f, na.rm), class = c(\"ecdf\", \n\"stepfun\", \"function\"), call = ecdf(faa[, \"distance\"]))\n\n\n\n(x &lt;- 3)\n\n3\n\n\n\nsummary(edf)\n\nEmpirical CDF:    800 unique values with summary\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  34.08  900.95 1267.44 1544.52 1960.44 6533.05 \n\n\n\nplot(edf, col = \"forestgreen\", xlab = \"Distance (ft)\", main = \"\", las = 1)\n\n\n\n\n\n\n\n\n\nedf(2000)  # estimate of Pr(landing distance &lt;= 2000 ft)\n\n0.76375\n\n\n\nhist(faa$distance, border = \"white\")\n\n\n\n\n\n\n\n\n\nshapiro.test(faa$distance)\n\n\n    Shapiro-Wilk normality test\n\ndata:  faa$distance\nW = 0.87205, p-value &lt; 2.2e-16\n\n\n\nx1 &lt;- rt(5000, df = 50)\nx2 &lt;- rt(10, df = 50)\nshapiro.test(x1)\nshapiro.test(x2)\n\n\n    Shapiro-Wilk normality test\n\ndata:  x1\nW = 0.99918, p-value = 0.0184\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  x2\nW = 0.95002, p-value = 0.6687\n\n\n\nplot(function(x) dt(x, df = 50), xlim = c(-5, 5))  # t-distribution with 50 d.f.\ncurve(dnorm, lty = 2, col = 2, add = TRUE)         # standard normal distribution\n\n\n\n\n\n\n\n\n\nqqnorm(faa$distance)  \nqqline(faa$distance, col = 2, lty = 2)\n\n\n\n\n\n\n\n\n\npar(mfrow = c(1, 2))\nhist(log(faa$distance), border = \"white\")\nqqnorm(log(faa$distance))  \nqqline(log(faa$distance), col = 2, lty = 2)\n\n\n\n\n\n\n\n\n\n(model &lt;- lm(distance ~ speed_ground, data = faa))\n\n\nCall:\nlm(formula = distance ~ speed_ground, data = faa)\n\nCoefficients:\n (Intercept)  speed_ground  \n    -1804.87         42.11  \n\n\n\nsummary(model)\n\n\nCall:\nlm(formula = distance ~ speed_ground, data = faa)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-957.96 -323.26  -82.72  207.20 2391.37 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -1804.8738    71.3171  -25.31   &lt;2e-16 ***\nspeed_ground    42.1088     0.8715   48.32   &lt;2e-16 ***\n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\nResidual standard error: 473.8 on 798 degrees of freedom\nMultiple R-squared:  0.7453,    Adjusted R-squared:  0.7449 \nF-statistic:  2335 on 1 and 798 DF,  p-value: &lt; 2.2e-16\n\n\n\npredict(model, newdata = faa[1:3, ])\n\n12739.3271636005422475.7221598284631187.03979321476\n\n\n\nplot(log(distance) ~ speed_ground, data = faa, col = \"blue\")\n#abline(lm(log(distance) ~ speed_ground, data = faa), lwd = 2)\n\n\n\n\n\n\n\n\n\nplot(fitted(model), residuals(model)); abline(h = 0, lty = 2, col = \"purple2\")\n\n\n\n\n\n\n\n\n\nsigma(model)\n\n\nhead(predict(model, newdata = faa[1:3, ]))\n\n\nhead(fitted(model))\n\n\nqqnorm(residuals(model))\n\n\n\n\n\n\n\n\n\nall.equal(residuals(model), faa$distance - predict(model))\n\n\nlogd &lt;- log(faa$distance)  # log transform\nhist(logd, breaks = 20)\n\n\n(t.obs &lt;- (mean(logd) - 6) / (sd(logd) / sqrt(length(logd) - 1)))  # (xbar - mu) / (s / sqrt(n))\n\n\n (p.val &lt;- 2 * pt(t.obs, df = length(logd) - 1, lower.tail = FALSE))  # 2 * Pr(T &gt; |t.obs|)\n\n\nmodel2 &lt;- lm(distance ~ speed_ground + height, data = faa)\nsummary(model2)\n\n\nCall:\nlm(formula = distance ~ speed_ground + height, data = faa)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-770.11 -338.13  -61.38  172.99 2477.18 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -2221.6518    83.0683 -26.745   &lt;2e-16 ***\nspeed_ground    42.1691     0.8328  50.636   &lt;2e-16 ***\nheight          13.6771     1.5588   8.774   &lt;2e-16 ***\n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\nResidual standard error: 452.8 on 797 degrees of freedom\nMultiple R-squared:  0.7677,    Adjusted R-squared:  0.7671 \nF-statistic:  1317 on 2 and 797 DF,  p-value: &lt; 2.2e-16\n\n\n\nvar(faa$distance)\n\n\nvar(predict(model2))\n\n\nvar(residuals(model2))\n\n\nvar(predict(model2)) + var(residuals(model2))\n\n\nvar(predict(model2)) / var(faa$distance)  # what is this ratio?\n\n\nsummary(model3 &lt;- lm(distance ~ speed_ground + height + duration, data = faa))\n\n\nCall:\nlm(formula = distance ~ speed_ground + height + duration, data = faa)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-750.11 -337.94  -59.48  178.31 2483.13 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -2193.9527    99.4214 -22.067   &lt;2e-16 ***\nspeed_ground    42.1434     0.8347  50.488   &lt;2e-16 ***\nheight          13.6713     1.5596   8.766   &lt;2e-16 ***\nduration        -0.1654     0.3259  -0.508    0.612    \n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\nResidual standard error: 453 on 796 degrees of freedom\nMultiple R-squared:  0.7678,    Adjusted R-squared:  0.7669 \nF-statistic: 877.2 on 3 and 796 DF,  p-value: &lt; 2.2e-16\n\n\n\nmodels &lt;- list(model, model2, model3)\nr2 &lt;- sapply(models, FUN = function(lin.mod) summary(lin.mod)$r.squared)\nplot(r2, type = \"b\", pch = 19)\n\n\n\n\n\n\n\n\n\nmodels &lt;- list(model, model2, model3)\nadj.r2 &lt;- sapply(models, FUN = function(lin.mod) summary(lin.mod)$adj.r.squared)\nplot(adj.r2, type = \"b\", pch = 19)\n\n\n\n\n\n\n\n\n\n(BICs &lt;- sapply(models, FUN = BIC))  # smaller is better (could use AIC as well)\n\n\nwhich.min(BICs)"
  },
  {
    "objectID": "code/lecture-4.html",
    "href": "code/lecture-4.html",
    "title": "BANA 7042",
    "section": "",
    "text": "# Install required package(s)\npkgs &lt;- c(\"faraway\", \"investr\")\nlib &lt;- installed.packages()[, \"Package\"]\ninstall.packages(setdiff(pkgs, lib))\n\n\n# Load Wisconsin breast cancer data set from faraway package\ndata(wcgs, package = \"faraway\")\n\nhead(wcgs)  # print first few records\n\n\nA data.frame: 6 √ó 13\n\n\n\nage\nheight\nweight\nsdp\ndbp\nchol\nbehave\ncigs\ndibep\nchd\ntypechd\ntimechd\narcus\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;fct&gt;\n\n\n\n\n1\n49\n73\n150\n110\n76\n225\nA2\n25\nA\nno\nnone\n1664\nabsent\n\n\n2\n42\n70\n160\n154\n84\n177\nA2\n20\nA\nno\nnone\n3071\npresent\n\n\n3\n42\n69\n160\n110\n78\n181\nB3\n0\nB\nno\nnone\n3071\nabsent\n\n\n4\n41\n68\n152\n124\n78\n132\nB4\n20\nB\nno\nnone\n3064\nabsent\n\n\n5\n59\n70\n150\n144\n86\n255\nB3\n20\nB\nyes\ninfdeath\n1885\npresent\n\n\n6\n44\n72\n204\n150\n90\n182\nB4\n0\nB\nno\nnone\n3102\nabsent\n\n\n\n\n\n\n# Binary regression with logit link\nwcgs.logit &lt;- glm(chd ~ . - typechd - timechd - behave, data = wcgs,\n                 family = quasibinomial)\n\n\nsummary(wcgs.logit)\n\n\nCall:\nglm(formula = chd ~ . - typechd - timechd - behave, family = quasibinomial, \n    data = wcgs)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.3568  -0.4350  -0.3115  -0.2204   2.8482  \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -12.246031   2.262122  -5.414 6.65e-08 ***\nage            0.061569   0.011982   5.138 2.94e-07 ***\nheight         0.007068   0.032204   0.219  0.82628    \nweight         0.008578   0.003757   2.283  0.02250 *  \nsdp            0.018312   0.006194   2.957  0.00313 ** \ndbp           -0.001175   0.010523  -0.112  0.91107    \nchol           0.010708   0.001479   7.239 5.66e-13 ***\ncigs           0.020966   0.004149   5.054 4.58e-07 ***\ndibepB        -0.658194   0.141074  -4.666 3.21e-06 ***\narcuspresent   0.210539   0.139057   1.514  0.13011    \n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n(Dispersion parameter for quasibinomial family taken to be 0.9342892)\n\n    Null deviance: 1769.2  on 3139  degrees of freedom\nResidual deviance: 1569.5  on 3130  degrees of freedom\n  (14 observations deleted due to missingness)\nAIC: NA\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n# Binary regression with Gaussian CDF/probit link\nwcgs.probit &lt;- glm(chd ~ . - typechd - timechd - behave, data = wcgs,\n                   family = binomial(link = \"probit\"))\n\n\n# Binary regression with cloglog link\nwcgs.cloglog &lt;- glm(chd ~ . - typechd - timechd - behave, data = wcgs,\n                    family = binomial(link = \"cloglog\"))\n\n\n# Binary regression with cauchit link\nwcgs.cauchit &lt;- glm(chd ~ . - typechd - timechd - behave, data = wcgs,\n                    family = binomial(link = \"cauchit\"))\n\n\nsummary(wcgs.cauchit)\n\n\nCall:\nglm(formula = chd ~ . - typechd - timechd - behave, family = binomial(link = \"cauchit\"), \n    data = wcgs)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6540  -0.4117  -0.3540  -0.3136   2.5124  \n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -24.086066   5.381867  -4.475 7.63e-06 ***\nage            0.115178   0.028247   4.077 4.55e-05 ***\nheight         0.053239   0.075906   0.701 0.483062    \nweight         0.002803   0.008466   0.331 0.740552    \nsdp            0.041702   0.011636   3.584 0.000339 ***\ndbp           -0.001380   0.021727  -0.064 0.949342    \nchol           0.019798   0.003201   6.185 6.21e-10 ***\ncigs           0.042856   0.008711   4.920 8.66e-07 ***\ndibepB        -1.319965   0.430059  -3.069 0.002146 ** \narcuspresent   0.711231   0.310369   2.292 0.021931 *  \n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1769.2  on 3139  degrees of freedom\nResidual deviance: 1615.5  on 3130  degrees of freedom\n  (14 observations deleted due to missingness)\nAIC: 1635.5\n\nNumber of Fisher Scoring iterations: 10\n\n\n\n# Compare coefficients\ncoefs &lt;- cbind(\n  \"logit\" = coef(wcgs.logit),\n  \"probit\" = coef(wcgs.probit),\n  \"cloglog\" = coef(wcgs.cloglog),\n  \"cauchit\" = coef(wcgs.cauchit)\n)\nround(coefs, digits = 3)\n\n\nA matrix: 10 √ó 4 of type dbl\n\n\n\nlogit\nprobit\ncloglog\ncauchit\n\n\n\n\n(Intercept)\n-12.246\n-6.452\n-11.377\n-24.086\n\n\nage\n0.062\n0.031\n0.057\n0.115\n\n\nheight\n0.007\n0.003\n0.007\n0.053\n\n\nweight\n0.009\n0.005\n0.007\n0.003\n\n\nsdp\n0.018\n0.010\n0.017\n0.042\n\n\ndbp\n-0.001\n-0.001\n-0.001\n-0.001\n\n\nchol\n0.011\n0.006\n0.010\n0.020\n\n\ncigs\n0.021\n0.011\n0.019\n0.043\n\n\ndibepB\n-0.658\n-0.341\n-0.604\n-1.320\n\n\narcuspresent\n0.211\n0.101\n0.206\n0.711\n\n\n\n\n\n\n# Compare fitted values (i.e., predicted probabilities)\npreds &lt;- cbind(\n  \"logit\" = fitted(wcgs.logit),\n  \"probit\" = fitted(wcgs.probit),\n  \"cloglog\" = fitted(wcgs.cloglog),\n  \"cauchit\" = fitted(wcgs.cauchit)\n)\nhead(round(preds, digits = 3))\n\n\nA matrix: 6 √ó 4 of type dbl\n\n\n\nlogit\nprobit\ncloglog\ncauchit\n\n\n\n\n1\n0.071\n0.072\n0.072\n0.076\n\n\n2\n0.073\n0.074\n0.075\n0.084\n\n\n3\n0.010\n0.006\n0.011\n0.038\n\n\n4\n0.010\n0.006\n0.012\n0.039\n\n\n5\n0.169\n0.167\n0.168\n0.150\n\n\n6\n0.034\n0.033\n0.035\n0.051\n\n\n\n\n\n\npalette(\"Okabe-Ito\")\nplot(sort(preds[, \"logit\"]), type = \"l\")\nlines(sort(preds[, \"probit\"]), col = 2)\nlines(sort(preds[, \"cloglog\"]), col = 3)\nlines(sort(preds[, \"cauchit\"]), col = 4)\nlegend(\"topleft\", legend = c(\"logit\", \"probit\", \"cloglog\", \"cauchit\"), \n       col = 1:4, lty = 1, inset = 0.01)\npalette(\"default\")\n\n\n\n\n\n\n\n\n\n# Compare deviances (similar to comparing SSEs in linear regression)\nsapply(list(wcgs.logit, wcgs.probit, wcgs.cloglog, wcgs.cauchit),\n       FUN = function(object) deviance(object))\n\n\n1569.471528859881565.682778947981571.995018956091615.47659286525\n\n\n\n###############################################################################\n#\n# Understanding logistic regression with binomial data\n#\n# (predicting O-ring failure; the Challenger disaster)\n#\n###############################################################################\n\n\n# Read data documentation\n?faraway::orings\n\n\n# Load the oring data\ndata(orings, package = \"faraway\")\n\n# Inspect data\nhead(orings)\n\n\nA data.frame: 6 √ó 2\n\n\n\ntemp\ndamage\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n53\n5\n\n\n2\n57\n1\n\n\n3\n58\n1\n\n\n4\n63\n1\n\n\n5\n66\n0\n\n\n6\n67\n0\n\n\n\n\n\n\n# Scatterplot of proportion of damaged O-rings as a fucntion of temperature\nplot(damage / 6 ~ temp, data = orings)\n\n\n# Expand binomial trials into Bernoulli\ntmp &lt;- rep(orings$temp, each = 6)\ndmg &lt;- sapply(orings$damage, FUN = function(x) rep(c(0, 1), times = c(6 - x, x)))\n#head(dmg)\norings2 &lt;- data.frame(\"temp\" = tmp, \"damage\" = as.vector(dmg))\nhead(orings2, n = 15)\n            \n# Fit a logistic regression (LR) model using 0/1 version of the data\norings.lr &lt;- glm(damage ~ temp, data = orings2, \n                 family = binomial(link = \"logit\"))\ncoef(orings.lr)\n\n\nA data.frame: 15 √ó 2\n\n\n\ntemp\ndamage\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n53\n0\n\n\n2\n53\n1\n\n\n3\n53\n1\n\n\n4\n53\n1\n\n\n5\n53\n1\n\n\n6\n53\n1\n\n\n7\n57\n0\n\n\n8\n57\n0\n\n\n9\n57\n0\n\n\n10\n57\n0\n\n\n11\n57\n0\n\n\n12\n57\n1\n\n\n13\n58\n0\n\n\n14\n58\n0\n\n\n15\n58\n0\n\n\n\n\n\n(Intercept)11.6629896662235temp-0.216233663599986\n\n\n\n# What is the chance that an O-ring will be damaged at 31F? Give a point \n# estimate as well as a 95% confidence interval.\npredict(orings.lr, newdata = data.frame(\"temp\" = 31), type = \"response\", \n        se = TRUE)\n\n\n# Better approach for a 95% CI?\npred &lt;- predict(orings.lr, newdata = data.frame(\"temp\" = 31), type = \"link\", \n                se = TRUE)\nplogis(pred$fit + c(-qnorm(0.975), qnorm(0.975)) * pred$se.fit)\n\n\n    $fit\n        1: 0.993034154696766\n    $se.fit\n        1: 0.0115330160107244\n    $residual.scale\n        1\n\n\n\n\n0.8444823833398830.999732875172426\n\n\n\n# What will happen to the chance of damage if the temperature were to drop by 30F?\nexp(-30 * coef(orings.lr)[2])\n\n\n# Is this extrapolating?\npalette(\"Okabe-Ito\")\nplot(damage / 6 ~ temp, data = orings, pch = 19, cex = 1.3,\n     col = adjustcolor(1, alpha.f = 0.3), xlim = c(0, 100), ylim = c(0, 1))\nx &lt;- seq(from = 0, to = 100, length = 1000)\ny &lt;- predict(orings.lr, newdata = data.frame(\"temp\" = x), type = \"response\")\nlines(x, y, lwd = 2, col = 2)\nabline(v = 31, lty = 2, col = 3)\npalette(\"default\")\n\n\n\n\n\n\n\n\n\n# More interesting question: at what temperature(s) can we expect the risk/\n# probability of damage to exceed 0.8?\n#\n# This is a problem of inverse estimation, which is the purpose of the investr\n# package in R; see https://journal.r-project.org/archive/2014/RJ-2014-009/index.html\n#\n# To install from CRAN, use\n#\n# &gt; install.packages(\"investr\")\n#\n# See ?investr::invest for details and examples\ninvestr::invest(orings.lr, y0 = 0.8, interval = \"Wald\", lower = 40, upper = 60)\n\n\n# Equivalent LR model using original binomial data \norings.lr2 &lt;- glm(cbind(damage, 6 - damage) ~ temp, data = orings,\n                  family = binomial(link = \"logit\"))\ncoef(orings.lr2)\n\n(Intercept)11.6629896952652temp-0.216233664113662\n\n\n\n# Print summary of model fit\nsummary(orings.lr2)\n\n\n# Compute residual deviance\nr &lt;- residuals(orings.lr2, type = \"deviance\")\nsum(r ^ 2)  # similar to SSE statistic in linear regression\n\n\n# Over-dispersion\n#\n# * Over-dispersion is said to exist when there is more variability than \n#   expected under the response distribution; similar for underdispersion.\n#\n# * For a correctly specified model, the Pearson chi-square statistic and the \n#   deviance, divided by their degrees of freedom, should be approximately \n#   equal to one. When their values are much larger than one, the assumption \n#   of binomial variability might not be valid and the data are said to exhibit \n#   overdispersion. Underdispersion, which results in the ratios being less \n#   than one, occurs less often in practice.\n#\n# * Overall performance of the fitted model can be measured by several \n#   different goodness-of-fit tests. Two tests that require replicated data \n#   (multiple observations with the same values for all the predictors) are the \n#   Pearson chi-square goodness-of-fit test and the deviance goodness-of-fit \n#   test (analagous to the multiple linear regression lack-of-fit F-test).\n#\n# * When fitting a model, there are several problems that can cause the \n#   goodness-of-fit statistics to exceed their degrees of freedom. Among these \n#   are such problems as outliers in the data, using the wrong link function, \n#   omitting important terms from the model, and needing to transform some \n#   predictors. These problems should be eliminated before proceeding to use \n#   the following methods to correct for overdispersion.\n#\n# * A large difference between the Pearson statistic and the deviance provides \n#   some evidence that the data are too sparse to use either statistic.\n\n\n# Can use quasi-binomial family to account for over-dispersion\nfit.qb &lt;- glm(cbind(damage, 6 - damage) ~ temp, data = orings, family = quasibinomial)\nsummary(fit.qb)\n\n\nCall:\nglm(formula = cbind(damage, 6 - damage) ~ temp, family = quasibinomial, \n    data = orings)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.9529  -0.7345  -0.4393  -0.2079   1.9565  \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) 11.66299    3.81077   3.061  0.00594 **\ntemp        -0.21623    0.06148  -3.517  0.00205 **\n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n(Dispersion parameter for quasibinomial family taken to be 1.336542)\n\n    Null deviance: 38.898  on 22  degrees of freedom\nResidual deviance: 16.912  on 21  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n#\n# Same as using quasi-binomial family above\n#\n\n# Estimate of dispersion parameter; analagous to MSE in linear reg.\n(phi &lt;- sum(residuals(orings.lr2, type = \"pearson\") ^ 2) / orings.lr2$df.residual)\n\n# Print model summary based on estimated dispersion parameter\nsummary(orings.lr2, dispersion = phi)\n\n1.33654188207683\n\n\n\nCall:\nglm(formula = cbind(damage, 6 - damage) ~ temp, family = binomial(link = \"logit\"), \n    data = orings)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.9529  -0.7345  -0.4393  -0.2079   1.9565  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 11.66299    3.81077   3.061 0.002209 ** \ntemp        -0.21623    0.06148  -3.517 0.000436 ***\n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n(Dispersion parameter for binomial family taken to be 1.336542)\n\n    Null deviance: 38.898  on 22  degrees of freedom\nResidual deviance: 16.912  on 21  degrees of freedom\nAIC: 33.675\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n# How is the residual deviance computed?\ndeviance(orings.lr2)\n\n# Computing by hand\nd &lt;- residuals(orings.lr2, type = \"deviance\")\nsum(d ^ 2)  # analagous to SSE in linear reg."
  },
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "Assignments and exams.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nSubtitle\n\n\n\n\n\n\n\n\nBANA 7042: Assignment 1\n\n\nLinear Regression\n\n\n\n\n\n\nBANA 7042: Assignment 2\n\n\nLogistic regression (part I)\n\n\n\n\n\n\nBANA 7042: Assignment 3\n\n\nLogistic regression (part II)\n\n\n\n\n\n\nBANA 7042: Assignment 4\n\n\nLogistic Regression III\n\n\n\n\n\n\nBANA 7042: Assignment 5\n\n\nMultinomial & Ordinal Regression\n\n\n\n\n\n\nBANA 7042: Assignment 6\n\n\nCount Regression\n\n\n\n\n\n\nBANA 7042: Final Exam\n\n\n¬†\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Final Exam",
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/final.html",
    "href": "assignments/final.html",
    "title": "BANA 7042: Final Assignment/Exam",
    "section": "",
    "text": "Note: Please be sure to properly label all figures and include a caption for each!\nQuestion 1. The data below gives the number of new AIDS cases each year in a country, from 1981 onwards (Source: Venables and Ripley (2003), slightly modified).\ncases &lt;- c(12, 14, 33, 50, 67, 74, 123, 141, 165, 204, 253, 246, 240, 246, 232)\nyear &lt;- 1:15 + 1980\npar(mfrow = c(2, 2))  # set up 2x2 plotting grid\nplot(&lt;my-model&gt;, which = 1:4)\nQuestion2\nSource: Bilder and Loughin (2015)\nIn order to maximize sales, items within grocery stores are strategically placed to draw customer attention. This exercise examines one type of item‚Äîbreakfast cereal. Typically, in large grocery stores, boxes of cereal are placed on sets of shelves located on one side of the aisle. By placing particular boxes of cereals on specific shelves, grocery stores may better attract customers to them. To investigate this further, a random sample of size 10 was taken from each of four shelves at a Dillons grocery store in Manhattan, KS. These data are given in the cereal.csv file. The response variable is the shelf number, which is numbered from bottom (1) to top (4), and the explanatory variables are the sugar, fat, and sodium content of the cereals. Using these data, complete the following:\n# Function to rescale a variable to be within (0, 1)\nstand01 &lt;- function (x) { \n    (x - min(x)) / (max(x) - min(x)) \n}\n\n# Standardize and rescale data for analysis\ncereal2 &lt;- data.frame(\n  Shelf = cereal$Shelf, \n  sugar = stand01(cereal$sugar_g / cereal$size_g), \n  fat = stand01(cereal$fat_g / cereal$size_g), \n  sodium = stand01(cereal$sodium_mg / cereal$size_g)\n)\nboxplot(sugar ~ Shelf, data = cereal2, ylab = \"Sugar\", xlab = \"Shelf\", \n        pars = list(outpch = NA))\nstripchart(cereal2$sugar ~ cereal2$Shelf, lwd = 2, col = \"red\", \n           method = \"jitter\", vertical = TRUE, pch = 1, add = TRUE)\nBased on your visual analysis, discuss if possible content differences exist among the shelves.\n#install.packages(\"pdp\")\nlibrary(pdp)\npfun &lt;- function(object, newdata) {\n  probs &lt;- predict(object, newdata = newdata, type = \"probs\")\n  colMeans(probs)\n}\n# Assuming the name of your model is `fit`\npd &lt;- partial(fit, pred.var = \"sugar\", pred.fun = pfun, plot = FALSE)\nlattice::xyplot(yhat ~ sugar|yhat.id, data = pd, type = \"l\", \n                ylab = \"probability\")\nQuestion 3 The failure of an O-ring on the space shuttle Challenger‚Äôs booster rockets led to its destruction in 1986. Using data on previous space shuttle launches, Dalal et al.¬†(1989) examine the probability of an O-ring failure as a function of temperature at launch and combustion pressure; in class, we looked at only temperature. Data from their paper is included in the challenger.csv file. Below are the variables:\n‚Ä¢ Flight: Flight number ‚Ä¢ Temp: Temperature (F) at launch ‚Ä¢ Pressure: Combustion pressure (psi) ‚Ä¢ O.ring: Number of primary field O-ring failures ‚Ä¢ Number: Total number of primary field O-rings (six total, three each for the two booster rockets)\nThe response variable is O.ring, and the explanatory variables are Temp and Pressure. Complete the following:\nQuestion 4. Consider the Bikeshare data set from the ISLR2 package. The data can be loaded into R using the following code:\n# install.packages(\"ISLR2\")  # run this first if you don't have ISLR2 installed\nbike &lt;- ISLR2::Bikeshare\nRead the help page for the data set using ?ISLR2::Bikeshare in the R console or by visiting the package documentation at the above link. Build an appropriate model using bikers as the response against the following predictors: workingday, temp, weathersit, mnth, and hr. Create a 1-2 page report describing the data, variables, and model you‚Äôve built (e.g., what kind of model did you choose and why). Be sure to include a well-formatted regression table with coefficients and standard errors. Any evidence of overdispersion or zero-inflation? If so, how did you deal with it. Provide an interpretation for each coefficient (for categorical variables, you only need to provide an interpretation for one of the categories). Discuss which variables seem the most important in predicting the response. Under what conditions does bike rentals seem to be highest? Do these results make sense? Include effect plots for what you consider to be the two most important predictors and describe any trends you see (be sure to explain how you selected these predictors in the first place). What recommendations can you make to the bike rental agency to improve the rental sales? DO NOT INCLUDE ANY R CODE OR DIRECT OUTPUT. Use well-formatted tables and graphics with captions. This is a report for a stakeholder, so simplicity, good grammar, and complete sentences are key. (Feel free to put R code and output in an appendix if you feel it‚Äôs necessary. Hint: It‚Äôs not.)"
  },
  {
    "objectID": "assignments/final.html#footnotes",
    "href": "assignments/final.html#footnotes",
    "title": "BANA 7042: Final Assignment/Exam",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe author of nnet::multinom() suggests this can help with the convergence of parameter estimates when using the function. We recommend the 0-1 rescaling because slow convergence occurs here when estimating the model in part (d) without re-scaling.‚Ü©Ô∏é"
  },
  {
    "objectID": "assignments/assignment-02.html",
    "href": "assignments/assignment-02.html",
    "title": "BANA 7042: Assignment 2",
    "section": "",
    "text": "Please clearly circle (or highlight) your multiple choice answers\nQuestion 1: True or false. In the LR model, it is assumed that there is a linear relationship between the predictor variables and the mean response (i.e., probability of success or \\(Y = 1\\)). If false, write the correct statement.\nQuestion 2 Why is ordinary linear regression not appropriate for binary outcomes?\n\nIt only has one weight per feature\nIt only has one output value\nThe bias parameter skews the output value\nIts predictions are not restricted to values between 0 and 1\n\nQuestion 3 Large values of the log-likelihood statistic indicates that:\n\nThere is potential multicollinearity in the data\nThe associated model fits the data well\nAs the predictor variables increase in value, the likelihood of the outcome occurring decreases\nThe associated model is a poor fit of the data\n\nQuestion 4 LR is used to predict the probability of a ___?\n\nBinary independent variable\nBinary dependent variable.\nContinuous dependent variable.\nContinuous independent variable.\n\nQuestion 5 In a logistic regression, if the predicted logit is 0, what‚Äôs the transformed probability?\n\n0\n1\n0.5\n0.05",
    "crumbs": [
      "Final Exam",
      "BANA 7042: Assignment 2"
    ]
  },
  {
    "objectID": "assignments/assignment-02.html#part-i-multiple-choice-1-point-each",
    "href": "assignments/assignment-02.html#part-i-multiple-choice-1-point-each",
    "title": "BANA 7042: Assignment 2",
    "section": "",
    "text": "Please clearly circle (or highlight) your multiple choice answers\nQuestion 1: True or false. In the LR model, it is assumed that there is a linear relationship between the predictor variables and the mean response (i.e., probability of success or \\(Y = 1\\)). If false, write the correct statement.\nQuestion 2 Why is ordinary linear regression not appropriate for binary outcomes?\n\nIt only has one weight per feature\nIt only has one output value\nThe bias parameter skews the output value\nIts predictions are not restricted to values between 0 and 1\n\nQuestion 3 Large values of the log-likelihood statistic indicates that:\n\nThere is potential multicollinearity in the data\nThe associated model fits the data well\nAs the predictor variables increase in value, the likelihood of the outcome occurring decreases\nThe associated model is a poor fit of the data\n\nQuestion 4 LR is used to predict the probability of a ___?\n\nBinary independent variable\nBinary dependent variable.\nContinuous dependent variable.\nContinuous independent variable.\n\nQuestion 5 In a logistic regression, if the predicted logit is 0, what‚Äôs the transformed probability?\n\n0\n1\n0.5\n0.05",
    "crumbs": [
      "Final Exam",
      "BANA 7042: Assignment 2"
    ]
  },
  {
    "objectID": "assignments/assignment-02.html#part-ii-short-answer-response-3-points-each",
    "href": "assignments/assignment-02.html#part-ii-short-answer-response-3-points-each",
    "title": "BANA 7042: Assignment 2",
    "section": "Part II: short answer response (3 points each)",
    "text": "Part II: short answer response (3 points each)\nQuestion 6. For ease of notation, let \\(\\eta = \\boldsymbol{x}^\\top\\boldsymbol{\\beta} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1 + \\cdots + \\beta_p x_p\\) be the linear predictor. For the LR model, where \\[\n\\mathrm{logit}\\left(p\\right) = \\boldsymbol{x}^\\top\\boldsymbol{\\beta},\n\\] show that \\[\np = \\frac{1}{1 + \\exp\\left(-\\eta\\right)}\n\\]\nQuestion 7: (2 pts.) Using historical data from past offers sent to customers, your company uses a logistic regression model to predict the likelihood that a customer will redeem a particular offer that was sent to them (e.g., a coupon for milk). The model is to be used on a new set of \\(n = 500\\) customers from the same population that the original training sample came from. However, your stakeholder asks another question. In addition to predicting the probability that each of the 500 new customers will redeem the offer sent to them, you‚Äôre asked to also provide an estimate of the total number of offers that will be redeeemd. Discuss how you would approach this problem. (No wrong answer here so don‚Äôt Google it, I just want to hear your group‚Äôs solution in detail.)\nFor the next few questions, you‚Äôll be using the blood donation data available from the UC Irvine Machine Learning Repository; you can download the data from here.\nQuestion 8 Import the data into R and fit a simple logistic regression model using Donated_Blood as the binary response and Recency as the predictor. Interpret the coefficient of Recency (be sure to use full sentences and include the units). How would you interpret the estimated intercept in this model?\nQuestion 9 Plot the fitted LR model from Question 8 on the probability scale (i.e., Recency on the \\(x\\)-axis and the predicted probability of donating on the \\(y\\)-axis). The donation center asks you to estimate the Recency associated with a predicted likelihood of donating of \\(p = 0.5\\). (In Pharmacology, this is referred to as the median effective dose.) Provide an estimate using your fitted model and describe how you arrived at this value.\nQuestion 10 Fit an LR model using all four predictors and provide an interpretation for each coefficient estimate. Without ‚ÄúGoogling‚Äù or ‚ÄúChatGPT-ing‚Äù, come up with a way of measuring the relative ‚Äúimportance‚Äù of each variable. (There‚Äôs a million ways to do this, none of which are perfect, so please use your own intuition and expertise to devise a way to accomplish this.) Which variable seems most important in predicting Donate_Blood? Provide a table with the ranking of each variable. Describe how your group arrived at these rankings. What are some potential drawbacks of your approach?",
    "crumbs": [
      "Final Exam",
      "BANA 7042: Assignment 2"
    ]
  },
  {
    "objectID": "assignments/assignment-06.html",
    "href": "assignments/assignment-06.html",
    "title": "BANA 7042: Assignment 6",
    "section": "",
    "text": "This assignment covers count regression models (Poisson, Negative Binomial).\nComing Soon",
    "crumbs": [
      "Final Exam",
      "BANA 7042: Assignment 6"
    ]
  },
  {
    "objectID": "assignments/assignment-06.html#overview",
    "href": "assignments/assignment-06.html#overview",
    "title": "BANA 7042: Assignment 6",
    "section": "",
    "text": "This assignment covers count regression models (Poisson, Negative Binomial).\nComing Soon",
    "crumbs": [
      "Final Exam",
      "BANA 7042: Assignment 6"
    ]
  },
  {
    "objectID": "assignments/assignment-05.html",
    "href": "assignments/assignment-05.html",
    "title": "BANA 7042: Assignment 5",
    "section": "",
    "text": "Please review Chapter 7 of ‚ÄúExtending the Linear Model with R: Generalized Linear, Mixed Effects and Nonparametric Regression Models‚Äù (second edition) by Julian J. Faraway. The chapter covers multinomial and ordinal regression models, which are used for modeling categorical response variables with more than two categories. You can find a PDF copy of the book in resources/elmr2.pdf (direct link).\nFor this assignment, please do Exercise 1 (a)‚Äì(j) from chapter 7 located on pp.¬†147‚Äì148. You can load the associated data set using the following code:\n\n# install.packages(\"faraway\")  # Uncomment this line to install the faraway package if you don't have it already\nhead(hsb &lt;- faraway::hsb)\n\n   id gender  race    ses schtyp     prog read write math science socst\n1  70   male white    low public  general   57    52   41      47    57\n2 121 female white middle public vocation   68    59   53      63    61\n3  86   male white   high public  general   44    33   54      58    31\n4 141   male white   high public vocation   63    44   47      53    56\n5 172   male white middle public academic   47    52   57      53    61\n6 113   male white middle public academic   44    52   51      63    61",
    "crumbs": [
      "Final Exam",
      "BANA 7042: Assignment 5"
    ]
  },
  {
    "objectID": "assignments/assignment-05.html#overview",
    "href": "assignments/assignment-05.html#overview",
    "title": "BANA 7042: Assignment 5",
    "section": "",
    "text": "Please review Chapter 7 of ‚ÄúExtending the Linear Model with R: Generalized Linear, Mixed Effects and Nonparametric Regression Models‚Äù (second edition) by Julian J. Faraway. The chapter covers multinomial and ordinal regression models, which are used for modeling categorical response variables with more than two categories. You can find a PDF copy of the book in resources/elmr2.pdf (direct link).\nFor this assignment, please do Exercise 1 (a)‚Äì(j) from chapter 7 located on pp.¬†147‚Äì148. You can load the associated data set using the following code:\n\n# install.packages(\"faraway\")  # Uncomment this line to install the faraway package if you don't have it already\nhead(hsb &lt;- faraway::hsb)\n\n   id gender  race    ses schtyp     prog read write math science socst\n1  70   male white    low public  general   57    52   41      47    57\n2 121 female white middle public vocation   68    59   53      63    61\n3  86   male white   high public  general   44    33   54      58    31\n4 141   male white   high public vocation   63    44   47      53    56\n5 172   male white middle public academic   47    52   57      53    61\n6 113   male white middle public academic   44    52   51      63    61",
    "crumbs": [
      "Final Exam",
      "BANA 7042: Assignment 5"
    ]
  },
  {
    "objectID": "assignments/assignment-04.html",
    "href": "assignments/assignment-04.html",
    "title": "BANA 7042: Assignment 4",
    "section": "",
    "text": "This assignment covers advanced topics in logistic regression.\nComing Soon",
    "crumbs": [
      "Final Exam",
      "BANA 7042: Assignment 4"
    ]
  },
  {
    "objectID": "assignments/assignment-04.html#overview",
    "href": "assignments/assignment-04.html#overview",
    "title": "BANA 7042: Assignment 4",
    "section": "",
    "text": "This assignment covers advanced topics in logistic regression.\nComing Soon",
    "crumbs": [
      "Final Exam",
      "BANA 7042: Assignment 4"
    ]
  },
  {
    "objectID": "assignments/assignment-03.html",
    "href": "assignments/assignment-03.html",
    "title": "BANA 7042: Assignment 3",
    "section": "",
    "text": "Note: Please be sure to properly label all figures and include a caption for each!\nFor this assignment, you‚Äôll be working with data from the direct marketing campaigns (phone calls) of a Portuguese banking institution. The goal is to predict the likelihood that a client will subscribe a term deposit (i.e., the binary variable labeled y). The data are available from the UC Irvine Machine Learning Repository.\nDirect URL: https://archive.ics.uci.edu/dataset/222/bank+marketing\nI have already downloaded the data and split them into two samples, which are available on the corresponding Assignments tab in our course‚Äôs Canvas page:\n\nbank.csv - these data will be used to build/train your logistic regression models;\nbank_new.csv - these data will be used for testing your final logistic regression model.\n\nQuestion 1: Use the bank.csv data to build an initial logistic regression model using all available predictors.\n\nDoes there appear to be any issues with the model? If so, please elaborate on what you see.\nUse the methods described in class to explore and potentially reduce the model further and explain your process (e.g., look for potential multicollinearity and redundant features, etc.).\nDraw an ROC curve and calibration plot for your final model. Which plot do you think is more useful given the task at hand?\nRead the background documentation for these data here. Does there appear to be any leakage variables in the data? If so, describe which ones and refit your model without those predictors. How do the ROC and calibration curves compare to the previous model (e.g., better or worst)? Does this make sense?\n\nQuestion 2: Try exploring forward selection (BS) and backward elimination (BE). For FS, start with an intercept-only model. For BE, you can start with your final model from Question 1. How do the results compare between FS and BE? How do they compare to your final model from Question 1?\nQuestion 3: Using the Brier score metric (see function below), compute the LOCO-based variable importance for all of the variables in your final model and display the results using a simple dotchart (do this using the Brier score). Do the results make sense? (You can use and modify the R code we walked through in class.)\n\n# Brier score function to compute MSE between binary y and probability p\n#\n# Args:\n#   y - the binary 0/1 outcome\n#   p - the predicted probability that y=1\nbrier_score &lt;- function(y, p) {\n  mean((y - p) ^ 2)\n}\n\nQuestion 4: Apply your final model to the bank_new.csv data to obtain the predicted probability that each customer will subscribe to a term deposit if offered. Using these results, construct a cumulative gains chart (like we did in class). If we used your model to identify the 1000 customers most likely to subscribe, how many successes can we expect? Be sure to include your plot as well.",
    "crumbs": [
      "Final Exam",
      "BANA 7042: Assignment 3"
    ]
  },
  {
    "objectID": "assignments/assignment-01.html",
    "href": "assignments/assignment-01.html",
    "title": "BANA 7042: Assignment 1",
    "section": "",
    "text": "Please clearly circle (or highlight) your multiple choice answers\nQuestion 1: In a multiple linear regression model, which of the following are considered as random?\nQuestion 2: When developing a linear regression model, adding additional predictors to the model will\nQuestion 3: What is the best way to identify potential multicollinearity?\nQuestion 4: When building a linear regression model of the form \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\\) for a data set, the presence of multicollinearity in the data may result in _________ standard errors for the slope coefficients (\\(\\beta_1\\) and \\(\\beta_2\\)) than if the data came from an orthogonal design (i.e., when \\(X_1\\) and \\(X_2\\) are uncorrelated).\nQuestion 5: What does the following residual versus fitted value plot suggest about a model between a single predictor \\(X\\) and \\(Y\\)?\nQuestion 6: When a linear regression model is being developed, adding additional variables to the model will\nQuestion 7: The ordinary residuals refer to\nQuestion 8: For a fitted simple linear regression model, which one of the following properties is NOT true?\nQuestion 9: The diagonal elements of hat matrix, also referred to as the hat values or leverage values, measures the influence of observation \\(i\\) on the regression line when removing observation \\(i\\).\nQuestion 10: In a regression study, a 95% confidence interval for \\(\\beta_1\\) was given as: \\(\\left(-5, 2\\right)\\). What does this confidence interval mean?\nQuestion 11: In a regression study, a 95% confidence interval for \\(\\beta_1\\) was given as: \\(\\left(-5, 2\\right)\\). Which of following is correct?\nQuestion 12: Point A in the far right is likely to be\nQuestion 13: Which of the following cases can lead to multicollinearity?\nQuestion 14: What does the last line of output ‚ÄúF-statistic‚Ä¶.p-value: ‚Ä¶‚Äù indicate in the following multiple regression output?\nCall:\nlm(formula = mpg ~ cyl + disp + hp + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0562 -1.4636 -0.4281  1.2854  5.8269 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 40.82854    2.75747  14.807 1.76e-14 ***\ncyl         -1.29332    0.65588  -1.972 0.058947 .  \ndisp         0.01160    0.01173   0.989 0.331386    \nhp          -0.02054    0.01215  -1.691 0.102379    \nwt          -3.85390    1.01547  -3.795 0.000759 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.513 on 27 degrees of freedom\nMultiple R-squared:  0.8486,    Adjusted R-squared:  0.8262 \nF-statistic: 37.84 on 4 and 27 DF,  p-value: 1.061e-10\nQuestion 15: If one wishes to incorporate seasonal dummy variables for monthly data into a regression model, how many dummy variables should be in the model?\nQuestion 16: Consider a simple linear regression model where \\(\\log(Y) = \\beta_0 + \\beta_1 X + \\varepsilon\\). If \\(\\beta_1 = 0.05\\), which best describes this coefficient‚Äôs interpretation?",
    "crumbs": [
      "Final Exam",
      "BANA 7042: Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-01.html#part-ii-short-answer-response",
    "href": "assignments/assignment-01.html#part-ii-short-answer-response",
    "title": "BANA 7042: Assignment 1",
    "section": "Part II: short answer response",
    "text": "Part II: short answer response\nFor the multiple linear regression model \\(Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\dots + \\beta_p X_{p-1, i} + \\epsilon_i\\)\nQuestion 17: (2 pts.) Recall that the variance inflation factor for \\(X_j\\) is defined as \\(VIF_j = \\left(1 - R_j^2\\right)^{-1}\\). Explain in one sentence what \\(R_j^2\\) is. Explain in one sentence why it is better to use VIFs than pairwise correlations when trying to detect the presence of multicollinearity in a regression data set.\nQuestion 18: (2 pts.) For variable selection criteria based on residuals, among \\(PRESS\\), \\(SSE\\), \\(R^2\\), \\(R_{adj}^2\\), and \\(MSE\\), which are ‚Äúgood‚Äù criteria for variable selection? Explain your reason in two sentences.\nQuestion 19: (4 pts.) Please state one possible violation of normal linear regression model assumptions for each of the residual plots below (one sentence for each plot):\n\n\n\n\n\n\n\n\n\nQuestion 20: (4 pts.) Suppose you want to build a linear regression for response variable weight ( \\(Y\\) ) using covariate height ( \\(X_1\\) ) and gender ( \\(X_2 = 0\\) for female and \\(X_2 = 1\\) for male).\n\nSuppose you want to allow the slope (of height) to be the same for different gender but different intercept, how would you build the linear regression model? Please specify the model in one line.\nSuppose you want to allow both slope (of height) and intercept to be different for different gender, how would you build the linear regression model? Please specify the model in one line.\n\nQuestion 21: (2 pts.) An engineer has stated: ‚ÄúReduction of the number of candidate explanatory variables should always be done using the objective forward stepwise regression procedure.‚Äù Discuss.\nQuestion 22: (4 pts.) A junior investment analyst used a polynomial regression model of relatively high order in a research seminar on municipal bonds and obtained an \\(R^2\\) of 0.991 in the regression of net interest yield of bond ( \\(Y\\) ) on industrial diversity index of municipality ( \\(X\\) ) for seven bond issues. A colleague, unimpressed, said: ‚ÄúYou overfitted. Your curve follows the random effects in the data.‚Äù Comment on the colleague‚Äôs criticism.\nQuestion 23: (4 pts.) In a regression study of factors affecting learning time for a certain task (measured in minutes), gender of learner was included as a predictor variable (\\(X_2\\)) that was coded \\(X_2 = 1\\) if male and \\(X_2 = 0\\) if female. It was found that the estimated coefficient of \\(X_2\\) was \\(\\widehat{\\beta}_2 = 22.3\\) with a standard error of 3.8. An observer questioned whether the coding scheme for gender is fair because it results in a positive coefficient, leading to longer learning times for males than females. Comment.\nQuestion 24: (2 pts.) A student stated: ‚ÄúAdding predictor variables to a regression model can never reduce \\(R^2\\), so we should include all available predictor variables in the model.‚Äù Comment.\nQuestion 25: (2 pts.) Evaluate the following statement: ‚ÄúFor the least squares method to be fully valid, it is required that the distribution of \\(Y\\) be normal.‚Äù\nQuestion 26: (2 pts.) What is a residual? Why are residuals important in regression analysis?\nQuestion 27: (3 pts.) Explain the purpose of \\(k\\)-fold cross-validation in regression modeling. Why is it generally preferred over a single train-test split for assessing model performance? (2-3 sentences)\nQuestion 28: Consider a dataset where the response variable \\(Y\\) is binary (0 or 1). Explain why ordinary linear regression is not appropriate for modeling this relationship, and what problem arises if you try to use it. (2-3 sentences)\nQuestion 29: In the updated review slides, we discussed linear regression within the framework of Generalized Linear Models (GLMs). Identify and describe the three components of a GLM (Random Component, Systematic Component, and Link Function) specifically as they apply to the normal linear regression model.\nQuestion 30 (Bonus): (5 pts.) An analyst wanted to fit the regression model \\(Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\dots + \\beta_p X_{p-1, i} + \\epsilon_i\\) by the method of least squares when it is known that \\(\\beta_2 = 4\\). How can the analyst obtain the desired fit using standard statistical software (e.g., R, Python, or SAS)? No need to run any code, just describe in general how you could accomplish this.",
    "crumbs": [
      "Final Exam",
      "BANA 7042: Assignment 1"
    ]
  },
  {
    "objectID": "assignments/final-exam.html",
    "href": "assignments/final-exam.html",
    "title": "BANA 7042: Final Exam",
    "section": "",
    "text": "Note: Please be sure to properly label all figures and include a caption for each!\nBefore answering the following questions, please do the following:",
    "crumbs": [
      "Final Exam",
      "BANA 7042: Final Exam"
    ]
  },
  {
    "objectID": "assignments/final-exam.html#questions",
    "href": "assignments/final-exam.html#questions",
    "title": "BANA 7042: Final Exam",
    "section": "Questions",
    "text": "Questions\nThe winequality.csv data set includes data on both red and white wines; for this analysis, you‚Äôll only be focusing on the red wines. The response variable of interest is quality.\n\nSubset the data to only include observations on red wines. Describe the distribution of the response. Based on the nature of the response variable, what type of regression model would you suggest starting with (and why)?\nExplore the data using summary statistics and graphics. Do any of the variables appear to have an association with the overal quality score? If so, which? Describe the nature of these associations.\nConstruct a binary response according to the following rule \\[\nY = \\begin{cases}\n  1, \\quad \\texttt{quality} \\ge 7 \\\\\n  0, \\quad \\texttt{quality} &lt; 7\n\\end{cases}\n\\]\n\nFit a logistic regression to the data using all possible main effects (i.e., include each variable, but no interaction effects). Assess the performance of this model. Does the model seem well-calibrated? Discuss and provide a plot of a calibration curve.\n\nInterpret the effect of the predictor alcohol on the odds that quality &gt;= 7. Construct an effect plot visualizing the effect of alcohol on the probability that quality &gt;= 7 and describe the relationship. Does this plot look linear or nonlinear? If nonlinear, discuss how this is possible.\nDiscuss reasons why the modeling approach used in 3) is ill-advised for modeling these data.\nFit an ordinal regression model to the data using the original response (i.e., quality) using the orm() function from R package rms. Construct an effect plot for each predictor showing the effect on the predicted probability that quality &gt;= 7. From these, try to determine the top three predictors solely in terms of their effect on the predicted probability that quality &gt;= 7.\nConsider a single observation \\(x_0\\) (i.e., a single red wine) with the following characteristics:\n\n\n\n                            \nfixed.acidity         7.3000\nvolatile.acidity      0.6500\ncitric.acid           0.0000\nresidual.sugar        1.2000\nchlorides             0.0650\nfree.sulfur.dioxide  15.0000\ntotal.sulfur.dioxide 21.0000\ndensity               0.9946\npH                    3.3900\nsulphates             0.4700\nalcohol              10.0000\n\n\nBased on your fitted model from 6), provide estimates for the following quantities:\n\n\\(\\mathrm{P}\\left(\\texttt{quality} = 7 | x_0\\right)\\)\n\\(\\mathrm{P}\\left(\\texttt{quality} \\ge 7 | x_0\\right)\\)\n\\(\\mathrm{P}\\left(\\texttt{quality} = 9 | x_0\\right)\\)\n\\(\\mathrm{P}\\left(\\texttt{quality} \\le 9 | x_0\\right)\\)\n\n\nYou‚Äôre asked to use your model from part 6) to provide predictions for the white wines included in the original sample. Discuss whether or not you think this is a reasonable request and why. What would you do in practice (e.g., what if this was your boss asking)?",
    "crumbs": [
      "Final Exam",
      "BANA 7042: Final Exam"
    ]
  },
  {
    "objectID": "code/lecture-2.html",
    "href": "code/lecture-2.html",
    "title": "Visualizing discrete data",
    "section": "",
    "text": "# Install required package(s)\npkgs &lt;- c(\"faraway\", \"ggplot2\", \" RPart\", \" RPart.plot\", \"partykit\", \"pdp\")\nlib &lt;- installed.packages()[, \"Package\"]\ninstall.packages(setdiff(pkgs, lib))\n# Load WCGS data set from faraway package\ndata(wcgs, package = \"faraway\")\n\nhead(wcgs)  # print first few records\n\n\nA data.frame: 6 √ó 13\n\n\n\nage\nheight\nweight\nsdp\ndbp\nchol\nbehave\ncigs\ndibep\nchd\ntypechd\ntimechd\narcus\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;fct&gt;\n\n\n\n\n1\n49\n73\n150\n110\n76\n225\nA2\n25\nA\nno\nnone\n1664\nabsent\n\n\n2\n42\n70\n160\n154\n84\n177\nA2\n20\nA\nno\nnone\n3071\npresent\n\n\n3\n42\n69\n160\n110\n78\n181\nB3\n0\nB\nno\nnone\n3071\nabsent\n\n\n4\n41\n68\n152\n124\n78\n132\nB4\n20\nB\nno\nnone\n3064\nabsent\n\n\n5\n59\n70\n150\n144\n86\n255\nB3\n20\nB\nyes\ninfdeath\n1885\npresent\n\n\n6\n44\n72\n204\n150\n90\n182\nB4\n0\nB\nno\nnone\n3102\nabsent\n# Print general structure of object\nstr(wcgs)\n# Extract the three columns of interest and print a summary of each\nsummary(wcgs[, c(\"chd\", \"height\", \"cigs\")])\n# Construct a pie chart of the (binary) response\nptab &lt;- prop.table(table(wcgs$chd))  # convert frequencies to proportions\npie(ptab, main = \"Pie chart of Coronary Heart Disease\")\n# Bar chart\nbarplot(ptab, las = 1, col = \"forestgreen\")\n# Mosaic plot showing relationship between cigs and chd\nplot(chd ~ cigs, data = wcgs)\nlibrary(lattice)\n\n# Nonparametric density plot of height by chd\ndensityplot(~ height, groups = chd, data = wcgs, auto.key = TRUE)\nlibrary(ggplot2)\n\n# Nonparametric density plot of height by chd using ggplot2\nggplot(wcgs, aes(x = height, color = chd)) + \n  geom_density() +\n  theme_bw()\n# Boxplot of cigs vs. chd status\nplot(cigs ~ chd, data = wcgs, col = c(2, 3))\n# Boxplot of height vs. chd status with notches\nplot(height ~ chd, data = wcgs, col = c(2, 3), notch = TRUE)\n# Detour: decision trees are immensely useful tools for exploring new data sets\n# Fit a default CART-like decision tree \nrpart.plot::rpart.plot(rpart::rpart(chd ~ ., data = wcgs))\n# Fit a default conditional inference tree\nplot(partykit::ctree(chd ~ height + cigs, data = wcgs))"
  },
  {
    "objectID": "code/lecture-2.html#the-linear-probability-model",
    "href": "code/lecture-2.html#the-linear-probability-model",
    "title": "Visualizing discrete data",
    "section": "The Linear Probability Model",
    "text": "The Linear Probability Model\n\nwcgs$y &lt;- ifelse(wcgs$chd == \"no\", 0, 1)  # convert binary factory to a 0/1 indicator\n\n# Try fitting a linear model\nwrong.fit &lt;- lm(y ~ height + cigs, data = wcgs)\nsummary(wrong.fit)\n\n\nCall:\nlm(formula = y ~ height + cigs, data = wcgs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.25454 -0.09831 -0.06298 -0.05736  0.95387 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.0718275  0.1338592  -0.537    0.592    \nheight       0.0018723  0.0019171   0.977    0.329    \ncigs         0.0019539  0.0003339   5.851 5.38e-09 ***\n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\nResidual standard error: 0.2722 on 3151 degrees of freedom\nMultiple R-squared:  0.0111,    Adjusted R-squared:  0.01047 \nF-statistic: 17.69 on 2 and 3151 DF,  p-value: 2.303e-08\n\n\n\nAssumptions of the LR model\n\nBinary Response: The response variable \\(Y\\) must be binary (e.g., 0/1, yes/no, etc.)\nIndependence: The observations must be independent of one another (i.e., no repeated measures)\nLinearity: There must be a linear relationship between the predictors and the logit of the response\nNo Multicollinearity: The predictors should not be highly correlated with one another\nSample Size: Since LR uses MLE, it typically requires a larger sample size than ordinary linear regression for the results to be reliable (asymptotic properties)"
  },
  {
    "objectID": "code/lecture-2.html#logistic-regression",
    "href": "code/lecture-2.html#logistic-regression",
    "title": "Visualizing discrete data",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\\(E\\left(Y|x_1, x_2, \\dots\\right) = \\mu\\) Linear model (assume \\(y\\) is normal):\n\\(\\mu = \\boldsymbol{\\beta}\\boldsymbol{X}\\)\nGeneralized linear model (covers exponential family of distributions):\n\\(g\\left(\\mu\\right) = \\boldsymbol{\\beta}\\boldsymbol{X}\\)\nLogistic regression (assume \\(y\\) is Bernoulli):\n\\(\\mu = p \\in \\left[0, 1\\right]\\) and \\(g\\left(\\mu\\right) = \\log\\left(\\frac{p}{1 - p}\\right)\\)\n\n# Mapping real numbers on the logit scale to the probability scale\nx &lt;- seq(from = -10, to = 10, length = 999)  \ny &lt;- 1 / (1 + exp(-x))  # same as y &lt;- plogis(x) or y &lt;- exp(x) / (1 + exp(x))\nplot(x, y, type = \"l\", xlab = \"Logits\", ylab = \"Probability\")\nabline(h = c(0, 1), lty = 2)\n\n\n\n\n\n\n\n\n\n# Fit a generalized linear model (GLM); a logistic regression model, in this case\nbetter.fit &lt;- glm(chd ~ height + cigs, data = wcgs, family = binomial(link = \"logit\"))\nsummary(better.fit)\n\n\nCall:\nglm(formula = chd ~ height + cigs, family = binomial(link = \"logit\"), \n    data = wcgs)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -4.50161    1.84186  -2.444   0.0145 *  \nheight       0.02521    0.02633   0.957   0.3383    \ncigs         0.02313    0.00404   5.724 1.04e-08 ***\n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1781.2  on 3153  degrees of freedom\nResidual deviance: 1749.0  on 3151  degrees of freedom\nAIC: 1755\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n# Comparing models (analogous to general linear F-test for comparing nested linear models)\nfit0 &lt;- glm(chd ~ 1, family = binomial(link = \"logit\" ), data = wcgs)\nfit3 &lt;- glm(chd ~ height + cigs, family = binomial(link = \"logit\" ), data = wcgs)\n\n# What statistical hypothesis is being tested here?\nanova(fit3, fit0, test = \"Chi\")\n\n\nA anova: 2 √ó 5\n\n\n\nResid. Df\nResid. Dev\nDf\nDeviance\nPr(&gt;Chi)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n3151\n1749.049\nNA\nNA\nNA\n\n\n2\n3153\n1781.244\n-2\n-32.19451\n1.02106e-07"
  },
  {
    "objectID": "code/lecture-2.html#effect-visualization",
    "href": "code/lecture-2.html#effect-visualization",
    "title": "Visualizing discrete data",
    "section": "Effect visualization",
    "text": "Effect visualization\n\nsummary(better.fit)\n\n\nCall:\nglm(formula = chd ~ height + cigs, family = binomial(link = \"logit\"), \n    data = wcgs)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -4.50161    1.84186  -2.444   0.0145 *  \nheight       0.02521    0.02633   0.957   0.3383    \ncigs         0.02313    0.00404   5.724 1.04e-08 ***\n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1781.2  on 3153  degrees of freedom\nResidual deviance: 1749.0  on 3151  degrees of freedom\nAIC: 1755\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n# Let's visualize the effect of height on the probability scale while holding cigs constant\nnewd &lt;- data.frame(\"height\" = 70, \"cigs\" = 0:99)\nprint(head(newd))\nhead(pred &lt;- predict(better.fit, newdata = newd, type = \"link\"))\nplot(newd$cigs, pred, type = \"l\", xlab = \"cigs\", ylab = \"Predicted Probability\")\n\n  height cigs\n1     70    0\n2     70    1\n3     70    2\n4     70    3\n5     70    4\n6     70    5\n\n\n1-2.737068827512212-2.713941429464043-2.690814031415864-2.667686633367685-2.644559235319516-2.62143183727133\n\n\n\n\n\n\n\n\n\n\nlibrary(pdp)\n\n# Prediction wrapper that tells `pdp::partial()` how to compute predictions from given model\npfun &lt;- function(object, newdata) {\n  mean(predict(object, newdata = newdata, type = \"response\"))\n}\n\n# Compute (approximate) partial dependence\npd &lt;- partial(better.fit, pred.var = \"height\", pred.fun = pfun)\nplot(pd, type = \"l\")\n\n\n\n\n\n\n\n\n\npd2 &lt;- partial(better.fit, pred.var = c(\"height\", \"cigs\"), chull = TRUE, pred.fun = pfun, progress = \"text\")\nlattice::wireframe(yhat ~ height * cigs, data = pd2, shade = TRUE)"
  },
  {
    "objectID": "code/lecture-2.html#variable-selection",
    "href": "code/lecture-2.html#variable-selection",
    "title": "Visualizing discrete data",
    "section": "Variable selection",
    "text": "Variable selection\n\n# Backward elimination\nfit.back &lt;- MASS::stepAIC(better.fit, direction = \"backward\")\ncoef(fit.back)\n\n\n# Forward selection\nfit.forw &lt;- MASS::stepAIC(fit0, direction = \"forward\")\ncoef(fit.forw)\n\n\n# WTF?"
  },
  {
    "objectID": "code/lecture-2.html#optimization-under-the-hood",
    "href": "code/lecture-2.html#optimization-under-the-hood",
    "title": "Visualizing discrete data",
    "section": "Optimization (Under the hood)",
    "text": "Optimization (Under the hood)\n\n# Response (as a binary 0/1 variable)\ny &lt;- wcgs$y\n\n# Model matrix; includes a column for the intercept by default\nX &lt;- model.matrix(~ height + cigs, data = wcgs)\n\n# Function to compute the negative log-likelihood (as a function of the betas)\nnll &lt;- function(beta) {\n  lp &lt;- X %*% beta\n  -sum(y * lp - log(1 + exp(lp)))\n}\n\n# Use general optimization\noptim(coef(wrong.fit), fn = nll, \n      control = list(\"maxit\" = 9999, \"reltol\" = 1e-20))"
  },
  {
    "objectID": "code/lecture-6.html",
    "href": "code/lecture-6.html",
    "title": "BANA 7042",
    "section": "",
    "text": "# Install required package(s)\npkgs &lt;- c(\"faraway\", \"investr\", \"mgcv\", \"performance\", \"pscl\")\nlib &lt;- installed.packages()[, \"Package\"]\ninstall.packages(setdiff(pkgs, lib))\n\n\n# Y ~ Poisson(lambda = 0.5)\nset.seed(2004)  # for reproducibility\npar(mfrow = c(2, 2))\nfor (lambda in c(0.5, 2, 5, 15)) {\n  y &lt;- dpois(0:35, lambda = lambda)\n  barplot(y, xlab = \"y\", ylab = \"P(Y = y)\", names = 0:35, main = paste(\"E(Y) =\", lambda), \n          col = \"dodgerblue2\", border = \"dodgerblue2\", las = 1)\n}\n\n\n\n\n\n\n\n\n\ny &lt;- rpois(10000, lambda = 200)\nhist(y, 50)\n\n\n\n\n\n\n\n\n\n# Read about the Galapagos Islands species diversity data set\n?faraway::gala\n\n\n# Load the Galapagos Islands data\ndata(gala, package = \"faraway\")\n\n# Print structure of data frame\nstr(gala)\n\n'data.frame':   30 obs. of  7 variables:\n $ Species  : num  58 31 3 25 2 18 24 10 8 2 ...\n $ Endemics : num  23 21 3 9 1 11 0 7 4 2 ...\n $ Area     : num  25.09 1.24 0.21 0.1 0.05 ...\n $ Elevation: num  346 109 114 46 77 119 93 168 71 112 ...\n $ Nearest  : num  0.6 0.6 2.8 1.9 1.9 8 6 34.1 0.4 2.6 ...\n $ Scruz    : num  0.6 26.3 58.7 47.4 1.9 ...\n $ Adjacent : num  1.84 572.33 0.78 0.18 903.82 ...\n\n\n\ngala$Endemics &lt;- NULL  # remove the second variable\n\n\n# Print a summary of the data\nsummary(gala)\n\n    Species            Area            Elevation          Nearest     \n Min.   :  2.00   Min.   :   0.010   Min.   :  25.00   Min.   : 0.20  \n 1st Qu.: 13.00   1st Qu.:   0.258   1st Qu.:  97.75   1st Qu.: 0.80  \n Median : 42.00   Median :   2.590   Median : 192.00   Median : 3.05  \n Mean   : 85.23   Mean   : 261.709   Mean   : 368.03   Mean   :10.06  \n 3rd Qu.: 96.00   3rd Qu.:  59.237   3rd Qu.: 435.25   3rd Qu.:10.03  \n Max.   :444.00   Max.   :4669.320   Max.   :1707.00   Max.   :47.40  \n     Scruz           Adjacent      \n Min.   :  0.00   Min.   :   0.03  \n 1st Qu.: 11.03   1st Qu.:   0.52  \n Median : 46.65   Median :   2.59  \n Mean   : 56.98   Mean   : 261.10  \n 3rd Qu.: 81.08   3rd Qu.:  59.24  \n Max.   :290.20   Max.   :4669.32  \n\n\n\n# This is a relatively small data set, so a scatterplot matrix could be useful\npairs(gala)\n\n\n\n\n\n\n\n\n\npairs(gala, log = \"xy\")\n\nWarning message in xy.coords(x, y, xlabel, ylabel, log):\n‚Äú1 y value &lt;= 0 omitted from logarithmic plot‚Äù\nWarning message in xy.coords(x, y, xlabel, ylabel, log):\n‚Äú1 y value &lt;= 0 omitted from logarithmic plot‚Äù\nWarning message in xy.coords(x, y, xlabel, ylabel, log):\n‚Äú1 y value &lt;= 0 omitted from logarithmic plot‚Äù\nWarning message in xy.coords(x, y, xlabel, ylabel, log):\n‚Äú1 y value &lt;= 0 omitted from logarithmic plot‚Äù\nWarning message in xy.coords(x, y, xlabel, ylabel, log):\n‚Äú1 x value &lt;= 0 omitted from logarithmic plot‚Äù\nWarning message in xy.coords(x, y, xlabel, ylabel, log):\n‚Äú1 x value &lt;= 0 omitted from logarithmic plot‚Äù\nWarning message in xy.coords(x, y, xlabel, ylabel, log):\n‚Äú1 x value &lt;= 0 omitted from logarithmic plot‚Äù\nWarning message in xy.coords(x, y, xlabel, ylabel, log):\n‚Äú1 x value &lt;= 0 omitted from logarithmic plot‚Äù\nWarning message in xy.coords(x, y, xlabel, ylabel, log):\n‚Äú1 x value &lt;= 0 omitted from logarithmic plot‚Äù\nWarning message in xy.coords(x, y, xlabel, ylabel, log):\n‚Äú1 y value &lt;= 0 omitted from logarithmic plot‚Äù\nWarning message in xy.coords(x, y, xlabel, ylabel, log):\n‚Äú1 x value &lt;= 0 omitted from logarithmic plot‚Äù\nWarning message in xy.coords(x, y, xlabel, ylabel, log):\n‚Äú1 y value &lt;= 0 omitted from logarithmic plot‚Äù\n\n\n\n\n\n\n\n\n\n\n# Try taking logs...\npairs(~ log(Species) + log(Area) + log(Elevation) + log(Nearest) + \n      log(Scruz + 0.1) + log(Adjacent), data = gala)\n\n\n# Look at distribution of response\nhist(gala$Species, breaks = nrow(gala), xlab = \"Number of species\", \n     col = \"plum\", border = \"white\")\n#\n#\n# Is this enough to suggest a distirbution for modeling? \n#\n# https://github.com/bgreenwell/uc-bana7052-old/issues/6\n\n\n\n\n\n\n\n\n\n# Try OLS\ngala.norm &lt;- lm(log(Species) ~ log(Area) + log(Elevation) + log(Nearest) + \n                I(log(Scruz + 0.1)) + log(Adjacent), data = gala)\nsummary(gala.norm)\n\n# Some basic residual plots\npar(mfrow = c(2, 2))\nplot(gala.norm, which = 1:4)\n\n\nCall:\nlm(formula = log(Species) ~ log(Area) + log(Elevation) + log(Nearest) + \n    I(log(Scruz + 0.1)) + log(Adjacent), data = gala)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.4563 -0.5192 -0.1059  0.4632  1.3351 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          5.10569    1.64880   3.097  0.00493 ** \nlog(Area)            0.50350    0.09942   5.064 3.53e-05 ***\nlog(Elevation)      -0.37384    0.32242  -1.159  0.25767    \nlog(Nearest)        -0.06564    0.11475  -0.572  0.57262    \nI(log(Scruz + 0.1)) -0.08255    0.09517  -0.867  0.39433    \nlog(Adjacent)       -0.02488    0.04596  -0.541  0.59327    \n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\nResidual standard error: 0.7877 on 24 degrees of freedom\nMultiple R-squared:  0.7899,    Adjusted R-squared:  0.7461 \nF-statistic: 18.05 on 5 and 24 DF,  p-value: 1.941e-07\n\n\n\n\n\n\n\n\n\n\ncar::vif(gala.norm)\n\nlog(Area)5.66151078721322log(Elevation)5.61566326876307log(Nearest)1.55476003192185I(log(Scruz + 0.1))1.57238403979044log(Adjacent)1.05523190766178\n\n\n\n# A simple Poisson GLM assumes that Y ~ Poisson(mu), where E(Y) = mu and\n#\n#   log(mu) = b0 + b1*x1 + b2*x2 + ... (i.e., the linear predictor).\n#\n# Note that 0 &lt; mu &lt; infinity, so the log transform/link makes sense.\n\n# Fit a simple (additive) Poisson GLM to the data\ngala.pois &lt;- glm(Species ~ log(Area) + log(Elevation) + log(Nearest) + \n                 I(log(Scruz + 0.1)) + log(Adjacent), family = poisson(link = \"log\"), \n                 data = gala)\nsummary(gala.pois)\n\n\nCall:\nglm(formula = Species ~ log(Area) + log(Elevation) + log(Nearest) + \n    I(log(Scruz + 0.1)) + log(Adjacent), family = poisson(link = \"log\"), \n    data = gala)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-5.4479  -2.6717  -0.4547   2.5613   8.2970  \n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          3.287941   0.284661  11.550  &lt; 2e-16 ***\nlog(Area)            0.348445   0.018029  19.327  &lt; 2e-16 ***\nlog(Elevation)       0.036421   0.056983   0.639  0.52272    \nlog(Nearest)        -0.040644   0.013781  -2.949  0.00318 ** \nI(log(Scruz + 0.1)) -0.030045   0.010492  -2.864  0.00419 ** \nlog(Adjacent)       -0.089014   0.006948 -12.812  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3510.73  on 29  degrees of freedom\nResidual deviance:  359.12  on 24  degrees of freedom\nAIC: 531.96\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n# Check for dispersion\nperformance::check_overdispersion(gala.pois)\n\nOverdispersion detected.\n\n\n\n\n# Overdispersion test\n       dispersion ratio =  16.559\n  Pearson's Chi-Squared = 397.420\n                p-value = &lt; 0.001\n\n\n\n\n\n# Same as above, but doing it by hand\n# Goodness of fit test (akin to the F test for linear regression)\n(gof &lt;- sum(residuals(gala.pois, type = \"pearson\") ^ 2))  # test statistic\npchisq(gof, df = df.residual(gala.pois), lower = FALSE)  # p-value\n\n397.420460059123\n\n\n2.5416081573052e-69\n\n\n\n# Check VIFs\ncar::vif(gala.pois)\n\nlog(Area)5.804485309982log(Elevation)5.98127389121554log(Nearest)1.52201481509847I(log(Scruz + 0.1))1.66491078411416log(Adjacent)1.38825332890851\n\n\n\n# Does the model fit the data well?\npred &lt;- predict(gala.pois, type = \"response\")\nplot(pred, gala$Species, xlab = \"Predicted count\", ylab = \"Observed count\")\nabline(0, 1, lty = 2, col = \"red3\")\n\n\n\n\n\n\n\n\n\n# Update model summary (i.e., standard errors with new dispersion parameter)\ndp &lt;- gof / df.residual(gala.pois)\nprint(dp)\nsummary(gala.pois, dispersion = dp)\n\n[1] 16.55919\n\n\n\nCall:\nglm(formula = Species ~ log(Area) + log(Elevation) + log(Nearest) + \n    I(log(Scruz + 0.1)) + log(Adjacent), family = poisson(link = \"log\"), \n    data = gala)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-5.4479  -2.6717  -0.4547   2.5613   8.2970  \n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          3.28794    1.15837   2.838  0.00453 ** \nlog(Area)            0.34844    0.07337   4.749 2.04e-06 ***\nlog(Elevation)       0.03642    0.23188   0.157  0.87519    \nlog(Nearest)        -0.04064    0.05608  -0.725  0.46859    \nI(log(Scruz + 0.1)) -0.03005    0.04270  -0.704  0.48162    \nlog(Adjacent)       -0.08901    0.02827  -3.149  0.00164 ** \n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n(Dispersion parameter for poisson family taken to be 16.55919)\n\n    Null deviance: 3510.73  on 29  degrees of freedom\nResidual deviance:  359.12  on 24  degrees of freedom\nAIC: 531.96\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n# Try using quasi-Poisson nmodel instead (equivalent to above, but more automatic)\ngala.pois &lt;- glm(Species ~ log(Area) + log(Elevation) + log(Nearest) + \n                 I(log(Scruz + 0.1)) + log(Adjacent), family = quasipoisson(link = \"log\"), \n                 data = gala)\nsummary(gala.pois)\n\n\nCall:\nglm(formula = Species ~ log(Area) + log(Elevation) + log(Nearest) + \n    I(log(Scruz + 0.1)) + log(Adjacent), family = quasipoisson(link = \"log\"), \n    data = gala)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-5.4479  -2.6717  -0.4547   2.5613   8.2970  \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          3.28794    1.15837   2.838  0.00908 ** \nlog(Area)            0.34844    0.07337   4.749 7.85e-05 ***\nlog(Elevation)       0.03642    0.23188   0.157  0.87650    \nlog(Nearest)        -0.04064    0.05608  -0.725  0.47559    \nI(log(Scruz + 0.1)) -0.03005    0.04270  -0.704  0.48839    \nlog(Adjacent)       -0.08901    0.02827  -3.149  0.00435 ** \n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n(Dispersion parameter for quasipoisson family taken to be 16.55919)\n\n    Null deviance: 3510.73  on 29  degrees of freedom\nResidual deviance:  359.12  on 24  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n###############################################################################\n#\n# A simple Poisson regression epidemic model\n#\n###############################################################################\n\n\n# Enter the data by hand and plot\ncases &lt;- c(12, 14, 33, 50, 67, 74, 123, 141, 165, 204, 253, 246, 240)\nyear &lt;- 1:13 + 1980\nplot(year, cases, xlab = \"Year\", ylab = \"New AIDS cases\", ylim = c(0, 280),\n     pch = 19, col = 1, las = 1)\n\n\n\n\n\n\n\n\n\n# Does it look like the number of new cases each year is growing unchecked?\n\n\naids &lt;- data.frame(\"cases\" = cases, \"year\" = year)\nsummary(aids.pois1 &lt;- glm(cases ~ year, data = aids, family = poisson(link = \"log\")))\ninvestr::plotFit(aids.pois1, interval = \"confidence\", shade = TRUE)\n\n\nCall:\nglm(formula = cases ~ year, family = poisson(link = \"log\"), data = aids)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-4.6784  -1.5013  -0.2636   2.1760   2.7306  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.971e+02  1.546e+01  -25.68   &lt;2e-16 ***\nyear         2.021e-01  7.771e-03   26.01   &lt;2e-16 ***\n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 872.206  on 12  degrees of freedom\nResidual deviance:  80.686  on 11  degrees of freedom\nAIC: 166.37\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\n\n\n\n\n\n# Check residual plots (these will use the \"deviance\" residuals for a Poisson model)\npar(mfrow = c(2, 2))\nplot(aids.pois1, which = 1:4)\n\n\n\n\n\n\n\n\n\n# Plot residual vs. year\npalette(\"Okabe-Ito\")\nscatter.smooth(aids$year + 1980, residuals(aids.pois1, type = \"deviance\"), pch = 19,\n    xlab = \"Year\", ylab = \"Deviance residual\", lpars = list(col = 3))\nabline(h = 0, lty = 2, col = 2)\npalette(\"default\")\n\n\n\n\n\n\n\n\n\n# Perhaps a quadratic trend would be better\naids.pois2 &lt;- glm(cases ~ poly(year, degree = 2), data = aids, family = poisson(link = \"log\"))\nsummary(aids.pois2)\ninvestr::plotFit(aids.pois2, interval = \"confidence\", shade = TRUE)\npar(mfrow = c(2, 2))\nplot(aids.pois2, which = 1:4)\n\n\nCall:\nglm(formula = cases ~ poly(year, degree = 2), family = poisson(link = \"log\"), \n    data = aids)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.45903  -0.64491   0.08927   0.67117   1.54596  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              4.44867    0.03803 116.964  &lt; 2e-16 ***\npoly(year, degree = 2)1  3.46922    0.15742  22.038  &lt; 2e-16 ***\npoly(year, degree = 2)2 -0.95511    0.11896  -8.029 9.82e-16 ***\n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 872.2058  on 12  degrees of freedom\nResidual deviance:   9.2402  on 10  degrees of freedom\nAIC: 96.924\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Although the output from summary gives us the relevant p-value, we could test\n# this more generally using our reduced-vs-full model framework\nanova(aids.pois1, aids.pois2, test = \"Chisq\")\n\n\n#\n# NOTE: The sample size here is incredibly small, so be suspicious of any inference...\n#\n\n\nres &lt;- resid(aids.pois2, type = \"pearson\")\nplot(res, type = \"l\")\n\n\n# Try fitting a generalized additive model (GAM); in a GAM, you can specify\n# nonparametric smooth functions of any of the predictors and not worry\n# as much about finding a suitable transformation manually\nlibrary(mgcv)\n\n# Fit a simple Poisson GAM using a smooth function of year\naids.pois3 &lt;- gam(cases ~ s(year), data = aids, family = poisson(link = \"log\"))\nsummary(aids.pois3)\nplot(aids.pois3, rug = TRUE)\n\n\nFamily: poisson \nLink function: log \n\nFormula:\ncases ~ s(year)\n\nParametric coefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   4.4578     0.0381     117   &lt;2e-16 ***\n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\nApproximate significance of smooth terms:\n          edf Ref.df Chi.sq p-value    \ns(year) 3.973  4.907  574.3  &lt;2e-16 ***\n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\nR-sq.(adj) =  0.989   Deviance explained = 99.2%\nUBRE = 0.30615  Scale est. = 1         n = 13\n\n\n\n\n\n\n\n\n\n\n###############################################################################\n#\n# Dealing with zero-inflated outcomes\n#\n###############################################################################\n\n\n# The state wildlife biologists want to model how many fish are being caught by \n# fishermen at a state park. Visitors are asked how long they stayed, how many \n# people were in the group, were there children in the group, and how many fish \n# were caught. Some visitors do not fish, but there is no data on whether a \n# person fished or not. Some visitors who did fish did not catch any fish so \n# there are excess zeros in the data because of the people that did not fish.\n#\n# Our sample consists of We have data on N=250 groups that went to a park. Each \n# group was questioned about how many fish they caught (count), how many \n# children were in the group (child), how many people were in the group \n# (persons), and whether or not they brought a camper to the park (camper).\n#\n# The data can be read in as follows:\nfish &lt;- read.csv(\"https://stats.idre.ucla.edu/stat/data/fish.csv\")\n\n\n# Retain only variables of interest and print summary\nfish &lt;- fish[, c(\"count\", \"child\", \"persons\", \"camper\")]\nsummary(fish)\n\n     count             child          persons          camper     \n Min.   :  0.000   Min.   :0.000   Min.   :1.000   Min.   :0.000  \n 1st Qu.:  0.000   1st Qu.:0.000   1st Qu.:2.000   1st Qu.:0.000  \n Median :  0.000   Median :0.000   Median :2.000   Median :1.000  \n Mean   :  3.296   Mean   :0.684   Mean   :2.528   Mean   :0.588  \n 3rd Qu.:  2.000   3rd Qu.:1.000   3rd Qu.:4.000   3rd Qu.:1.000  \n Max.   :149.000   Max.   :3.000   Max.   :4.000   Max.   :1.000  \n\n\n\n# Scatterplot matrix\npairs(log(count + 0.1) ~ ., data = fish)\n\n\n\n\n\n\n\n\n\n# Simple Poisson model\nfish.pois &lt;- glm(count ~ ., data = fish, family = poisson(link = \"log\"))\nsummary(fish.pois)\nbarplot(table(fish$count))\n\n\nCall:\nglm(formula = count ~ ., family = poisson(link = \"log\"), data = fish)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-6.8096  -1.4431  -0.9060  -0.0406  16.1417  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.98183    0.15226  -13.02   &lt;2e-16 ***\nchild       -1.68996    0.08099  -20.87   &lt;2e-16 ***\npersons      1.09126    0.03926   27.80   &lt;2e-16 ***\ncamper       0.93094    0.08909   10.45   &lt;2e-16 ***\n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2958.4  on 249  degrees of freedom\nResidual deviance: 1337.1  on 246  degrees of freedom\nAIC: 1682.1\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n\n\n\n\n\n\n# Check for zero-inflation\nperformance::check_zeroinflation(fish.pois)\n\nModel is underfitting zeros (probable zero-inflation).\n\n\n\n\n# Check for zero-inflation\n   Observed zeros: 142\n  Predicted zeros: 95\n            Ratio: 0.67\n\n\n\n\n\n# In addition to predicting the number of fish caught, there is interest in \n# predicting the existence of excess zeros (i.e., the zeroes that were not \n# simply a result of bad luck or lack of fishing skill). In particular, we'd\n# like to estimate the effect of party size on catching zero fish.\n#\n# We can accomplish this in several ways, but popular choices include \n#\n#   1) the zero-inflated Poisson (or negative binomial) model\n#   2) the hurdle model\n#\n# In this example, we'll use a simple hurdle model, which essentially fits \n# two seperate models:\n#\n#   * P(Y = 0) via a logistic regression\n#   * P(Y = j), j &gt; 0 via a truncated Poisson regression\n#\n# Below is a basic example that will help us answer the question(s) of interest\nlibrary(pscl)\nsummary(fish.hurdle &lt;- hurdle(count ~ child + camper | persons, data = fish))\n\n\nCall:\nhurdle(formula = count ~ child + camper | persons, data = fish)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-0.8590 -0.7384 -0.6782 -0.1234 23.9679 \n\nCount model coefficients (truncated poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.64668    0.08278  19.892   &lt;2e-16 ***\nchild       -0.75918    0.09004  -8.432   &lt;2e-16 ***\ncamper       0.75166    0.09112   8.249   &lt;2e-16 ***\nZero hurdle model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -0.7808     0.3240  -2.410   0.0160 *\npersons       0.1993     0.1161   1.716   0.0862 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 14 \nLog-likelihood: -1047 on 5 Df\n\n\n\n# Check the logit part manually\nz &lt;- fish\nz$count &lt;- ifelse(z$count == 0, 0, 1)\nglm(count ~ persons, data = z, family = binomial)\n\n\n# Interpretation:\n#\n# * For every additional child the expected log number of the fish caught \n#   reduces by 0.759.\n# \n# * Being a camper increases the expected log number of fish caufght by 0.752.\n#\n# * For every additional person the log odds of catching zero fish increases by \n#   0.199\n\n\n# Predict a new observation\nnd &lt;- data.frame(\"child\" = 0, \"persons\" = 3, \"camper\" = 1)\npredict(fish.hurdle, newdata = nd, type = \"prob\")\n\n\n# Try using a ZIP model instead\nsummary(fish.zip &lt;- zeroinfl(count ~ child + camper | persons, data = fish))\n\n\nCall:\nzeroinfl(formula = count ~ child + camper | persons, data = fish)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.2369 -0.7540 -0.6080 -0.1921 24.0847 \n\nCount model coefficients (poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.59789    0.08554  18.680   &lt;2e-16 ***\nchild       -1.04284    0.09999 -10.430   &lt;2e-16 ***\ncamper       0.83402    0.09363   8.908   &lt;2e-16 ***\n\nZero-inflation model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.2974     0.3739   3.470 0.000520 ***\npersons      -0.5643     0.1630  -3.463 0.000534 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 12 \nLog-likelihood: -1032 on 5 Df"
  },
  {
    "objectID": "code/lecture-3.html",
    "href": "code/lecture-3.html",
    "title": "BANA 7042",
    "section": "",
    "text": "#\n# Logistic Regression (LR) II\n#\n# * Variable ranking/model selection\n# * Model interpretation (again)\n# * Intro (albeit very brief) to penalized and nonparametric LR models\n# * Assessing performance via ROC and calibration curves\n#\n\n\n# Install required package(s)\npkgs &lt;- c(\"car\", \"corrplot\", \"earth\", \"faraway\", \"glmnet\", \"ggplot2\", \"Hmisc\", \"plotmo\", \"pdp\", \"pROC\", \"rms\", \"vip\")\nlib &lt;- installed.packages()[, \"Package\"]\ninstall.packages(setdiff(pkgs, lib))\n\n\n# Load Wisconsin breast cancer data set from faraway package\ndata(wcgs, package = \"faraway\")\n\nhead(wcgs)  # print first few records\n\n\nA data.frame: 6 √ó 13\n\n\n\nage\nheight\nweight\nsdp\ndbp\nchol\nbehave\ncigs\ndibep\nchd\ntypechd\ntimechd\narcus\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;fct&gt;\n\n\n\n\n1\n49\n73\n150\n110\n76\n225\nA2\n25\nA\nno\nnone\n1664\nabsent\n\n\n2\n42\n70\n160\n154\n84\n177\nA2\n20\nA\nno\nnone\n3071\npresent\n\n\n3\n42\n69\n160\n110\n78\n181\nB3\n0\nB\nno\nnone\n3071\nabsent\n\n\n4\n41\n68\n152\n124\n78\n132\nB4\n20\nB\nno\nnone\n3064\nabsent\n\n\n5\n59\n70\n150\n144\n86\n255\nB3\n20\nB\nyes\ninfdeath\n1885\npresent\n\n\n6\n44\n72\n204\n150\n90\n182\nB4\n0\nB\nno\nnone\n3102\nabsent\n\n\n\n\n\n\n?faraway::wcgs\n\n\n###############################################################################\n#\n# Model selection based on p-values\n#\n###############################################################################\n\n\n# Fit an additive logistic regression model (i.e., no interaction effects)\nlr.fit &lt;- glm(chd ~ height + cigs, family = binomial(link = \"logit\"), data = wcgs)\n\n# Print a summary of the fitted model\nsummary(lr.fit)\n\n\nCall:\nglm(formula = chd ~ height + cigs, family = binomial(link = \"logit\"), \n    data = wcgs)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.0041  -0.4425  -0.3630  -0.3499   2.4357  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -4.50161    1.84186  -2.444   0.0145 *  \nheight       0.02521    0.02633   0.957   0.3383    \ncigs         0.02313    0.00404   5.724 1.04e-08 ***\n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1781.2  on 3153  degrees of freedom\nResidual deviance: 1749.0  on 3151  degrees of freedom\nAIC: 1755\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n# Fit a reduced model by dropping the term for height\nlr.fit.reduced &lt;- glm(chd ~ cigs, family = binomial(link = \"logit\"), data = wcgs)\n\n# Compare full model to reduced model; what null hypothesis is implied here?\nanova(lr.fit.reduced, lr.fit)#, test = \"Chi\")\n\n\nA anova: 2 √ó 4\n\n\n\nResid. Df\nResid. Dev\nDf\nDeviance\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n3152\n1749.969\nNA\nNA\n\n\n2\n3151\n1749.049\n1\n0.9202473\n\n\n\n\n\n\n# I don't think drop1() or add1() are very useful; see ?drop1 for further details\ndrop1(lr.fit, test = \"Chi\")\n\n\nA anova: 3 √ó 5\n\n\n\nDf\nDeviance\nAIC\nLRT\nPr(&gt;Chi)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\n1749.049\n1755.049\nNA\nNA\n\n\nheight\n1\n1749.969\n1753.969\n0.9202473\n3.374101e-01\n\n\ncigs\n1\n1780.119\n1784.119\n31.0695040\n2.489521e-08\n\n\n\n\n\n\n# Compute AIC directly\nc(\"AIC (full)\" = AIC(lr.fit), \"AIC (reduced)\" = AIC(lr.fit.reduced))\n\nAIC (full)1755.0492329635AIC (reduced)1753.96948024571\n\n\n\n# Problems with the p-value approach:\n#\n# * Not what inference was designed for\n# * Does not take into account multiple testing issue\n\n\n###############################################################################\n#\n# Let's look at some effect plots using various packages...\n#\n###############################################################################\n\n\n# The plotmo package\nplotmo::plotmo(lr.fit)\n\n plotmo grid:    height cigs\n                     70    0\n\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(pdp)\n\n# Set theme for ggplot2 graphics\ntheme_set(theme_bw())\n\n# These plots will be linear on the logit scale (why?) but nonlinear on the probability scale \npartial(lr.fit, pred.var = \"cigs\", prob = TRUE, plot = TRUE, rug = TRUE,\n        plot.engine = \"ggplot2\") +\n  geom_rug(data = wcgs, aes(x = cigs), alpha = 0.2, inherit.aes = FALSE)  # why add this?\n\n\n# Easy enough to do by hand; here, we're plotting the predicted probability of \n# developing CHD as a function of cigs while holding height constant at its \n# median value (i.e., 70 inches)\nnewd &lt;- data.frame(\"cigs\" = 0:99, height = 70)\nprob &lt;- predict(lr.fit, newdata = newd, type = \"response\")\nplot(newd$cigs, prob, type = \"l\", lty = 1, las = 1, \n     xlab = \"Number of cigarettes smoked per day\", \n     ylab = \"Conditional probabiluty of CHD\")\n\n\n###############################################################################\n#\n# Variable (i.e., model) selection without p-values\n#\n# Note: Pr(selecting the \"right\" subset of features | data) = 0\n#\n###############################################################################\n\n\nlr.fit.all &lt;- glm(chd ~ ., family = binomial(link = \"logit\"), data = wcgs)#, maxit = 9999)\n\nWarning message:\n‚Äúglm.fit: algorithm did not converge‚Äù\n\n\n\n# Notice anything?\nsummary(lr.fit.all)\n\n\nCall:\nglm(formula = chd ~ ., family = binomial(link = \"logit\"), data = wcgs)\n\nDeviance Residuals: \n       Min          1Q      Median          3Q         Max  \n-2.409e-06  -2.409e-06  -2.409e-06  -2.409e-06   2.409e-06  \n\nCoefficients: (1 not defined because of singularities)\n                  Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)      2.657e+01  2.228e+05   0.000    1.000\nage             -6.021e-14  1.208e+03   0.000    1.000\nheight          -1.314e-13  3.067e+03   0.000    1.000\nweight           1.866e-14  3.832e+02   0.000    1.000\nsdp              2.085e-14  6.738e+02   0.000    1.000\ndbp             -3.013e-14  1.068e+03   0.000    1.000\nchol            -3.390e-16  1.524e+02   0.000    1.000\nbehaveA2        -2.937e-13  2.413e+04   0.000    1.000\nbehaveB3        -2.583e-14  2.444e+04   0.000    1.000\nbehaveB4         1.605e-13  2.937e+04   0.000    1.000\ncigs            -8.704e-15  4.526e+02   0.000    1.000\ndibepB                  NA         NA      NA       NA\ntypechdinfdeath  3.101e-08  5.876e+04   0.000    1.000\ntypechdnone     -5.313e+01  5.223e+04  -0.001    0.999\ntypechdsilent    2.652e-09  6.574e+04   0.000    1.000\ntimechd          1.661e-15  1.080e+01   0.000    1.000\narcuspresent     5.601e-13  1.429e+04   0.000    1.000\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1.7692e+03  on 3139  degrees of freedom\nResidual deviance: 1.8217e-08  on 3124  degrees of freedom\n  (14 observations deleted due to missingness)\nAIC: 32\n\nNumber of Fisher Scoring iterations: 25\n\n\n\n# What (potentially) happened?\n\n# Let's inspect the data a bit more; we'll start with a SPLOM\ny &lt;- ifelse(wcgs$chd == \"yes\", 1, 0)\npalette(\"Okabe-Ito\")\npairs(wcgs, col = adjustcolor(y + 1, alpha.f = 0.1))\npalette(\"default\")  # back to default color pal\n\n\n# Which columns contain missing values?\nsapply(wcgs, FUN = function(column) mean(is.na(column)))\n\nage0height0weight0sdp0dbp0chol0.00380469245402663behave0cigs0dibep0chd0typechd0timechd0arcus0.000634115409004439\n\n\n\n# Look at correlations between numeric features\nnum &lt;- sapply(wcgs, FUN = is.numeric)  # identify numeric columns\n(corx &lt;- cor(wcgs[, num], use = \"pairwise.complete.obs\"))  # simple correlation matrix\n\n\nA matrix: 8 √ó 8 of type dbl\n\n\n\nage\nheight\nweight\nsdp\ndbp\nchol\ncigs\ntimechd\n\n\n\n\nage\n1.000000000\n-0.095375682\n-0.034404537\n0.16574640\n0.13919776\n0.089188510\n-0.005033852\n-0.070919630\n\n\nheight\n-0.095375682\n1.000000000\n0.532935466\n0.01837357\n0.01027555\n-0.088937779\n0.014911292\n-0.009895169\n\n\nweight\n-0.034404537\n0.532935466\n1.000000000\n0.25324962\n0.29592019\n0.008537442\n-0.081747507\n-0.065350046\n\n\nsdp\n0.165746397\n0.018373573\n0.253249623\n1.00000000\n0.77290641\n0.123061297\n0.029977529\n-0.107884203\n\n\ndbp\n0.139197757\n0.010275549\n0.295920186\n0.77290641\n1.00000000\n0.129597108\n-0.059342317\n-0.110693969\n\n\nchol\n0.089188510\n-0.088937779\n0.008537442\n0.12306130\n0.12959711\n1.000000000\n0.096031834\n-0.095390054\n\n\ncigs\n-0.005033852\n0.014911292\n-0.081747507\n0.02997753\n-0.05934232\n0.096031834\n1.000000000\n-0.093933141\n\n\ntimechd\n-0.070919630\n-0.009895169\n-0.065350046\n-0.10788420\n-0.11069397\n-0.095390054\n-0.093933141\n1.000000000\n\n\n\n\n\n\n# Visualize correlations; can be useful if you have a lot of features\ncorrplot::corrplot(corx, method = \"square\", order = \"FPC\", type = \"lower\", diag = TRUE)\n\n\n\n\n\n\n\n\n\n# What about categorical features?\nxtabs(~ behave + dibep, data = wcgs)  # perfect correlation?\n\n\n# Redunancy analysis (but first remove response and leakage predictors)\nHmisc::redun(~ ., nk = 0, data = subset(wcgs, select = -c(chd, typechd, timechd)))\n\n\n?Hmisc::redun\n\n\n# Refit model without leakage or redundant features\nsummary(lr.fit.all &lt;- glm(chd ~ . - typechd - timechd - dibep, family = binomial(link = \"logit\"), data = wcgs))\n\n\nCall:\nglm(formula = chd ~ . - typechd - timechd - dibep, family = binomial(link = \"logit\"), \n    data = wcgs)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.3653  -0.4362  -0.3128  -0.2208   2.8603  \n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -12.331126   2.350347  -5.247 1.55e-07 ***\nage            0.061812   0.012421   4.977 6.47e-07 ***\nheight         0.006903   0.033335   0.207  0.83594    \nweight         0.008637   0.003892   2.219  0.02647 *  \nsdp            0.018146   0.006435   2.820  0.00481 ** \ndbp           -0.000916   0.010903  -0.084  0.93305    \nchol           0.010726   0.001531   7.006 2.45e-12 ***\nbehaveA2       0.082920   0.222909   0.372  0.70990    \nbehaveB3      -0.618013   0.245032  -2.522  0.01166 *  \nbehaveB4      -0.487224   0.321325  -1.516  0.12944    \ncigs           0.021036   0.004298   4.895 9.84e-07 ***\narcuspresent   0.212796   0.143915   1.479  0.13924    \n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1769.2  on 3139  degrees of freedom\nResidual deviance: 1569.1  on 3128  degrees of freedom\n  (14 observations deleted due to missingness)\nAIC: 1593.1\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n# 1. What about dbp and sbp, aren't they highly correlated? \n# 2. Does multcollinearity seem to be causing issues? \n\n\nplot(sdp ~ dbp, data = wcgs)\n\n\n\n\n\n\n\n\n\n# Check (generalized) variance inflation factors (VIFs)\ncar::vif(lr.fit.all)\n\n\nsummary(lr.fit.all)\n\n\nsummary(lr.fit.all &lt;- update(lr.fit.all, formula = . ~ . - sdp))\n\n\nCall:\nglm(formula = chd ~ age + height + weight + dbp + chol + behave + \n    cigs + arcus, family = binomial(link = \"logit\"), data = wcgs)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.2908  -0.4385  -0.3165  -0.2218   2.9560  \n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -12.041972   2.344264  -5.137 2.79e-07 ***\nage            0.065303   0.012322   5.300 1.16e-07 ***\nheight         0.005524   0.033290   0.166  0.86821    \nweight         0.008804   0.003912   2.250  0.02442 *  \ndbp            0.022216   0.007091   3.133  0.00173 ** \nchol           0.010775   0.001518   7.097 1.28e-12 ***\nbehaveA2       0.133768   0.222417   0.601  0.54755    \nbehaveB3      -0.578278   0.244802  -2.362  0.01817 *  \nbehaveB4      -0.455897   0.321158  -1.420  0.15574    \ncigs           0.022303   0.004261   5.235 1.65e-07 ***\narcuspresent   0.212350   0.143615   1.479  0.13924    \n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1769.2  on 3139  degrees of freedom\nResidual deviance: 1576.8  on 3129  degrees of freedom\n  (14 observations deleted due to missingness)\nAIC: 1598.8\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n# How to measure relative influence/importance of each feature?\n#\n# A (very) crude way in an additive GzLM is to look at the absolute \n# value of the associated test statistics or standardized coefficients\nvip::vi(lr.fit.all)  # see ?vip::vi for details\n\n\n# Leave-one-covariate-out (LOCO) approach; similar to (but better than) permutation importance\n#\n# Note: Would be better to incorporate some form of cross-validation (or bootstrap)\nxnames &lt;- setdiff(names(wcgs), c(\"chd\", \"typechd\", \"timechd\", \"dibep\", \"sdp\"))  # predictors only\nvi.scores &lt;- numeric(length(xnames))\nnames(vi.scores) &lt;- xnames\n(baseline &lt;- deviance(lr.fit.all))  # smaller is better; could also use AUC, Brier score, etc.\nfor (xname in xnames) {\n  data.copy &lt;- subset(wcgs, select = -c(typechd, timechd, dibep))\n  data.copy[[xname]] &lt;- NULL\n  fit.new &lt;- glm(chd ~ ., data = data.copy, family = binomial(link = \"logit\"))\n  vi.scores[xname] &lt;- deviance(fit.new) - baseline  # measure drop in performance\n}\nsort(vi.scores, decreasing = TRUE)\nbarplot(sort(vi.scores, decreasing = TRUE), cex.names = 0.8)\n\n\n# Variable selection using backward elimination with AIC; which variables were dropped?\nsummary(lr.fit.back &lt;- MASS::stepAIC(lr.fit.all, direction = \"backward\", trace = 0))\n\n\nCall:\nglm(formula = chd ~ age + weight + dbp + chol + behave + cigs + \n    arcus, family = binomial(link = \"logit\"), data = wcgs)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.2897  -0.4380  -0.3161  -0.2219   2.9554  \n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -11.691116   1.009395 -11.582  &lt; 2e-16 ***\nage            0.065167   0.012293   5.301 1.15e-07 ***\nweight         0.009166   0.003244   2.826  0.00472 ** \ndbp            0.022019   0.006992   3.149  0.00164 ** \nchol           0.010750   0.001511   7.115 1.12e-12 ***\nbehaveA2       0.134135   0.222408   0.603  0.54644    \nbehaveB3      -0.578433   0.244800  -2.363  0.01813 *  \nbehaveB4      -0.455984   0.321150  -1.420  0.15565    \ncigs           0.022366   0.004243   5.271 1.35e-07 ***\narcuspresent   0.213639   0.143407   1.490  0.13629    \n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1769.2  on 3139  degrees of freedom\nResidual deviance: 1576.9  on 3130  degrees of freedom\n  (14 observations deleted due to missingness)\nAIC: 1596.9\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n# Variable selection using forward selection with AIC; which variables were added?\nm &lt;- glm(chd ~ cigs, data = subset(wcgs, select = -c(typechd, timechd, dibep)), \n         family = binomial(link = \"logit\"))\n(lr.fit.forward &lt;- MASS::stepAIC(m, direction = \"forward\", trace = 0))\n\n\nadd1(m, scope = ~ height + weight)\n\n\n# \n# * glmnet fits elastic net (ENET) model paths for some generalized linear models (GLMs)\n#\n# * Essentially, ENET = ridge regression + LASSO (or L1 penalty + L2 penalty)\n#\n#   - The ridge part helps deal with multicollinearity (more stable coefficients)\n#   - The LASSO part helps with variable selection\n#\n# * See Section 6.2 of ISL book: https://hastie.su.domains/ISLR2/ISLRv2_website.pdf (FREE!!)\n#\n# * From my HOMLR book with Brad: https://bradleyboehmke.github.io/HOML/regularized-regression.html (FREE!!)\n# \n# * Intro videos: https://www.dataschool.io/15-hours-of-expert-machine-learning-videos/\n#\nlibrary(glmnet)\n\n# Fit an elastic net model (i.e., LASSO and ridge penalties) using 5-fold CV\ndim(wcgs.complete &lt;- na.omit(wcgs))\nX &lt;- model.matrix(~. - chd - typechd - timechd - behave - 1 , data = wcgs.complete)\nlr.enet &lt;- cv.glmnet(X, y = ifelse(wcgs.complete$chd == \"yes\", 1, 0), family = \"binomial\", nfold = 5,\n                    keep = TRUE)\nplot(lr.enet)\n\nLoading required package: Matrix\n\nLoaded glmnet 4.1-7\n\n\n\n\n314013\n\n\n\n\n\n\n\n\n\n\n# Compute cross-validated performance measures\nperf &lt;- assess.glmnet(lr.enet$fit.preval, \n                      newy = ifelse(wcgs.complete$chd == \"yes\", 1, 0),\n                      family = \"binomial\")\n\n# Plot results\npar(mfrow = c(2, 1))\n#plot(perf$mse, type = \"b\", ylab = \"MSE (i.e., Brier score)\", col = 2)\nplot(perf$deviance, type = \"b\", ylab = \"Binomial deviance\", col = 3, las = 1)\nplot(perf$auc, type = \"b\", ylab = \"AUROC\", col = 4, las = 1)\n\n\n\n\n\n\n\n\n\n# Inspect coefficients from three fits\ncoef(lr.fit.back)\nround(cbind(coef(lr.enet, s = \"lambda.1se\"), coef(lr.enet, s = \"lambda.min\")), digits = 5)\n\n\n#\n# For kicks, let's try a nonparammetric logistic regression model via the\n# *multivariate adaptive regression splines* (MARS) algorithm; think of MARS\n# as an automatic *generalized linear model* (GLM) that will try to find useful\n# transformations and interaction effects to include.\n#\n# Some sueful resources:\n#\n#   * Mine and Brad's slides from a two-day machine learning workshop for Analytics Connect '18\n#     https://koalaverse.github.io/AnalyticsSummit18/04-MARS.html#1\n#\n#   * earth package vignette: http://www.milbo.org/doc/earth-notes.pdf\n#\nlibrary(earth)\n\n# Fit a degree-2 MARS model (i.e., allow up to 2-way interaction effects)\nlr.mars &lt;- earth(chd ~ . - typechd - timechd - behave - sdp, data = na.omit(wcgs), \n                 degree = 2, glm = list(family = binomial))\n\n# Print summary of model fit\nsummary(lr.mars)\n\nLoading required package: Formula\n\nLoading required package: plotmo\n\nLoading required package: plotrix\n\nLoading required package: TeachingDemos\n\n\n\nCall: earth(formula=chd~.-typechd-timechd-behave-sdp, data=na.omit(wcgs),\n            glm=list(family=binomial), degree=2)\n\nGLM coefficients\n                                    yes\n(Intercept)                 -1.76699612\nh(age-42)                    0.19935137\nh(245-weight)                0.05454156\nh(364-chol)                 -0.00992654\nh(chol-364)                  0.16223325\nh(60-cigs)                  -0.02496502\nh(age-42) * dibepB          -0.06886581\nh(age-42) * h(185-weight)   -0.00156440\nh(age-42) * h(110-dbp)      -0.00285367\nh(226-weight) * h(364-chol) -0.00067032\nh(weight-226) * h(364-chol)  0.00036179\nh(245-weight) * h(294-chol)  0.00045115\nh(308-chol) * h(60-cigs)     0.00001614\nh(chol-308) * h(60-cigs)    -0.00208908\n\nGLM (family binomial, link logit):\n nulldev   df       dev   df   devratio     AIC iters converged\n 1769.17 3139   1550.13 3126      0.124    1578     6         1\n\nEarth selected 14 of 20 terms, and 6 of 8 predictors\nTermination condition: Reached nk 21\nImportance: chol, age, dibepB, dbp, cigs, weight, height-unused, ...\nNumber of terms at each degree of interaction: 1 5 8\nEarth GCV 0.06985911    RSS 214.702    GRSq 0.06433647    RSq 0.08361117\n\n\n\nplot(lr.mars)  # not terribly useful\n\n\nlibrary(vip)\n\n# Variable importance plots\nvip1 &lt;- vip(lr.fit.all, geom = \"point\", include_type = TRUE)\nvip2 &lt;- vip(lr.mars, geom = \"point\", include_type = TRUE)  # see ?vip::vi_model for details\ngridExtra::grid.arrange(vip1, vip2, nrow = 1)\n\n\n# Look at a PD plot of age\npd1 &lt;- partial(lr.mars, pred.var = \"cigs\", plot = TRUE, plot.engine = \"ggplot2\", rug = TRUE) + ggtitle(\"Logit scale\")\npd2 &lt;- partial(lr.mars, pred.var = \"cigs\", plot = TRUE, prob = TRUE, plot.engine = \"ggplot2\", rug = TRUE) + ggtitle(\"Probability scale\")\ngridExtra::grid.arrange(pd1, pd2, nrow = 1)\n\n\nhist(wcgs$chol, 50)\n\n\n###############################################################################\n#\n# Assessing performance\n#\n###############################################################################\n\n\n# Confusion matrix (i.e., 2x2 contingency table of classification results)\ny &lt;- na.omit(wcgs)$chd  # observed classes\nprob &lt;- predict(lr.fit.back, type = \"response\")  # predicted probabilities\nclasses &lt;- ifelse(prob &gt; 0.5, \"yes\", \"no\")  # classification based on 0.5 threshold\n(cm &lt;- table(\"actual\" = y, \"predicted\" = classes))  # confusion matrix\n\n      predicted\nactual   no  yes\n   no  2880    5\n   yes  253    2\n\n\n\n# Compute sensitivity and specificity\ntp &lt;- sum(classes == \"yes\" & y == \"yes\")  # true positives\ntn &lt;- sum(classes == \"no\"  & y == \"no\")  # true negatives\nfp &lt;- sum(classes == \"yes\" & y == \"no\")  # false positives\nfn &lt;- sum(classes == \"no\"  & y == \"yes\")  # false negatives\n(tpr &lt;- tp / (tp + fn))  # sensitivity\n(tnr &lt;- tn / (tn + fp))  # specificity\n\n0.00784313725490196\n\n\n0.998266897746967\n\n\n\n# But what do these numbers truly represent, and are they really that meaningful?\n\n# Notes:\n#\n# Sensitivity = TPR = TP / (TP + FN) = Pr(model predicts CHD | subject has CHD)\n#\n# Specificity = TNR = TN / (TN + FP) = Pr(model predicts no CHD | subject does not have CHD)\n#\n# Don't these seem backward? E.g., Pr(known | unknown) vs. Pr(unknown | known). In the context of\n# this problem, for example, if you were the patient, which of the two probabilities seems more relevant:\n#\n#   Pr(model predicts you have CHD | you have CHD)   OR   Pr(you have CHD | model predicts you have CHD)\n#\n# An ROC curve is nothing more than a plot of TPR vs. 1-TNR for a range of thresholds (not just 0.5)\n#\n# Useful resources (would be wise to read these): \n#\n#   * https://twitter.com/f2harrell/status/1101595033382326273\n#   * https://www.fharrell.com/post/mlconfusion/\n\n\nthreshold &lt;- seq(from = 0, to = 1, length = 999)\ntp &lt;- tn &lt;- fp &lt;- fn &lt;- numeric(length(threshold))\nfor (i in seq_len(length(threshold))) {\n  classes &lt;- ifelse(prob &gt; threshold[i], \"yes\", \"no\")\n  tp[i] &lt;- sum(classes == \"yes\" & y == \"yes\")  # true positives\n  tn[i] &lt;- sum(classes == \"no\"  & y == \"no\")  # true negatives\n  fp[i] &lt;- sum(classes == \"yes\" & y == \"no\")  # false positives\n  fn[i] &lt;- sum(classes == \"no\"  & y == \"yes\")  # false negatives\n}\ntpr &lt;- tp / (tp + fn)  # sensitivity\ntnr &lt;- tn / (tn + fp)  # specificity\n\n# Plot ROC curve\nplot(tnr, y = tpr, type = \"l\", col = 2, lwd = 2, xlab = \"TNR (or specificity)\", \n     ylab = \"TPR (or sensitivity)\")\nabline(1, -1, lty = 2)\n\n\n\n\n\n\n\n\n\n# Can be useful to use a package sometimes (e.g., for computing are under the ROC curve; AKA AUROC or AUC)\nplot(roc1 &lt;- pROC::roc(y, predictor = prob))\nroc1\n\n# Compare to ROC curve from MARS fit\nprob2 &lt;- predict(lr.mars, newdata = na.omit(wcgs), type = \"response\")[, \"yes\"]\nlines(roc2 &lt;- pROC::roc(y, predictor = prob2), col = 2)\nroc2\n\n\n#\n# A calibration curve and lift chart are often more useful\n#\n\n\n# Function to compute lift and cumulative gain charts\nlift &lt;- function(prob, y, pos.class = NULL, cumulative = TRUE) {\n  if (!all(sort(unique(y)) == c(0, 1))) {\n    if (is.null(pos.class)) {\n      stop(\"A value for `pos.class` is required whenever `y` is not a 0/1 \",\n           \"outcome.\", call. = FALSE)\n    }\n    y &lt;- ifelse(y == pos.class, 1, 0)\n  }\n  ord &lt;- order(prob, decreasing = TRUE)\n  prob &lt;- prob[ord]\n  y &lt;- y[ord]\n  prop &lt;- seq_along(y) / length(y)\n  lift &lt;- if (isTRUE(cumulative)) {\n    cumsum(y)\n  } else {\n    (cumsum(y) / seq_along(y)) / mean(y)\n  }\n  structure(list(\"lift\" = lift, \"prop\" = prop, \"cumulative\" = cumulative,\n                 \"y\" = y), class = \"lift\")\n}\n\n\n# Cumulative gain chart; what does this plot tell us?\nl &lt;- lift(prob, y = y, pos.class = \"yes\")\nplot(l[[\"prop\"]], l[[\"lift\"]], type = \"l\", \n     xlab = \"Proportion of sample\", ylab = \"Cumulative lift\", \n     las = 1, lwd = 2, col = 2)\nabline(0, sum(wcgs$chd == \"yes\"), lty = 2)\n\n# Compare to MARS fit\nl2 &lt;- lift(prob2, y = y, pos.class = \"yes\")\nlines(l2[[\"prop\"]], l2[[\"lift\"]], col = 3)\n\n\n# Function to compute the Brier score; the Brier score is an example of a \n# *proper scoring rule* and is a better performance metric than AUROC when \n# probabilities are of interest (which they usually are)\nbrier.score &lt;- function(y, prob) {  # this is a relative measure, like log loss/binomial deviance, SSE/RMSE, AIC/BIC, etc.\n  mean((prob - y) ^ 2)  # y should be in {0, 1}\n}\n\n# Compute Brier score for previous models\ny01 &lt;- ifelse(y == \"yes\", 1, 0)\nfit0 &lt;- glm(chd ~ 1, data = na.omit(wcgs), family = binomial(link = \"logit\"))\nbrier.score(y01, prob = predict(fit0, type = \"response\"))\nbrier.score(y01, prob = predict(lr.fit.back, type = \"response\"))\nbrier.score(y01, prob = predict(lr.enet, newx = X, type = \"response\", s = \"lambda.min\")[, 1L, drop = TRUE])\n\n\n# Same as Brier score for NULL (i.e., intercept-only) model\n(p &lt;- mean(wcgs$chd == \"yes\"))  # observed proportion of CHD in data\nmean((p - y01) ^ 2)  # baseline Brier score for comparison\n\n0.0814838300570704\n\n\n0.0746151708253856\n\n\n\n# Let's also look at a calibration curve; what does this plot tell us?\n#\n# Defn: A predictive model is said to be well-calibrated if its predictions \n# match the observed distributions in the data; that is, that a fraction of \n# about p of the events we predict with probability p actually occur.\n#\n# Calibration plots should become standard practice for all probability models\n# (looking specifically at you, Machine Learning!)\n#\n# Some useful resources on calibration:\n#\n# * https://scikit-learn.org/stable/modules/calibration.html\n# * https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf\n# * https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-11/issue-2/Beyond-sigmoids--How-to-obtain-well-calibrated-probabilities-from/10.1214/17-EJS1338SI.full\n# * Pretty much anything Frank Harrell says about calibration... try googling \"Frank Harrell calibration\"\n#\n# Fun fact: I did my dissertation on calibration in regression (related, but entirely different application)\n# Topics in Statistical Calibration: https://apps.dtic.mil/sti/pdfs/ADA598921.pdf\nrms::val.prob(prob, y = y01)\n\nDxy0.506609576239508C (ROC)0.753304788119754R20.137916289901616D0.0609263138582237D:Chi-sq192.308625514822D:p&lt;NA&gt;U-0.00063694267515967U:Chi-sq-1.36424205265939e-12U:p1Q0.0615632565333833Brier0.0698210551064829Intercept-2.44192054806228e-15Slope1.00000000000002Emax0.339521205566152E900.0187367891577582Eavg0.00855844779669346S:z0.249599681406934S:p0.802896944990776\n\n\n\n\n\n\n\n\n\n\n# Check calibration of MARS model\nrms::val.prob(prob2, y = y01)\n\n\n# Check calibration of ENET model\nprob3 &lt;- predict(lr.enet, newx = X, type = \"response\", s = \"lambda.min\")[, 1L, drop = TRUE]\nrms::val.prob(prob3, y = y01)\n\n\n# Role your own solution using *regression splines*\ntemp &lt;- data.frame(\"y\" = y, \"p\" = prob)\ncal &lt;- glm(y ~ splines::ns(p, df = 6), data = temp,\n           family = binomial(link = \"logit\"))\n\n# Plot the results\nxx &lt;- seq(min(prob), max(prob), length = 200)\nyy &lt;- predict(cal, newdata = data.frame(p = xx), type = \"response\")\nxx &lt;- xx[!is.na(yy)]\nyy &lt;- yy[!is.na(yy)]\nplot(xx, yy, xlim = c(0, 1), ylim = c(0, 1), type = \"l\", col = 2,\n     xlab = \"Predicted probability\",\n     ylab = \"Calibrated probability\")\nabline(0, 1, lty = 2)\nrug(prob, col = adjustcolor(1, alpha.f = 0.2))\n\n\n# General conclusion(s):\n#\n# * A simpe additive logistic regression model (i.e., no fancy transformations or interaction effects)\n#   seems comparable (in terms of calibration and performance) to more complex logistic regresison models."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BANA 7042: Statistical Modeling",
    "section": "",
    "text": "Welcome to the course website for BANA 7042: Statistical Modeling."
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "BANA 7042: Statistical Modeling",
    "section": "Quick Links",
    "text": "Quick Links\n\nSyllabus"
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "BANA 7042: Statistical Modeling",
    "section": "Instructor",
    "text": "Instructor\nBrandon Greenwell"
  },
  {
    "objectID": "index.html#announcements",
    "href": "index.html#announcements",
    "title": "BANA 7042: Statistical Modeling",
    "section": "Announcements",
    "text": "Announcements\n\nWelcome to the class!"
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#western-collaborative-group-study",
    "href": "slides/02-logistic-regression-1.html#western-collaborative-group-study",
    "title": "Logistic Regression (Part I)",
    "section": "Western collaborative group study",
    "text": "Western collaborative group study\n\\(N = 3154\\) healthy young men aged 39‚Äì59 from the San Francisco area were assessed for their personality type. All were free from coronary heart disease at the start of the research. Eight and a half years later change in this situation was recorded. See ?faraway::wcgs in R.\n\n\nShow R code\n# install.packages(\"faraway\")\nhead(wcgs &lt;- faraway::wcgs)",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#structure-of-wcgs-data",
    "href": "slides/02-logistic-regression-1.html#structure-of-wcgs-data",
    "title": "Logistic Regression (Part I)",
    "section": "Structure of wcgs data",
    "text": "Structure of wcgs data\n\n\nShow R code\nstr(wcgs)\n\n\n'data.frame':   3154 obs. of  13 variables:\n $ age    : int  49 42 42 41 59 44 44 40 43 42 ...\n $ height : int  73 70 69 68 70 72 72 71 72 70 ...\n $ weight : int  150 160 160 152 150 204 164 150 190 175 ...\n $ sdp    : int  110 154 110 124 144 150 130 138 146 132 ...\n $ dbp    : int  76 84 78 78 86 90 84 60 76 90 ...\n $ chol   : int  225 177 181 132 255 182 155 140 149 325 ...\n $ behave : Factor w/ 4 levels \"A1\",\"A2\",\"B3\",..: 2 2 3 4 3 4 4 2 3 2 ...\n $ cigs   : int  25 20 0 20 20 0 0 0 25 0 ...\n $ dibep  : Factor w/ 2 levels \"A\",\"B\": 1 1 2 2 2 2 2 1 2 1 ...\n $ chd    : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 2 1 1 1 1 1 ...\n $ typechd: Factor w/ 4 levels \"angina\",\"infdeath\",..: 3 3 3 3 2 3 3 3 3 3 ...\n $ timechd: int  1664 3071 3071 3064 1885 3102 3074 3071 3064 1032 ...\n $ arcus  : Factor w/ 2 levels \"absent\",\"present\": 1 2 1 1 2 1 1 1 1 2 ...",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#description-of-each-variable",
    "href": "slides/02-logistic-regression-1.html#description-of-each-variable",
    "title": "Logistic Regression (Part I)",
    "section": "Description of each variable",
    "text": "Description of each variable\n\n\nShow R code\n?wcgs\n\n\n\nage: age in years\nheight: height in inches\nweight: weight in pounds\nsdp: systolic blood pressure in mm Hg\ndbp: diastolic blood pressure in mm Hg\nchol: fasting serum cholesterol in mm %\nbehave: behavior type which is a factor with levels A1 A2 B3 B4\ncigs: number of cigarettes smoked per day\ndibep: behavior type a factor with levels A (Agressive) B (Passive)\nchd: coronary heat disease developed is a factor with levels no yes\ntypechd: type of coronary heart disease is a factor with levels angina infdeath none silent\ntimechd: time of CHD event or end of follow-up\narcus: arcus senilis is a factor with levels absent present",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#for-now-well-focus-on-3-variables",
    "href": "slides/02-logistic-regression-1.html#for-now-well-focus-on-3-variables",
    "title": "Logistic Regression (Part I)",
    "section": "For now, we‚Äôll focus on 3 variables",
    "text": "For now, we‚Äôll focus on 3 variables\n\n\nShow R code\nsummary(wcgs[, c(\"chd\", \"height\", \"cigs\")])\n\n\n  chd           height           cigs     \n no :2897   Min.   :60.00   Min.   : 0.0  \n yes: 257   1st Qu.:68.00   1st Qu.: 0.0  \n            Median :70.00   Median : 0.0  \n            Mean   :69.78   Mean   :11.6  \n            3rd Qu.:72.00   3rd Qu.:20.0  \n            Max.   :78.00   Max.   :99.0  \n\n\n\n Anything interesting stick out?",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data",
    "text": "Visualizing discrete data\nPie chart of response ü§Æ\n\n\nShow R code\n# Construct a pie chart of the (binary) response; I'm not a fan of pie charts in general\nptab &lt;- prop.table(table(wcgs$chd))  # convert frequencies to proportions\nptab  # inspect output\n\n\n\n        no        yes \n0.91851617 0.08148383 \n\n\nShow R code\npie(prop.table(table(wcgs$chd)),\n    main = \"Pie chart of Coronary Heart Disease\")",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data-1",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data-1",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data",
    "text": "Visualizing discrete data\nBar charts tend to be more effective\n\n\nShow R code\nbarplot(ptab, las = 1, col = \"forestgreen\")",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data-2",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data-2",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data",
    "text": "Visualizing discrete data\nMosaic plot showing relationship between cigs and chd; not incredibly useful IMO unless both variables are categorical\n\n\nShow R code\nplot(chd ~ cigs, data = wcgs)",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data-3",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data-3",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data",
    "text": "Visualizing discrete data\nNonparametric density plot of height by chd status using lattice graphics:\n\n\nShow R code\nlibrary(lattice)\n\ndensityplot(~ height, groups = chd, data = wcgs, auto.key = TRUE)",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data-4",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data-4",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data",
    "text": "Visualizing discrete data\nBoxplot of cigs vs.¬†chd status\n\n\nShow R code\nplot(cigs ~ chd, data = wcgs, col = c(2, 3))",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data-5",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data-5",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data",
    "text": "Visualizing discrete data\nBoxplot of height vs.¬†chd status with notches\n\n\nShow R code\nplot(height ~ chd, data = wcgs, col = c(2, 3), notch = TRUE)",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data-detour",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data-detour",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data (detour)",
    "text": "Visualizing discrete data (detour)\n\nDecision trees üå≤üå¥üå≥ are immensely useful for exploring new data sets!\nSome useful resources:\n\nSee the documentation for R packages rpart, party, and partykit\nFifty Years of Classification and Regression Trees\nRecursive Partitioning and Applications (logistic regression and trees)\nThe ultimate tree book üòé",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data-6",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data-6",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data",
    "text": "Visualizing discrete data\nStandard CART-like decision tree using rpart and rpart.plot (all variables allowed):\n\n\nShow R code\nrpart.plot::rpart.plot(rpart::rpart(chd ~ ., data = wcgs))",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#visualizing-discrete-data-7",
    "href": "slides/02-logistic-regression-1.html#visualizing-discrete-data-7",
    "title": "Logistic Regression (Part I)",
    "section": "Visualizing discrete data",
    "text": "Visualizing discrete data\nConditional inference tree using partykit (using only variables of interest):\n\n\nShow R code\nplot(partykit::ctree(chd ~ height + cigs, data = wcgs))",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#observations-so-far",
    "href": "slides/02-logistic-regression-1.html#observations-so-far",
    "title": "Logistic Regression (Part I)",
    "section": "Observations so far‚Ä¶",
    "text": "Observations so far‚Ä¶\n\nIt seems that the cigs is positively associated with the binary response chd\nIt is not clear how, if at all, height is associated with chd\nQuestion: how can we build a model to examine these potential associations?",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#linear-models",
    "href": "slides/02-logistic-regression-1.html#linear-models",
    "title": "Logistic Regression (Part I)",
    "section": "Linear models",
    "text": "Linear models\nRecall that in linear regression we model the conditional mean response as a linear function in some fixed, but known parameters \\(\\boldsymbol{\\beta}\\):\n\\[\nE\\left(Y|\\boldsymbol{x}\\right) = \\beta_0 + \\beta_1x_1 + \\dots \\beta_px_p = \\boldsymbol{\\beta}^\\top\\boldsymbol{x}\n\\]\n\nIf \\(Y\\) is a binary random variable, then what is \\(E\\left(Y|\\boldsymbol{x}\\right)\\)?",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#the-linear-probability-lp-model",
    "href": "slides/02-logistic-regression-1.html#the-linear-probability-lp-model",
    "title": "Logistic Regression (Part I)",
    "section": "The linear probability (LP) model",
    "text": "The linear probability (LP) model\nIt turns out that \\(E\\left(Y|\\boldsymbol{x}\\right) = P\\left(Y = 1|\\boldsymbol{x}\\right)\\). The LP model assumes that \\[\nP\\left(Y = 1|\\boldsymbol{x}\\right) = \\beta_0 + \\beta_1x_1 + \\dots \\beta_px_p = \\boldsymbol{\\beta}^\\top\\boldsymbol{x}\n\\]\n\nIs this reasonable?",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#the-lp-model-chd-vs.-cigs",
    "href": "slides/02-logistic-regression-1.html#the-lp-model-chd-vs.-cigs",
    "title": "Logistic Regression (Part I)",
    "section": "The LP model: chd vs.¬†cigs",
    "text": "The LP model: chd vs.¬†cigs\n\n\nShow R code\ny &lt;- ifelse(wcgs$chd == \"yes\", 1, 0)\nsummary(fit &lt;- lm(y ~ cigs, data = wcgs))\n\n\n\nCall:\nlm(formula = y ~ cigs, data = wcgs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.25268 -0.09794 -0.05876 -0.05876  0.94124 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.0587608  0.0062041   9.471  &lt; 2e-16 ***\ncigs        0.0019588  0.0003339   5.867 4.91e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2722 on 3152 degrees of freedom\nMultiple R-squared:  0.0108,    Adjusted R-squared:  0.01049 \nF-statistic: 34.42 on 1 and 3152 DF,  p-value: 4.91e-09\n\n\nAre the standard errors here appropriate? Why/why not?",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#the-lp-model-chd-vs.-cigs-1",
    "href": "slides/02-logistic-regression-1.html#the-lp-model-chd-vs.-cigs-1",
    "title": "Logistic Regression (Part I)",
    "section": "The LP model: chd vs.¬†cigs",
    "text": "The LP model: chd vs.¬†cigs\n\n\nShow R code\nplot(y ~ cigs, data = wcgs, ylim = c(0, 1), las = 1)\nabline(fit, col = 2)",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#the-logistic-regression-lr-model",
    "href": "slides/02-logistic-regression-1.html#the-logistic-regression-lr-model",
    "title": "Logistic Regression (Part I)",
    "section": "The logistic regression (LR) model",
    "text": "The logistic regression (LR) model\nAssume \\(Y \\sim \\mathrm{Bernoulli}\\left(p\\right)\\), where \\[\np = p\\left(\\boldsymbol{x}\\right) = P\\left(Y = 1|\\boldsymbol{x}\\right) = E\\left(Y|\\boldsymbol{x}\\right)\n\\] and \\[\n\\mathrm{logit}\\left(p\\right) = \\log\\left(\\frac{p}{1-p}\\right) = \\boldsymbol{\\beta}^\\top\\boldsymbol{x}\n\\] In other words, LR models the logit of the mean response as a linear function in \\(\\boldsymbol{\\beta}\\); we‚Äôll refer to the term \\(\\eta = \\boldsymbol{\\beta}^\\top\\boldsymbol{x}\\) as the linear predictor. Why does this make more sense?",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#the-logistic-regression-lr-model-1",
    "href": "slides/02-logistic-regression-1.html#the-logistic-regression-lr-model-1",
    "title": "Logistic Regression (Part I)",
    "section": "The logistic regression (LR) model",
    "text": "The logistic regression (LR) model\nCan always solve for \\(p\\) to get predictions on the raw probability scale (Homework 2 üòÑ): \\[\np\\left(\\boldsymbol{x}\\right) = \\frac{\\exp\\left(\\boldsymbol{\\beta}^\\top\\boldsymbol{x}\\right)}{1 + \\exp\\left(\\boldsymbol{\\beta}^\\top\\boldsymbol{x}\\right)}\n\\]\n\nNote how the LR model is nonlinear in \\(p\\)!",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#assumptions-of-the-lr-model",
    "href": "slides/02-logistic-regression-1.html#assumptions-of-the-lr-model",
    "title": "Logistic Regression (Part I)",
    "section": "Assumptions of the LR model",
    "text": "Assumptions of the LR model\n\nBinary response: The response variable \\(Y\\) must be binary (e.g., 0/1, yes/no, etc.)\nIndependence: The observations must be independent of one another (i.e., no repeated measures)\nLinearity: There must be a linear relationship between the predictors and the logit of the response\nNo multicollinearity: The predictors should not be highly correlated with one another\nSample size: Since LR uses MLE, it typically requires a larger sample size than ordinary linear regression for the results to be reliable (asymptotic properties)",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r",
    "href": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r",
    "title": "Logistic Regression (Part I)",
    "section": "Fitting an LR model in R",
    "text": "Fitting an LR model in R\n\nUse the glm() function instead of lm()\nGLM stands for generalized linear model, which includes the LR and ordinary linear regression models as special cases\nMany (but not all) of the models we‚Äôll discuss in throughout this course belong to the class of GLMs\nNote how we have to specifcy the family argument! (see ?glm for details)\nThe response can be a 0/1 indicator or a factor variable (be careful with interpretation and which class is used as the baseline):\n\n\n\nShow R code\nlevels(wcgs$chd)\n\n\n[1] \"no\"  \"yes\"",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r-1",
    "href": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r-1",
    "title": "Logistic Regression (Part I)",
    "section": "Fitting an LR model in R",
    "text": "Fitting an LR model in R\n\n\nShow R code\nsummary(fit.lr &lt;- glm(chd ~ cigs, data = wcgs, family = binomial))\n\n\n\nCall:\nglm(formula = chd ~ cigs, family = binomial, data = wcgs)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.742160   0.092111 -29.770  &lt; 2e-16 ***\ncigs         0.023220   0.004042   5.744 9.22e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1781.2  on 3153  degrees of freedom\nResidual deviance: 1750.0  on 3152  degrees of freedom\nAIC: 1754\n\nNumber of Fisher Scoring iterations: 5",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r-2",
    "href": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r-2",
    "title": "Logistic Regression (Part I)",
    "section": "Fitting an LR model in R",
    "text": "Fitting an LR model in R\nLet‚Äôs compare the fitted LR and LP models:\n\n\nShow R code\nprob &lt;- predict(fit.lr, newdata = data.frame(cigs = 0:99), type = \"response\")\nplot(y ~ cigs, data = wcgs, ylab = \"chd\", las = 1, xlim = c(0, 99))\nlines(0:99, y = prob, col = 2)\nabline(fit, col = 3, lty = 2)\nlegend(\"topright\", legend = c(\"LR fit\", \"LP fit\"), lty = c(1, 2), col = c(2, 3))",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r-3",
    "href": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r-3",
    "title": "Logistic Regression (Part I)",
    "section": "Fitting an LR model in R",
    "text": "Fitting an LR model in R\nLet‚Äôs compare the fitted LR and LP models:\n\n\nShow R code\nprob &lt;- predict(fit.lr, newdata = data.frame(cigs = 0:999), type = \"response\")\nplot(y ~ cigs, data = wcgs, ylab = \"chd\", las = 1, xlim = c(0, 999))\nlines(0:999, y = prob, col = 2)\nabline(fit, col = 3, lty = 2)\nlegend(\"topright\", legend = c(\"LR fit\", \"LP fit\"), lty = c(1, 2), col = c(2, 3))",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r-4",
    "href": "slides/02-logistic-regression-1.html#fitting-an-lr-model-in-r-4",
    "title": "Logistic Regression (Part I)",
    "section": "Fitting an LR model in R",
    "text": "Fitting an LR model in R\nNow let‚Äôs include an additional predictor (i.e., height):\n\n\nShow R code\nsummary(fit.lr2 &lt;- glm(chd ~ cigs + height, data = wcgs, family = binomial))\n\n\n\nCall:\nglm(formula = chd ~ cigs + height, family = binomial, data = wcgs)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -4.50161    1.84186  -2.444   0.0145 *  \ncigs         0.02313    0.00404   5.724 1.04e-08 ***\nheight       0.02521    0.02633   0.957   0.3383    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1781.2  on 3153  degrees of freedom\nResidual deviance: 1749.0  on 3151  degrees of freedom\nAIC: 1755\n\nNumber of Fisher Scoring iterations: 5",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#interpreting-lr-coefficients",
    "href": "slides/02-logistic-regression-1.html#interpreting-lr-coefficients",
    "title": "Logistic Regression (Part I)",
    "section": "Interpreting LR coefficients",
    "text": "Interpreting LR coefficients\n\nLet \\(p = P\\left(Y = 1\\right)\\) and \\(1 - p = P\\left(Y = 0\\right)\\)\nThe odds of \\(Y = 1\\) occuring is defined as \\(p / \\left(1 - p\\right)\\)\nFor a fair coin ü™ô, the probability of getting tails is \\(p = 0.5\\). Therefore, the odds of getting tails vs.¬†heads is \\(p / (1 - p) = 0.5 / 0.5 = 1\\). (We might also say the odds of getting tails is ‚Äú1 to 1‚Äù or ‚Äú1:1‚Äù).\n\n\n\n\nFor a fair die üé≤, what are the odds of rolling a 2?",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#interpreting-lr-coefficients-1",
    "href": "slides/02-logistic-regression-1.html#interpreting-lr-coefficients-1",
    "title": "Logistic Regression (Part I)",
    "section": "Interpreting LR coefficients",
    "text": "Interpreting LR coefficients\nThe logit models the log odds of success (i.e., \\(Y = 1|\\boldsymbol{x}\\)) \\[\n\\log\\left(\\mathrm{odds}\\right) = \\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots \\beta_px_p\n\\] Exponentiating both, we get \\[\n\\mathrm{odds} = \\frac{p}{1-p} = \\exp{\\left(\\beta_0\\right)}\\times\\exp{\\left(\\beta_1x_1\\right)}\\times\\exp{\\left(\\beta_2x_2\\right)}\\times\\dots\\times\\exp{\\left(\\beta_px_p\\right)}\n\\]\n\nIn the LR model, \\(\\beta_i\\) represents the change in the log odds when \\(x_i\\) increases by one unit (all else held constant)\nIn the LR model, \\(\\exp\\left(\\beta_i\\right)\\) represents the multiplicative increase in the odds when \\(x_i\\) increases by one unit (all else held constant)\nCANNOT interpret the coefficients in terms of \\(p\\) directly‚Ä¶effect plots to the rescue üõü!!",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#wcgs-study",
    "href": "slides/02-logistic-regression-1.html#wcgs-study",
    "title": "Logistic Regression (Part I)",
    "section": "WCGS study",
    "text": "WCGS study\n\n\nShow R code\ncoef(fit.lr2)\n\n\n(Intercept)        cigs      height \n-4.50161397  0.02312740  0.02520779 \n\n\n\n\nHolding height constant, for every additional cigarette smoked per day the predicted log odds of developing chd increases by 0.023\nHolding height constant, for every additional cigarette smoked per day the predicted odds of developing chd increases multiplicatively by 1.023",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#effect-plots",
    "href": "slides/02-logistic-regression-1.html#effect-plots",
    "title": "Logistic Regression (Part I)",
    "section": "Effect plots",
    "text": "Effect plots\nLots of different methods and packages:\n\nMarginal effects via effects library\nPartial dependence (PD) plots and individual conditional expectation (ICE) plots via the pdp package\nMarginal effect and PD plots via the plotmo library\nAnd many, many more (see R‚Äôs ML Task View)\nAll have their own assumptions and drawbacks; typically, similar in shape when the model is additive in nature (i.e., no interaction effects)",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#effect-plots-1",
    "href": "slides/02-logistic-regression-1.html#effect-plots-1",
    "title": "Logistic Regression (Part I)",
    "section": "Effect plots",
    "text": "Effect plots\nThe plotmo library is an ‚Äúeasy button‚Äù for quick and dirty effect plots (other variables are held fixed at their median or most frequent value) and supports a wide range of models\n\n\nShow R code\nplotmo::plotmo(fit.lr2)\n\n\n plotmo grid:    cigs height\n                    0     70",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#effect-plots-2",
    "href": "slides/02-logistic-regression-1.html#effect-plots-2",
    "title": "Logistic Regression (Part I)",
    "section": "Effect plots",
    "text": "Effect plots\nI generally prefer PD plots; see Greenwell (2017) for details\n\n\nShow R code\nlibrary(pdp)\nlibrary(ggplot2)\n\ntheme_set(theme_bw())\npartial(fit.lr2, pred.var = \"cigs\", prob = TRUE, plot = TRUE, \n        plot.engine = \"ggplot2\", rug = TRUE) + \n    ylim(0, 1) + \n    ylab(\"Probability\")",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#effect-plots-3",
    "href": "slides/02-logistic-regression-1.html#effect-plots-3",
    "title": "Logistic Regression (Part I)",
    "section": "Effect plots",
    "text": "Effect plots\nCan easily extend these methods to two or three variables:\n\n\nShow R code\npd.2d &lt;- partial(fit.lr2, pred.var = c(\"cigs\", \"height\"), chull = TRUE)\nplotPartial(pd.2d)",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#effect-plots-4",
    "href": "slides/02-logistic-regression-1.html#effect-plots-4",
    "title": "Logistic Regression (Part I)",
    "section": "Effect plots",
    "text": "Effect plots\nThree-dimensional plots look cool, but generally aren‚Äôt all that useful:\n\n\nShow R code\nlattice::wireframe(yhat ~ height * cigs, data = pd.2d, shade = TRUE)",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#ml-estimation-bernoulli",
    "href": "slides/02-logistic-regression-1.html#ml-estimation-bernoulli",
    "title": "Logistic Regression (Part I)",
    "section": "ML estimation (Bernoulli)",
    "text": "ML estimation (Bernoulli)\n\nIn the linear model, LS and ML estimation are equivalent and rather straightforward\nIn LR, ML estimation is more common1\nRecall that if \\(Y_i \\stackrel{iid}{\\sim} Bernoulli\\left(p\\right)\\), then the likelihood function is defined as\n\n\\[\nL\\left(p\\right) = \\prod_{i=1}^n p^{y_i} \\left(1-p\\right)^{1-y_i}\n\\]\nTechnically, there are a number of ways to estimate the parameters of an LR model, but ML estimation is most common.",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#ml-estimation-bernoulli-1",
    "href": "slides/02-logistic-regression-1.html#ml-estimation-bernoulli-1",
    "title": "Logistic Regression (Part I)",
    "section": "ML estimation (Bernoulli)",
    "text": "ML estimation (Bernoulli)\n\nTheoretically, goal is to maximize \\(L\\left(p\\right)\\)\nIn practice, it‚Äôs easier to work with the log likelihood \\(l\\left(p\\right) = log\\left[L\\left(p\\right)\\right]\\)\n\n\\[\n\\begin{align}\nl\\left(p\\right) &= \\log\\left[\\prod_{i=1}^n p^{y_i} \\left(1-p\\right)^{1-y_i}\\right]\\\\\n&= \\cdots\\\\\n&= \\log\\left(p\\right)\\sum_{i=1}^ny_i + \\log\\left(1-p\\right)\\sum_{i=1}^n\\left(n-y_i\\right)\n\\end{align}\n\\]",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#ml-estimation-lr",
    "href": "slides/02-logistic-regression-1.html#ml-estimation-lr",
    "title": "Logistic Regression (Part I)",
    "section": "ML estimation (LR)",
    "text": "ML estimation (LR)\n\nIn LR, \\[\np_i = p\\left(\\boldsymbol{x}_i\\right) = \\frac{\\exp\\left(\\boldsymbol{\\beta}^\\top\\boldsymbol{x}_i\\right)}{1 + \\exp\\left(\\boldsymbol{\\beta}^\\top\\boldsymbol{x}_i\\right)}\n\\]\nThis becomes a more complicated optimization problem!\nEquivalent to minimizing log loss in machine learning\nLog loss is a proper scoring rule",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#ml-estimation-lr-1",
    "href": "slides/02-logistic-regression-1.html#ml-estimation-lr-1",
    "title": "Logistic Regression (Part I)",
    "section": "ML estimation (LR)",
    "text": "ML estimation (LR)\nMaximizing the log-likelihood is equivalent to minimizing the negative log-likelihood (which is more convenient):\n\n\nShow R code\n# Response (as a binary 0/1 variable)\ny &lt;- ifelse(wcgs$chd == \"yes\", 1, 0)\n\n# Model matrix; includes a column for the intercept by default\nX &lt;- model.matrix(~ cigs + height, data = wcgs)\n\n# Function to compute the negative log-likelihood (as a function of the betas)\nnll &lt;- function(beta) {\n  lp &lt;- X %*% beta  # linear predictor; same as b0 + b1*x1 + b2*x2 + ...\n  -sum(y * lp - log(1 + exp(lp)))\n}\n\n# Use general optimization; would be better to use gradient and hessian info (e.g., first and second derivative info)\nlp.fit &lt;- lm(y ~ cigs + height, data = wcgs)\noptim(coef(lp.fit), fn = nll, \n      control = list(\"maxit\" = 9999, \"reltol\" = 1e-20))\n\n\n$par\n(Intercept)        cigs      height \n-4.50161158  0.02312743  0.02520775 \n\n$value\n[1] 874.5246\n\n$counts\nfunction gradient \n     658       NA \n\n$convergence\n[1] 0\n\n$message\nNULL",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#wald-statistic",
    "href": "slides/02-logistic-regression-1.html#wald-statistic",
    "title": "Logistic Regression (Part I)",
    "section": "Wald statistic",
    "text": "Wald statistic\n\nConsider the usual marginal test: \\(H_0: \\beta_j = 0\\) vs.¬†\\(H_1: \\beta_j \\ne 0\\)\nAsymptotically speaking, \\(Z_i = \\hat{\\beta}_i / \\mathrm{SE}\\left(\\hat{\\beta}_i\\right)\\) has a standard normal distribution under \\(H_0\\)\nThis leads to the usual Wald-based confidence intervals, etc.\nBetter (but more complicated) approaches available, like likelihood ratio tests, profile likelihood methods, and the bootstrap.",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#extending-the-general-linear-f-test",
    "href": "slides/02-logistic-regression-1.html#extending-the-general-linear-f-test",
    "title": "Logistic Regression (Part I)",
    "section": "Extending the general linear F-test",
    "text": "Extending the general linear F-test\nIn LR, we move from a general linear F-test to a likelihood ratio test based on the \\(\\chi^2\\) distribution: \\[\nX = -2\\log\\left(\\frac{L_{H_0}}{L_{H_1}}\\right) = D_{H_0} - D_{H_1} \\sim \\chi^2\\left(df\\right)\n\\]\nTry by hand:\n\n\nShow R code\nfit.H0 &lt;- glm(chd ~ cigs, data = wcgs, family = binomial)\nfit.H1 &lt;- glm(chd ~ cigs + height, data = wcgs, family = binomial)\nD.H0 &lt;- deviance(fit.H0)  # same as `-2*logLik(fit.H0)`\nD.H1 &lt;- deviance(fit.H1)  # same as `-2*logLik(fit.H1)`\nchisq.stat &lt;- D.H0 - D.H1\npval &lt;- pchisq(chisq.stat, df = 1, lower.tail = FALSE)\nc(\"stat\" = chisq.stat, \"pval\" = pval)\n\n\n     stat      pval \n0.9202473 0.3374101",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#extending-the-general-linear-f-test-1",
    "href": "slides/02-logistic-regression-1.html#extending-the-general-linear-f-test-1",
    "title": "Logistic Regression (Part I)",
    "section": "Extending the general linear F-test",
    "text": "Extending the general linear F-test\nCan do this automatically using R‚Äôs anova() function:\n\n\nShow R code\nanova(fit.H0, fit.H1, test = \"Chi\")",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#confidence-intervals",
    "href": "slides/02-logistic-regression-1.html#confidence-intervals",
    "title": "Logistic Regression (Part I)",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nLots of ways to do this (some bad, some better). R defaults to using a profile likelihood method:\n\n\nShow R code\nconfint(fit.lr2, level = 0.95)\n\n\n                  2.5 %      97.5 %\n(Intercept) -8.13475465 -0.91297018\ncigs         0.01514949  0.03100534\nheight      -0.02619902  0.07702835",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/02-logistic-regression-1.html#questions",
    "href": "slides/02-logistic-regression-1.html#questions",
    "title": "Logistic Regression (Part I)",
    "section": "Questions‚ùì‚ùì‚ùì",
    "text": "Questions‚ùì‚ùì‚ùì",
    "crumbs": [
      "Logistic Regression (Part I)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#logistic-regression",
    "href": "slides/04-logistic-regression-3.html#logistic-regression",
    "title": "Logistic Regression (Part III)",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nIn logistic regression, we use the logit: \\[\n\\text{logit}\\left(p\\right) = \\boldsymbol{x}^\\top\\boldsymbol{\\beta} = \\eta\n\\] which results in \\[\np = \\left[1 + \\exp\\left(-\\eta\\right)\\right]^{-1} = F\\left(\\eta\\right)\n\\]\nTechnically, \\(F\\) can be any monotonic function that maps \\(\\eta\\) to \\(\\left[0, 1\\right]\\) (e.g., any CDF will work)\n\\(F^{-1}\\) is called the link function",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#the-probit-model",
    "href": "slides/04-logistic-regression-3.html#the-probit-model",
    "title": "Logistic Regression (Part III)",
    "section": "The probit model",
    "text": "The probit model\n\nThe probit model uses \\(F\\left(\\eta\\right) = \\Phi\\left(\\eta\\right)\\) (i.e., the CDF of a standard normal distribution) \\[\nP\\left(Y = 1 | \\boldsymbol{x}\\right) = \\Phi\\left(\\beta_0 + \\beta_1x_1 + \\cdots\\right) = \\Phi\\left(\\boldsymbol{x}^\\top\\boldsymbol{\\beta}\\right)\n\\]\n\\(F^{-1} = \\Phi^{-1}\\) is called the probit link, which yields \\[\n\\text{probit}\\left(p\\right) = \\Phi^{-1}\\left(p\\right) = \\boldsymbol{x}^\\top\\boldsymbol{\\beta}\n\\]\nThe term probit is short for probability unit\nProposed in Bliss (1934) and still common for modeling dose-response relationships",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#dobsons-beetle-data",
    "href": "slides/04-logistic-regression-3.html#dobsons-beetle-data",
    "title": "Logistic Regression (Part III)",
    "section": "Dobson‚Äôs beetle data",
    "text": "Dobson‚Äôs beetle data\nThe data give the number of flour beetles killed after five hour exposure to the insecticide carbon disulphide at eight different concentrations.\n\n\nShow R code\nbeetle &lt;- investr::beetle\nfit &lt;- glm(cbind(y, n-y) ~ ldose, data = beetle, \n           family = binomial(link = \"probit\"))\ninvestr::plotFit(\n  fit, pch = 19, cex = 1.2, lwd = 2, \n  xlab = \"Log dose of carbon disulphide\", ylab = \"Proportion killed\",\n  interval = \"confidence\", shade = TRUE, col.conf = \"lightskyblue\"\n)",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#other-link-functions",
    "href": "slides/04-logistic-regression-3.html#other-link-functions",
    "title": "Logistic Regression (Part III)",
    "section": "Other link functions",
    "text": "Other link functions\nCommon link function for binary regression include:\n\nLogit (most common and the default an most software)\nProbit (next most common)\nLog-log\nComplimentary log-log\nCauchit",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#application-to-wcgs-data-set",
    "href": "slides/04-logistic-regression-3.html#application-to-wcgs-data-set",
    "title": "Logistic Regression (Part III)",
    "section": "Application to wcgs data set",
    "text": "Application to wcgs data set\nComparing coefficients from different link functions:\n\n\nShow R code\nwcgs &lt;- na.omit(subset(faraway::wcgs, select = -c(typechd, timechd, behave)))\n\n# Fit binary GLMs with different link functions\nfit.logit &lt;- glm(chd ~ ., data = wcgs, family = binomial(link = \"logit\"))\nfit.probit &lt;- glm(chd ~ ., data = wcgs, family = binomial(link = \"probit\"))\nfit.cloglog &lt;- glm(chd ~ ., data = wcgs, family = binomial(link = \"cloglog\"))\nfit.cauchit &lt;- glm(chd ~ ., data = wcgs, family = binomial(link = \"cauchit\"))\n\n# Compare coefficients\ncoefs &lt;- cbind(\n  \"logit\" = coef(fit.logit),\n  \"probit\" = coef(fit.probit),\n  \"cloglog\" = coef(fit.cloglog),\n  \"cauchit\" = coef(fit.cauchit)\n)\nround(coefs, digits = 3)\n\n\n               logit probit cloglog cauchit\n(Intercept)  -12.246 -6.452 -11.377 -24.086\nage            0.062  0.031   0.057   0.115\nheight         0.007  0.003   0.007   0.053\nweight         0.009  0.005   0.007   0.003\nsdp            0.018  0.010   0.017   0.042\ndbp           -0.001 -0.001  -0.001  -0.001\nchol           0.011  0.006   0.010   0.020\ncigs           0.021  0.011   0.019   0.043\ndibepB        -0.658 -0.341  -0.604  -1.320\narcuspresent   0.211  0.101   0.206   0.711",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#application-to-wcgs-data-set-1",
    "href": "slides/04-logistic-regression-3.html#application-to-wcgs-data-set-1",
    "title": "Logistic Regression (Part III)",
    "section": "Application to wcgs data set",
    "text": "Application to wcgs data set\nComparing fitted values from different link functions:\n\n\nShow R code\n# Compare fitted values (i.e., predicted probabilities)\npreds &lt;- cbind(\n  \"logit\" = fitted(fit.logit),\n  \"probit\" = fitted(fit.probit),\n  \"cloglog\" = fitted(fit.cloglog),\n  \"cauchit\" = fitted(fit.cauchit)\n)\nhead(round(preds, digits = 3))\n\n\n  logit probit cloglog cauchit\n1 0.071  0.072   0.072   0.076\n2 0.073  0.074   0.075   0.084\n3 0.010  0.006   0.011   0.038\n4 0.010  0.006   0.012   0.039\n5 0.169  0.167   0.168   0.150\n6 0.034  0.033   0.035   0.051",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#application-to-wcgs-data-set-2",
    "href": "slides/04-logistic-regression-3.html#application-to-wcgs-data-set-2",
    "title": "Logistic Regression (Part III)",
    "section": "Application to wcgs data set",
    "text": "Application to wcgs data set\nComparing fitted values from different link functions:\n\n\nShow R code\nplot(sort(preds[, \"logit\"]), type = \"l\", ylab = \"Fitted value\")\nlines(sort(preds[, \"probit\"]), col = 2)\nlines(sort(preds[, \"cloglog\"]), col = 3)\nlines(sort(preds[, \"cauchit\"]), col = 4)\nlegend(\"topleft\", legend = c(\"logit\", \"probit\", \"cloglog\", \"cauchit\"), \n       col = 1:4, lty = 1, inset = 0.01)",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#as-a-latent-variable-model",
    "href": "slides/04-logistic-regression-3.html#as-a-latent-variable-model",
    "title": "Logistic Regression (Part III)",
    "section": "As a latent variable model",
    "text": "As a latent variable model\n\nLogistic regression (and the other link functions) has an equivalent formulation as a latent variable model\nConsider a linear model with continuous outcome \\(Y^\\star = \\boldsymbol{x}^\\top\\boldsymbol{\\beta} + \\epsilon\\)\nImagine that we can only observe the binary variable \\[\nY = \\begin{cases}\n1 \\quad \\text{if } Y^\\star &gt; 0, \\\\\n0 \\quad \\text{otherwise}\n\\end{cases}\n\\]\nAssuming \\(\\epsilon \\sim \\text{Logistic}\\left(0, 1\\right)\\) leads to the usual logit model\nAssuming \\(\\epsilon \\sim N\\left(0, 1\\right)\\) leads to the probit model\nAnd so on‚Ä¶",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#application-surrogate-residual",
    "href": "slides/04-logistic-regression-3.html#application-surrogate-residual",
    "title": "Logistic Regression (Part III)",
    "section": "Application: surrogate residual",
    "text": "Application: surrogate residual\n\nThere are several types of residuals defined for logistic regression (e.g., deviance residuals and Pearson residuals)\nThe surrogate residual act like the usual residual from a normal linear model and has similar properties!\nSee Liu and Zhang (2018), Greenwell et al.¬†(2018), and Cheng et al.¬†(2020) for details\nA novel R-squared measure based on the surrogate idea was proposed in Liu et al.¬†(2022)\nFor implementation, see R packages sure, SurrogateRsq, and PAsso",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#o-rings-example",
    "href": "slides/04-logistic-regression-3.html#o-rings-example",
    "title": "Logistic Regression (Part III)",
    "section": "O-rings example",
    "text": "O-rings example\nOn January 28, 1986, the Space Shuttle Challenger broke apart 73 seconds into its flight, killing all seven crew members aboard. The crash was linked to the failure of O-ring seals in the rocket engines. Data was collected on the 23 previous shuttle missions. The launch temperature on the day of the crash was 31 \\(^\\circ\\)F.\n\n\nShow R code\nhead(orings &lt;- faraway::orings)",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#o-rings-example-1",
    "href": "slides/04-logistic-regression-3.html#o-rings-example-1",
    "title": "Logistic Regression (Part III)",
    "section": "O-rings example",
    "text": "O-rings example\n\n\nShow R code\nplot(damage/6 ~ temp, data = orings, xlim = c(25, 85), pch = 19,\n     ylim = c(0, 1), ylab = \"Proportion damaged\")",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#o-rings-example-2",
    "href": "slides/04-logistic-regression-3.html#o-rings-example-2",
    "title": "Logistic Regression (Part III)",
    "section": "O-rings example",
    "text": "O-rings example\nExpand binomial data into independent Bernoulli trials\n\n\nShow R code\ntmp &lt;- rep(orings$temp, each = 6)\ndmg &lt;- sapply(orings$damage, FUN = function(x) rep(c(0, 1), times = c(6 - x, x)))\norings2 &lt;- data.frame(\"temp\" = tmp, \"damage\" = as.vector(dmg))\nhead(orings2, n = 15)",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#o-rings-example-3",
    "href": "slides/04-logistic-regression-3.html#o-rings-example-3",
    "title": "Logistic Regression (Part III)",
    "section": "O-rings example",
    "text": "O-rings example\nHere we‚Äôll just fit a logistic regression to the expanded bernoulli version of the data\n\n\nShow R code\n# Fit a logistic regression (LR) model using 0/1 version of the data\norings.lr &lt;- glm(damage ~ temp, data = orings2, \n                 family = binomial(link = \"logit\"))\nsummary(orings.lr)\n\n\n\nCall:\nglm(formula = damage ~ temp, family = binomial(link = \"logit\"), \n    data = orings2)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 11.66299    3.29616   3.538 0.000403 ***\ntemp        -0.21623    0.05318  -4.066 4.77e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 76.745  on 137  degrees of freedom\nResidual deviance: 54.759  on 136  degrees of freedom\nAIC: 58.759\n\nNumber of Fisher Scoring iterations: 6",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#o-rings-example-4",
    "href": "slides/04-logistic-regression-3.html#o-rings-example-4",
    "title": "Logistic Regression (Part III)",
    "section": "O-rings example",
    "text": "O-rings example\nWhat‚Äôs the estimated probability that an O-ring will be damaged at 31 \\(^\\circ\\)F? Give a point estimate as well as a 95% confidence interval.\n\n\n\nShow R code\npredict(orings.lr, newdata = data.frame(\"temp\" = 31), type = \"response\", \n        se = TRUE)\n\n\n$fit\n        1 \n0.9930342 \n\n$se.fit\n         1 \n0.01153302 \n\n$residual.scale\n[1] 1\n\n\nShow R code\n# Better approach for a 95% CI?\npred &lt;- predict(orings.lr, newdata = data.frame(\"temp\" = 31), \n                type = \"link\", se = TRUE)\nplogis(pred$fit + c(-qnorm(0.975), qnorm(0.975)) * pred$se.fit)\n\n\n[1] 0.8444824 0.9997329",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#o-rings-example-5",
    "href": "slides/04-logistic-regression-3.html#o-rings-example-5",
    "title": "Logistic Regression (Part III)",
    "section": "O-rings example",
    "text": "O-rings example\n\n\nShow R code\n# Is this extrapolating?\nplot(damage / 6 ~ temp, data = orings, pch = 19, cex = 1.3,\n     col = adjustcolor(1, alpha.f = 0.3), xlim = c(0, 100), ylim = c(0, 1))\nx &lt;- seq(from = 0, to = 100, length = 1000)\ny &lt;- predict(orings.lr, newdata = data.frame(\"temp\" = x), \n             type = \"response\")\nlines(x, y, lwd = 2, col = 2)\nabline(v = 31, lty = 2, col = 3)",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#o-rings-examples",
    "href": "slides/04-logistic-regression-3.html#o-rings-examples",
    "title": "Logistic Regression (Part III)",
    "section": "O-rings examples",
    "text": "O-rings examples\nMore interesting question: at what temperature(s) can we expect the risk/probability of damage to exceed 0.8?\n\nThis is a problem of inverse estimation, which is the purpose of the investr package!\n\n\nShow R code\n# To install from CRAN, use\n#\n# &gt; install.packages(\"investr\")\n#\n# See ?investr::invest for details and examples\ninvestr::invest(orings.lr, y0 = 0.8, interval = \"Wald\", lower = 40, upper = 60)\n\n\n estimate     lower     upper        se \n47.525881 39.993462 55.058299  3.843141",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#o-rings-example-6",
    "href": "slides/04-logistic-regression-3.html#o-rings-example-6",
    "title": "Logistic Regression (Part III)",
    "section": "O-rings example",
    "text": "O-rings example\nHere‚Äôs an equivalent logistic regression model fit to the original binomial version of the data (need to provide number of successes and number of failures)\n\n\nShow R code\norings.lr2 &lt;- glm(cbind(damage, 6 - damage) ~ temp, data = orings,\n                  family = binomial(link = \"logit\"))\nsummary(orings.lr2)\n\n\n\nCall:\nglm(formula = cbind(damage, 6 - damage) ~ temp, family = binomial(link = \"logit\"), \n    data = orings)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 11.66299    3.29626   3.538 0.000403 ***\ntemp        -0.21623    0.05318  -4.066 4.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 38.898  on 22  degrees of freedom\nResidual deviance: 16.912  on 21  degrees of freedom\nAIC: 33.675\n\nNumber of Fisher Scoring iterations: 6",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#overdispersion-1",
    "href": "slides/04-logistic-regression-3.html#overdispersion-1",
    "title": "Logistic Regression (Part III)",
    "section": "Overdispersion",
    "text": "Overdispersion\n\nFor a Bernoulli random variable \\(Y\\), \\(E(Y) = p\\) and \\(V(Y) = p(1 - p)\\).\nSometimes the data exhibit variance greater than expected. Adding a dispersion parameter makes the model more flexible: \\(V(Y) = \\sigma^2 p(1 - p)\\).\nOverdispersion occurs when variability exceeds expectations under the response distribution. Underdispersion is less common.",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#overdispersion-2",
    "href": "slides/04-logistic-regression-3.html#overdispersion-2",
    "title": "Logistic Regression (Part III)",
    "section": "Overdispersion",
    "text": "Overdispersion\n\nFor a correctly specified model, the Pearson chi-square statistic and deviance, divided by their degrees of freedom, should be about one. Values much larger indicate overdispersion.\nSuch goodness-of-fit statistics require replicated data. Problems like outliers, wrong link function, omitted terms, or untransformed predictors can inflate goodness-of-fit statistics.\nA large difference between the Pearson statistic and deviance suggests data sparsity.",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#o-rings-example-7",
    "href": "slides/04-logistic-regression-3.html#o-rings-example-7",
    "title": "Logistic Regression (Part III)",
    "section": "O-rings example",
    "text": "O-rings example\nYou can check for overdispersion manually or by using performance:\n\n\nShow R code\nperformance::check_overdispersion(orings.lr2)\n\n\n# Overdispersion test\n\n dispersion ratio = 1.206\n          p-value = 0.616",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#overdispersion-3",
    "href": "slides/04-logistic-regression-3.html#overdispersion-3",
    "title": "Logistic Regression (Part III)",
    "section": "Overdispersion",
    "text": "Overdispersion\nTwo common (but equivalent) ways to handle overdispersion in R:\n\nEstimate the dispersion parameter \\(\\sigma^2\\) and provide it to summary() to adjust the standard errors approriately\nUse the quasibinomial() family",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#overdispersion-4",
    "href": "slides/04-logistic-regression-3.html#overdispersion-4",
    "title": "Logistic Regression (Part III)",
    "section": "Overdispersion",
    "text": "Overdispersion\nEstimate the dispersion parameter \\(\\sigma^2\\); analagous to MSE in linear regression\n\n\nShow R code\n(sigma2 &lt;- sum(residuals(orings.lr2, type = \"pearson\") ^ 2) / orings.lr2$df.residual)\n\n\n[1] 1.336542\n\n\nShow R code\n# Print model summary based on estimated dispersion parameter\nsummary(orings.lr2, dispersion = sigma2)\n\n\n\nCall:\nglm(formula = cbind(damage, 6 - damage) ~ temp, family = binomial(link = \"logit\"), \n    data = orings)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 11.66299    3.81077   3.061 0.002209 ** \ntemp        -0.21623    0.06148  -3.517 0.000436 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1.336542)\n\n    Null deviance: 38.898  on 22  degrees of freedom\nResidual deviance: 16.912  on 21  degrees of freedom\nAIC: 33.675\n\nNumber of Fisher Scoring iterations: 6",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#overdispersion-5",
    "href": "slides/04-logistic-regression-3.html#overdispersion-5",
    "title": "Logistic Regression (Part III)",
    "section": "Overdispersion",
    "text": "Overdispersion\nCan use quasibinomial() family to account for over-dispersion automatically (notice the estimated coeficients don‚Äôt change)\n\n\nShow R code\norings.qb &lt;- glm(cbind(damage, 6 - damage) ~ temp, data = orings, \n                 family = quasibinomial)\nsummary(orings.qb)\n\n\n\nCall:\nglm(formula = cbind(damage, 6 - damage) ~ temp, family = quasibinomial, \n    data = orings)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) 11.66299    3.81077   3.061  0.00594 **\ntemp        -0.21623    0.06148  -3.517  0.00205 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 1.336542)\n\n    Null deviance: 38.898  on 22  degrees of freedom\nResidual deviance: 16.912  on 21  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 6",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#generalized-additive-models-gams",
    "href": "slides/04-logistic-regression-3.html#generalized-additive-models-gams",
    "title": "Logistic Regression (Part III)",
    "section": "Generalized additive models (GAMs)",
    "text": "Generalized additive models (GAMs)\n\n\\(\\text{logit}\\left(p\\right) = \\beta_0 + f_1\\left(x_1\\right) + f_2\\left(x_2\\right) + \\dots + f_p\\left(x_p\\right)\\)\n\nThe \\(f_i\\) are referred to as shape functions or term contributions and are often modeled using splines\nCan also include pairwise interactions of the form \\(f_{ij}\\left(x_i, x_j\\right)\\)\n\nEasy to interpret (e.g., just plot the individual shape functions)!\n\nInteraction effects can be understood with heat maps, etc.\n\nModern GAMs, called GA2Ms, automatically include relevant pairwise interaction effects",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/04-logistic-regression-3.html#explainable-boosting-machines-ebms",
    "href": "slides/04-logistic-regression-3.html#explainable-boosting-machines-ebms",
    "title": "Logistic Regression (Part III)",
    "section": "Explainable boosting machines (EBMs)",
    "text": "Explainable boosting machines (EBMs)\n\nEBM ‚âà .darkblue[GA2M] + .green[Boosting] + .tomato[Bagging]\nPython library: interpret",
    "crumbs": [
      "Logistic Regression (Part III)"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#count-data",
    "href": "slides/06-count-regression.html#count-data",
    "title": "Regression for Counts and Rates",
    "section": "Count data",
    "text": "Count data\n\nA type of data in which the observations take on non-negative integer values \\(\\left\\{0, 1, 2, \\dots\\right\\}\\)\nExamples include:\n\nThe number of patients who come to the ER of Children‚Äôs Hospital between 9PM and 1AM.\nThe number of shoppers in Kenwood Towne Centre on a calendar day.\nThe number of Google searches (in a week) for flights to Shanghai right before Lunar New Year.",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-poisson-distribution",
    "href": "slides/06-count-regression.html#the-poisson-distribution",
    "title": "Regression for Counts and Rates",
    "section": "The Poisson distribution",
    "text": "The Poisson distribution\n\nThe simplest distribution for modeling counts is the Poisson distribution:\n\n\\(Y \\sim \\mathrm{Poi}\\left(\\mu\\right)\\)\n\\(f\\left(y\\right) = \\mathrm{P}\\left(Y = y\\right) = \\frac{\\exp\\left(-\\mu\\right)\\mu^y}{y!}\\), for \\(y = 0, 1, 2, \\dots\\)\n\\(\\mathrm{E}\\left(Y\\right) = \\mathrm{Var}\\left(Y\\right) = \\mu\\), where \\(\\mu \\in \\left(0, \\infty\\right)\\)\nFact: \\(\\sum_i Y_i \\sim \\mathrm{Poi}\\left(\\sum_i \\mu_i\\right)\\) (aggregated data)\n\nAn interesting characteristic of the Poisson is that the mean and variance are equal to each other",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-poisson-distribution-1",
    "href": "slides/06-count-regression.html#the-poisson-distribution-1",
    "title": "Regression for Counts and Rates",
    "section": "The Poisson distribution",
    "text": "The Poisson distribution\n\n\nShow R code\n# Install required package(s)\npkgs &lt;- c(\"faraway\", \"investr\", \"mgcv\", \"performance\", \"pscl\")\nlib &lt;- installed.packages()[, \"Package\"]\ninstall.packages(setdiff(pkgs, lib))\n\n# Y ~ Poisson(lambda = 0.5)\nset.seed(2004)  # for reproducibility\npar(mfrow = c(2, 2))\nfor (lambda in c(0.5, 2, 5, 15)) {\n  y &lt;- dpois(0:35, lambda = lambda)\n  barplot(y, xlab = \"y\", ylab = \"P(Y = y)\", names = 0:35, main = paste(\"E(Y) =\", lambda), \n          col = \"dodgerblue2\", border = \"dodgerblue2\", las = 1)\n}",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-poisson-distribution-2",
    "href": "slides/06-count-regression.html#the-poisson-distribution-2",
    "title": "Regression for Counts and Rates",
    "section": "The Poisson distribution",
    "text": "The Poisson distribution\n\n\nShow R code\ny &lt;- rpois(10000, lambda = 200)\nhist(y, br = 50)",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-poisson-distribution-3",
    "href": "slides/06-count-regression.html#the-poisson-distribution-3",
    "title": "Regression for Counts and Rates",
    "section": "The Poisson distribution",
    "text": "The Poisson distribution\n\nIf the count is some number out of some possible total, then the response would be more appropriately modeled as a binomial r.v.\nHowever, for small \\(p\\) and large \\(n\\), the Poisson distribution provides a reasonable approximation to the binomial; For example, in modeling the incidence of rare forms of cancer, the number of people affected is a small proportion of the population in a given geographical area\n\n\n\nShow R code\nc(\"Binomial\" = pbinom(5, size = 8, p = 0.7),\n  \"Poisson\" = ppois(5, lambda = 8 * 0.7))\n\n\n Binomial   Poisson \n0.4482262 0.5118609 \n\n\nShow R code\nc(\"Binomial\" = pbinom(5, size = 100, p = 0.05),\n  \"Poisson\" = ppois(5, lambda = 100 * 0.05))\n\n\n Binomial   Poisson \n0.6159991 0.6159607",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\nThere are 30 Galapagos islands and 7 variables in the data set. The relationship between the number of plant species (\\(Y\\)) and several geographic variables is of interest. The original data set contained several missing values which have been filled for convenience. See the faraway::galamiss data set for the original version.",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-1",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-1",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-2",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-2",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\nWe‚Äôll remove Endemics since we won‚Äôt be using it!\n\n\nShow R code\n# Load the Galapagos Islands data\ndata(gala, package = \"faraway\")\ngala$Endemics &lt;- NULL\n\n# Print structure of data frame\nstr(gala)\n\n\n'data.frame':   30 obs. of  6 variables:\n $ Species  : num  58 31 3 25 2 18 24 10 8 2 ...\n $ Area     : num  25.09 1.24 0.21 0.1 0.05 ...\n $ Elevation: num  346 109 114 46 77 119 93 168 71 112 ...\n $ Nearest  : num  0.6 0.6 2.8 1.9 1.9 8 6 34.1 0.4 2.6 ...\n $ Scruz    : num  0.6 26.3 58.7 47.4 1.9 ...\n $ Adjacent : num  1.84 572.33 0.78 0.18 903.82 ...",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-3",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-3",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\nSummary of data frame\n\n\nShow R code\nsummary(gala)\n\n\n    Species            Area             Elevation          Nearest     \n Min.   :  2.00   Min.   :   0.0100   Min.   :  25.00   Min.   : 0.20  \n 1st Qu.: 13.00   1st Qu.:   0.2575   1st Qu.:  97.75   1st Qu.: 0.80  \n Median : 42.00   Median :   2.5900   Median : 192.00   Median : 3.05  \n Mean   : 85.23   Mean   : 261.7087   Mean   : 368.03   Mean   :10.06  \n 3rd Qu.: 96.00   3rd Qu.:  59.2375   3rd Qu.: 435.25   3rd Qu.:10.03  \n Max.   :444.00   Max.   :4669.3200   Max.   :1707.00   Max.   :47.40  \n     Scruz           Adjacent      \n Min.   :  0.00   Min.   :   0.03  \n 1st Qu.: 11.03   1st Qu.:   0.52  \n Median : 46.65   Median :   2.59  \n Mean   : 56.98   Mean   : 261.10  \n 3rd Qu.: 81.08   3rd Qu.:  59.24  \n Max.   :290.20   Max.   :4669.32",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-4",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-4",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\n\n\nShow R code\npairs(gala)",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-5",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-5",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\n\n\nShow R code\npairs(~ log(Species) + log(Area) + log(Elevation) + \n  log(Nearest) + log(Scruz + 0.1) + log(Adjacent), data = gala)",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-6",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-6",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\nLet‚Äôs start with a linear model\n\n\nShow R code\ngala.ols &lt;- lm(log(Species) ~ log(Area) + log(Elevation) + log(Nearest) + I(log(Scruz + 0.1)) + log(Adjacent), data = gala)\nsummary(gala.ols)\n\n\n\nCall:\nlm(formula = log(Species) ~ log(Area) + log(Elevation) + log(Nearest) + \n    I(log(Scruz + 0.1)) + log(Adjacent), data = gala)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.4563 -0.5192 -0.1059  0.4632  1.3351 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          5.10569    1.64880   3.097  0.00493 ** \nlog(Area)            0.50350    0.09942   5.064 3.53e-05 ***\nlog(Elevation)      -0.37384    0.32242  -1.159  0.25767    \nlog(Nearest)        -0.06564    0.11475  -0.572  0.57262    \nI(log(Scruz + 0.1)) -0.08255    0.09517  -0.867  0.39433    \nlog(Adjacent)       -0.02488    0.04596  -0.541  0.59327    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7877 on 24 degrees of freedom\nMultiple R-squared:  0.7899,    Adjusted R-squared:  0.7461 \nF-statistic: 18.05 on 5 and 24 DF,  p-value: 1.941e-07",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-7",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-7",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\nResidual analysis\n\n\nShow R code\npar(mfrow = c(2, 2))\nplot(gala.ols, which = 1:4)",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#poisson-regression",
    "href": "slides/06-count-regression.html#poisson-regression",
    "title": "Regression for Counts and Rates",
    "section": "Poisson regression",
    "text": "Poisson regression\n\nNeed a way to link the mean response \\(\\mathrm{E}\\left(Y\\right) = \\mu \\in \\left(0, \\infty\\right)\\) to the linear predictor \\(\\eta = \\boldsymbol{x}^\\top\\boldsymbol{p}\\)\nIn Poisson we regression, we default to \\[\n\\log\\left(\\mu\\right) = \\boldsymbol{x}^\\top\\boldsymbol{p}\n\\]\nMaximum likelihood estimation provides a convenient estimate of \\(\\boldsymbol{\\beta}\\)",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-8",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-8",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\nTry a Poisson regression\n\n\nShow R code\nsummary(gala.poi &lt;- glm(Species ~ ., data = gala, family = poisson))\n\n\n\nCall:\nglm(formula = Species ~ ., family = poisson, data = gala)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.155e+00  5.175e-02  60.963  &lt; 2e-16 ***\nArea        -5.799e-04  2.627e-05 -22.074  &lt; 2e-16 ***\nElevation    3.541e-03  8.741e-05  40.507  &lt; 2e-16 ***\nNearest      8.826e-03  1.821e-03   4.846 1.26e-06 ***\nScruz       -5.709e-03  6.256e-04  -9.126  &lt; 2e-16 ***\nAdjacent    -6.630e-04  2.933e-05 -22.608  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3510.73  on 29  degrees of freedom\nResidual deviance:  716.85  on 24  degrees of freedom\nAIC: 889.68\n\nNumber of Fisher Scoring iterations: 5",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-9",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-9",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\nCheck mean-variance relationship\n\n\nShow R code\nmu &lt;- fitted(gala.poi)  # fitted fitted.values\ny &lt;- gala.poi$y  # observed response values\nplot(log(mu), log((y - mu)^2), xlab = expression(hat(mu)),\n     ylab = expression((y-hat(mu))^2))\nabline(0, 1)",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-10",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-10",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\nCrude check for overdispersion\n\n\nShow R code\nperformance::check_overdispersion(gala.poi)\n\n\n# Overdispersion test\n\n       dispersion ratio =  31.749\n  Pearson's Chi-Squared = 761.979\n                p-value = &lt; 0.001\n\n\nOverdispersion detected.",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#gal√°pagos-islands-data-11",
    "href": "slides/06-count-regression.html#gal√°pagos-islands-data-11",
    "title": "Regression for Counts and Rates",
    "section": "Gal√°pagos islands data",
    "text": "Gal√°pagos islands data\nSimilar to before, can use quasipoisson() family to correct fo overdispersion:\n\n\nShow R code\ngala.quasipoi &lt;- glm(Species ~ ., family = quasipoisson(link = \"log\"),  data = gala)\nsummary(gala.quasipoi)\n\n\n\nCall:\nglm(formula = Species ~ ., family = quasipoisson(link = \"log\"), \n    data = gala)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.1548079  0.2915901  10.819 1.03e-10 ***\nArea        -0.0005799  0.0001480  -3.918 0.000649 ***\nElevation    0.0035406  0.0004925   7.189 1.98e-07 ***\nNearest      0.0088256  0.0102622   0.860 0.398292    \nScruz       -0.0057094  0.0035251  -1.620 0.118380    \nAdjacent    -0.0006630  0.0001653  -4.012 0.000511 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 31.74921)\n\n    Null deviance: 3510.73  on 29  degrees of freedom\nResidual deviance:  716.85  on 24  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#rates-and-offsets",
    "href": "slides/06-count-regression.html#rates-and-offsets",
    "title": "Regression for Counts and Rates",
    "section": "Rates and offsets",
    "text": "Rates and offsets\n\nThe number of observed events may depend on a size variable that determines the number of opportunities for the events to occur\n\nFor example, the number of burglaries reported in different cities\n\nIn other cases, the size variable may be time\n\nFor example, the number of customers served by a sales worker (must take account of the differing amounts of time worked)",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#rates-and-offsets-1",
    "href": "slides/06-count-regression.html#rates-and-offsets-1",
    "title": "Regression for Counts and Rates",
    "section": "Rates and offsets",
    "text": "Rates and offsets\nAn experiment was conducted to determine the effect of gamma radiation on the numbers of chromosomal abnormalities (ca) observed. The number (cells), in hundreds of cells exposed in each run, differs. The dose amount (doseamt) and the rate (doserate) at which the dose is applied are the predictors of interest. The hypothesized model is as follows:\n\\[\n\\begin{align}\n\\log\\left(\\mathtt{ca}/\\mathtt{cells}\\right) &= \\boldsymbol{x}^\\top\\boldsymbol{\\beta} \\\\\n\\implies \\log\\left(\\mathtt{ca}\\right) &= \\log\\left(\\mathtt{cells}\\right) + \\boldsymbol{x}^\\top\\boldsymbol{\\beta}\n\\end{align}\n\\]",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#rates-and-offsets-2",
    "href": "slides/06-count-regression.html#rates-and-offsets-2",
    "title": "Regression for Counts and Rates",
    "section": "Rates and offsets",
    "text": "Rates and offsets\n\n\nShow R code\ndicentric &lt;- faraway::dicentric\ndicentric$dosef &lt;- factor(dicentric$doseamt)\nfit &lt;- glm(ca ~ offset(log(cells)) + log(doserate)*dosef, family = poisson, \n           data = dicentric)\nsummary(fit)\n\n\n\nCall:\nglm(formula = ca ~ offset(log(cells)) + log(doserate) * dosef, \n    family = poisson, data = dicentric)\n\nCoefficients:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)            -2.74671    0.03426 -80.165  &lt; 2e-16 ***\nlog(doserate)           0.07178    0.03518   2.041 0.041299 *  \ndosef2.5                1.62542    0.04946  32.863  &lt; 2e-16 ***\ndosef5                  2.76109    0.04349  63.491  &lt; 2e-16 ***\nlog(doserate):dosef2.5  0.16122    0.04830   3.338 0.000844 ***\nlog(doserate):dosef5    0.19350    0.04243   4.561  5.1e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 4753.00  on 26  degrees of freedom\nResidual deviance:   21.75  on 21  degrees of freedom\nAIC: 209.16\n\nNumber of Fisher Scoring iterations: 4",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#zero-inflated-outcomes",
    "href": "slides/06-count-regression.html#zero-inflated-outcomes",
    "title": "Regression for Counts and Rates",
    "section": "zero-inflated outcomes",
    "text": "zero-inflated outcomes\nThe state wildlife biologists want to model how many fish are being caught by fishermen at a state park. Visitors are asked how long they stayed, how many people were in the group, were there children in the group, and how many fish were caught. Some visitors do not fish, but there is no data on whether a person fished or not. Some visitors who did fish did not catch any fish so there are excess zeros in the data because of the people that did not fish.",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#zero-inflated-outcomes-1",
    "href": "slides/06-count-regression.html#zero-inflated-outcomes-1",
    "title": "Regression for Counts and Rates",
    "section": "Zero-inflated outcomes",
    "text": "Zero-inflated outcomes\nOur sample consists of We have data on N=250 groups that went to a park. Each group was questioned about how many fish they caught (count), how many children were in the group (child), how many people were in the group (persons), and whether or not they brought a camper to the park (camper).\nThe data can be read in as follows:\n\nfish &lt;- read.csv(\"https://stats.idre.ucla.edu/stat/data/fish.csv\")",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#zero-inflated-outcomes-2",
    "href": "slides/06-count-regression.html#zero-inflated-outcomes-2",
    "title": "Regression for Counts and Rates",
    "section": "Zero-inflated outcomes",
    "text": "Zero-inflated outcomes\n\n# Retain only variables of interest and print summary\nfish &lt;- fish[, c(\"count\", \"child\", \"persons\", \"camper\")]\nsummary(fish)\n\n     count             child          persons          camper     \n Min.   :  0.000   Min.   :0.000   Min.   :1.000   Min.   :0.000  \n 1st Qu.:  0.000   1st Qu.:0.000   1st Qu.:2.000   1st Qu.:0.000  \n Median :  0.000   Median :0.000   Median :2.000   Median :1.000  \n Mean   :  3.296   Mean   :0.684   Mean   :2.528   Mean   :0.588  \n 3rd Qu.:  2.000   3rd Qu.:1.000   3rd Qu.:4.000   3rd Qu.:1.000  \n Max.   :149.000   Max.   :3.000   Max.   :4.000   Max.   :1.000",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#zero-inflated-outcomes-3",
    "href": "slides/06-count-regression.html#zero-inflated-outcomes-3",
    "title": "Regression for Counts and Rates",
    "section": "Zero-inflated outcomes",
    "text": "Zero-inflated outcomes\n\n\nShow R code\npairs(log(count + 0.1) ~ ., data = fish)",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#zero-inflated-outcomes-4",
    "href": "slides/06-count-regression.html#zero-inflated-outcomes-4",
    "title": "Regression for Counts and Rates",
    "section": "Zero-inflated outcomes",
    "text": "Zero-inflated outcomes\nToo many zeros?\n\n\nShow R code\nbarplot(table(fish$count))",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#zero-inflated-outcomes-5",
    "href": "slides/06-count-regression.html#zero-inflated-outcomes-5",
    "title": "Regression for Counts and Rates",
    "section": "Zero-inflated outcomes",
    "text": "Zero-inflated outcomes\n\nfish.poi &lt;- glm(count ~ ., data = fish, family = poisson)\nsummary(fish.poi)\n\n\nCall:\nglm(formula = count ~ ., family = poisson, data = fish)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.98183    0.15226  -13.02   &lt;2e-16 ***\nchild       -1.68996    0.08099  -20.87   &lt;2e-16 ***\npersons      1.09126    0.03926   27.80   &lt;2e-16 ***\ncamper       0.93094    0.08909   10.45   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2958.4  on 249  degrees of freedom\nResidual deviance: 1337.1  on 246  degrees of freedom\nAIC: 1682.1\n\nNumber of Fisher Scoring iterations: 6\n\nperformance::check_zeroinflation(fish.poi)\n\n# Check for zero-inflation\n\n   Observed zeros: 142\n  Predicted zeros: 95\n            Ratio: 0.67\n\n\nModel is underfitting zeros (probable zero-inflation).",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-hurdle-model",
    "href": "slides/06-count-regression.html#the-hurdle-model",
    "title": "Regression for Counts and Rates",
    "section": "The hurdle model",
    "text": "The hurdle model\nIn addition to predicting the number of fish caught, there is interest in predicting the existence of excess zeros (i.e., the zeroes that were not simply a result of bad luck or lack of fishing skill). In particular, we‚Äôd like to estimate the effect of party size on catching zero fish.\nWe can accomplish this in several ways, but popular choices include:\n\nThe zero-inflated Poisson (or negative binomial) model\nThe hurdle model",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-hurdle-model-1",
    "href": "slides/06-count-regression.html#the-hurdle-model-1",
    "title": "Regression for Counts and Rates",
    "section": "The hurdle model",
    "text": "The hurdle model\nIn this example, we‚Äôll use a simple hurdle model, which essentially fits two separate models:\n\n\\(\\mathrm{P}\\left(Y = 0\\right)\\) via a logistic regression\n\\(\\mathrm{P}\\left(Y = j\\right)\\), \\(j = 1, 2, \\dots\\) via a truncated Poisson regression",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-hurdle-model-2",
    "href": "slides/06-count-regression.html#the-hurdle-model-2",
    "title": "Regression for Counts and Rates",
    "section": "The hurdle model",
    "text": "The hurdle model\nYou can fit hurdle models using the hurdle() function from package pscl:\n\n\nShow R code\nsuppressMessages(library(pscl))\n\nsummary(fish.hur &lt;- hurdle(count ~ child + camper | persons, data = fish))\n\n\n\nCall:\nhurdle(formula = count ~ child + camper | persons, data = fish)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-0.8590 -0.7384 -0.6782 -0.1234 23.9679 \n\nCount model coefficients (truncated poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.64668    0.08278  19.892   &lt;2e-16 ***\nchild       -0.75918    0.09004  -8.432   &lt;2e-16 ***\ncamper       0.75166    0.09112   8.249   &lt;2e-16 ***\nZero hurdle model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -0.7808     0.3240  -2.410   0.0160 *\npersons       0.1993     0.1161   1.716   0.0862 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 14 \nLog-likelihood: -1047 on 5 Df",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-hurdle-model-3",
    "href": "slides/06-count-regression.html#the-hurdle-model-3",
    "title": "Regression for Counts and Rates",
    "section": "The hurdle model",
    "text": "The hurdle model\nCheck the logit part directly:\n\nz &lt;- fish\nz$count &lt;- ifelse(z$count == 0, 0, 1)\nglm(count ~ persons, data = z, family = binomial)\n\n\nCall:  glm(formula = count ~ persons, family = binomial, data = z)\n\nCoefficients:\n(Intercept)      persons  \n    -0.7808       0.1993  \n\nDegrees of Freedom: 249 Total (i.e. Null);  248 Residual\nNull Deviance:      341.9 \nResidual Deviance: 339  AIC: 343",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-hurdle-model-4",
    "href": "slides/06-count-regression.html#the-hurdle-model-4",
    "title": "Regression for Counts and Rates",
    "section": "The hurdle model",
    "text": "The hurdle model\nInterpretation of the previous model\n\nThe expected log number of the fish caught reduces by 0.759 for every additional chile (all else held constant)\nBeing a camper increases the expected log number of fish caught by 0.752 (all else held constant)\nThe log odds of catching at least one fish increases by 0.199 for every additional person",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/06-count-regression.html#the-hurdle-model-5",
    "href": "slides/06-count-regression.html#the-hurdle-model-5",
    "title": "Regression for Counts and Rates",
    "section": "The hurdle model",
    "text": "The hurdle model\nPredicting new observations\n\nnewobs &lt;- data.frame(\"child\" = 0, \"persons\" = 3, \"camper\" = 1)\nhead(predict(fish.hur, newdata = newobs, type = \"prob\"))\n\n          0            1           2           3           4          5\n1 0.5456108 8.310743e-05 0.000457296 0.001677504 0.004615207 0.01015801\n           6          7          8          9         10         11         12\n1 0.01863138 0.02929101 0.04029322 0.04926936 0.05422062 0.05424495 0.04974685\n          13         14         15         16         17          18         19\n1 0.04211237 0.03310314 0.02428653 0.01670448 0.01081363 0.006611296 0.00382931\n           20          21           22           23           24          25\n1 0.002107066 0.001104196 0.0005523459 0.0002642839 0.0001211845 5.33451e-05\n            26           27           28           29           30          31\n1 2.257921e-05 9.203065e-06 3.617112e-06 1.372624e-06 5.035212e-07 1.78749e-07\n            32           33           34           35           36          37\n1 6.147253e-08 2.050004e-08 6.635342e-09 2.086329e-09 6.377754e-10 1.89694e-10\n            38           39           40           41           42           43\n1 5.493606e-11 1.550174e-11 4.264891e-12 1.144752e-12 2.999507e-13 7.676599e-14\n            44           45           46           47           48           49\n1 1.920011e-14 4.695466e-15 1.123333e-15 2.630256e-16 6.030375e-17 1.354365e-17\n           50           51           52           53           54           55\n1 2.98094e-18 6.432364e-19 1.361303e-19 2.826614e-20 5.760501e-21 1.152617e-21\n            56           57           58           59           60          61\n1 2.265085e-22 4.373178e-23 8.297681e-24 1.547719e-24 2.838759e-25 5.12137e-26\n            62           63           64           65           66           67\n1 9.090379e-27 1.587921e-27 2.730464e-28 4.622859e-29 7.708223e-30 1.266097e-30\n            68           69           70           71           72           73\n1 2.049017e-31 3.268015e-32 5.137756e-33 7.963475e-34 1.217188e-34 1.834942e-35\n           74           75           76           77           78           79\n1 2.72884e-36 4.004095e-37 5.798001e-38 8.286576e-39 1.169144e-39 1.628653e-40\n            80           81           82          83           84          85\n1 2.240402e-41 3.043887e-42 4.085095e-43 5.41641e-44 7.096101e-45 9.18731e-46\n            86           87           88           89           90           91\n1 1.175648e-46 1.487119e-47 1.859733e-48 2.299578e-49 2.811856e-50 3.400472e-51\n            92           93           94           95          96           97\n1 4.067606e-52 4.813306e-53 5.635119e-54 6.527803e-55 7.48313e-56 8.489832e-57\n            98           99          100          101          102          103\n1 9.533679e-58 1.059773e-58 1.166273e-59 1.270769e-60 1.371052e-61 1.464887e-62\n           104          105          106         107          108          109\n1 1.550095e-63 1.624638e-64 1.686701e-65 1.73477e-66 1.767688e-67 1.784706e-68\n           110          111          112          113          114          115\n1 1.785507e-69 1.770215e-70 1.739384e-71 1.693966e-72 1.635262e-73 1.564865e-74\n          116          117          118          119          120          121\n1 1.48459e-75 1.396395e-76 1.302308e-77 1.204354e-78 1.104487e-79 1.004529e-80\n           122          123          124          125          126          127\n1 9.061297e-82 8.107235e-83 7.195129e-84 6.334555e-85 5.532648e-86 4.794208e-87\n           128          129          130          131          132          133\n1 4.121871e-88 3.516351e-89 2.976709e-90 2.500648e-91 2.084809e-92 1.725052e-93\n           134          135          136          137          138          139\n1 1.416722e-94 1.154884e-95 9.345169e-97 7.506787e-98 5.986356e-99 4.73953e-100\n            140           141           142           143           144\n1 3.725587e-101 2.907791e-102 2.253525e-103 1.734258e-104 1.325375e-105\n            145           146           147           148           149\n1 1.005908e-106 7.582161e-108 5.676272e-109 4.220744e-110 3.117383e-111",
    "crumbs": [
      "Regression for Counts and Rates"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#western-collaborative-group-study",
    "href": "slides/03-logistic-regression-2.html#western-collaborative-group-study",
    "title": "Logistic Regression (Part II)",
    "section": "Western collaborative group study",
    "text": "Western collaborative group study\n\\(N = 3154\\) healthy young men aged 39‚Äì59 from the San Francisco area were assessed for their personality type. All were free from coronary heart disease at the start of the research. Eight and a half years later change in this situation was recorded. See ?faraway::wcgs in R.\n\n\nShow R code\n# install.packages(\"faraway\")\nhead(wcgs &lt;- faraway::wcgs)",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#exploratory-data-analysis",
    "href": "slides/03-logistic-regression-2.html#exploratory-data-analysis",
    "title": "Logistic Regression (Part II)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nTry fitting a full model first!\n\n\nShow R code\nlr.fit.all &lt;- glm(chd ~ ., family = binomial(link = \"logit\"), data = wcgs)#, maxit = 9999)\nsummary(lr.fit.all)\n\n\n\nCall:\nglm(formula = chd ~ ., family = binomial(link = \"logit\"), data = wcgs)\n\nCoefficients: (1 not defined because of singularities)\n                  Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)      2.657e+01  2.228e+05   0.000    1.000\nage             -3.651e-15  1.208e+03   0.000    1.000\nheight          -1.087e-14  3.067e+03   0.000    1.000\nweight           1.687e-15  3.832e+02   0.000    1.000\nsdp              2.409e-15  6.738e+02   0.000    1.000\ndbp             -3.019e-15  1.068e+03   0.000    1.000\nchol             8.301e-17  1.524e+02   0.000    1.000\nbehaveA2        -3.044e-14  2.413e+04   0.000    1.000\nbehaveB3         2.517e-16  2.444e+04   0.000    1.000\nbehaveB4         5.450e-15  2.937e+04   0.000    1.000\ncigs            -9.188e-16  4.526e+02   0.000    1.000\ndibepB                  NA         NA      NA       NA\ntypechdinfdeath -4.405e-06  5.876e+04   0.000    1.000\ntypechdnone     -5.313e+01  5.223e+04  -0.001    0.999\ntypechdsilent   -4.400e-06  6.574e+04   0.000    1.000\ntimechd          1.280e-16  1.080e+01   0.000    1.000\narcuspresent     5.666e-14  1.429e+04   0.000    1.000\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1.7692e+03  on 3139  degrees of freedom\nResidual deviance: 1.8217e-08  on 3124  degrees of freedom\n  (14 observations deleted due to missingness)\nAIC: 32\n\nNumber of Fisher Scoring iterations: 25",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#exploratory-data-analysis-1",
    "href": "slides/03-logistic-regression-2.html#exploratory-data-analysis-1",
    "title": "Logistic Regression (Part II)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nRefit without leakage variables typechd and timechd:\n\n\nShow R code\nwcgs &lt;- subset(wcgs, select = -c(typechd, timechd))\nlr.fit.all &lt;- glm(chd ~ ., family = binomial(link = \"logit\"), data = wcgs)#, maxit = 9999)\nsummary(lr.fit.all)\n\n\n\nCall:\nglm(formula = chd ~ ., family = binomial(link = \"logit\"), data = wcgs)\n\nCoefficients: (1 not defined because of singularities)\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -12.331126   2.350347  -5.247 1.55e-07 ***\nage            0.061812   0.012421   4.977 6.47e-07 ***\nheight         0.006903   0.033335   0.207  0.83594    \nweight         0.008637   0.003892   2.219  0.02647 *  \nsdp            0.018146   0.006435   2.820  0.00481 ** \ndbp           -0.000916   0.010903  -0.084  0.93305    \nchol           0.010726   0.001531   7.006 2.45e-12 ***\nbehaveA2       0.082920   0.222909   0.372  0.70990    \nbehaveB3      -0.618013   0.245032  -2.522  0.01166 *  \nbehaveB4      -0.487224   0.321325  -1.516  0.12944    \ncigs           0.021036   0.004298   4.895 9.84e-07 ***\ndibepB               NA         NA      NA       NA    \narcuspresent   0.212796   0.143915   1.479  0.13924    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1769.2  on 3139  degrees of freedom\nResidual deviance: 1569.1  on 3128  degrees of freedom\n  (14 observations deleted due to missingness)\nAIC: 1593.1\n\nNumber of Fisher Scoring iterations: 6",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#whats-going-on-with-dibep",
    "href": "slides/03-logistic-regression-2.html#whats-going-on-with-dibep",
    "title": "Logistic Regression (Part II)",
    "section": "What‚Äôs going on with dibep?",
    "text": "What‚Äôs going on with dibep?\nLet‚Äôs inspect the data a bit more; we‚Äôll start with a SPLOM\n\n\nShow R code\ny &lt;- ifelse(wcgs$chd == \"yes\", 1, 0)\npairs(wcgs, col = adjustcolor(y + 1, alpha.f = 0.1))",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#check-for-nas",
    "href": "slides/03-logistic-regression-2.html#check-for-nas",
    "title": "Logistic Regression (Part II)",
    "section": "Check for NAs",
    "text": "Check for NAs\nThe lapply() function (and friends) are quite useful!\n\n\nShow R code\n# Which columns contain missing values?\nsapply(wcgs, FUN = function(column) mean(is.na(column)))\n\n\n         age       height       weight          sdp          dbp         chol \n0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0038046925 \n      behave         cigs        dibep          chd        arcus \n0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0006341154",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#measures-of-association-how-to-choose",
    "href": "slides/03-logistic-regression-2.html#measures-of-association-how-to-choose",
    "title": "Logistic Regression (Part II)",
    "section": "Measures of Association: How to Choose?1",
    "text": "Measures of Association: How to Choose?1\n\nNice little paper from Harry Khamis, my old graduate advisor at WSU.",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#check-pairwise-correlations",
    "href": "slides/03-logistic-regression-2.html#check-pairwise-correlations",
    "title": "Logistic Regression (Part II)",
    "section": "Check pairwise correlations",
    "text": "Check pairwise correlations\nOnly looking at numeric columns:\n\n\nShow R code\n# Look at correlations between numeric features\nnum &lt;- sapply(wcgs, FUN = is.numeric)  # identify numeric columns\n(corx &lt;- cor(wcgs[, num], use = \"pairwise.complete.obs\"))  # simple correlation matrix\n\n\n                age      height       weight        sdp         dbp\nage     1.000000000 -0.09537568 -0.034404537 0.16574640  0.13919776\nheight -0.095375682  1.00000000  0.532935466 0.01837357  0.01027555\nweight -0.034404537  0.53293547  1.000000000 0.25324962  0.29592019\nsdp     0.165746397  0.01837357  0.253249623 1.00000000  0.77290641\ndbp     0.139197757  0.01027555  0.295920186 0.77290641  1.00000000\nchol    0.089188510 -0.08893778  0.008537442 0.12306130  0.12959711\ncigs   -0.005033852  0.01491129 -0.081747507 0.02997753 -0.05934232\n               chol         cigs\nage     0.089188510 -0.005033852\nheight -0.088937779  0.014911292\nweight  0.008537442 -0.081747507\nsdp     0.123061297  0.029977529\ndbp     0.129597108 -0.059342317\nchol    1.000000000  0.096031834\ncigs    0.096031834  1.000000000",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#check-pairwise-correlations-1",
    "href": "slides/03-logistic-regression-2.html#check-pairwise-correlations-1",
    "title": "Logistic Regression (Part II)",
    "section": "Check pairwise correlations",
    "text": "Check pairwise correlations\nOnly looking at numeric columns:\n\n\nShow R code\ncorrplot::corrplot(corx, method = \"square\", order = \"FPC\", type = \"lower\", diag = TRUE)",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#check-pairwise-correlations-2",
    "href": "slides/03-logistic-regression-2.html#check-pairwise-correlations-2",
    "title": "Logistic Regression (Part II)",
    "section": "Check pairwise correlations",
    "text": "Check pairwise correlations\nPairwise scatterplots with LOWESS smoothers:\n\n\nShow R code\npar(mfrow = c(1, 2))\nplot(weight ~ height, data = wcgs, col = adjustcolor(1, alpha.f = 0.4))\nlines(lowess(x=wcgs$height, y=wcgs$weight), lwd = 2, col = 2)\nplot(dbp ~ sdp, data = wcgs, col = adjustcolor(1, alpha.f = 0.4))\nlines(lowess(wcgs$sdp, y = wcgs$dbp), lwd = 2, col = 2)",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#what-about-categorical-variables",
    "href": "slides/03-logistic-regression-2.html#what-about-categorical-variables",
    "title": "Logistic Regression (Part II)",
    "section": "What about categorical variables?",
    "text": "What about categorical variables?\nContingency table cross-classifying dibep and behave:\n\n\nShow R code\n# What about categorical features?\nxtabs(~ behave + dibep, data = wcgs)  # perfect correlation?\n\n\n      dibep\nbehave    A    B\n    A1  264    0\n    A2 1325    0\n    B3    0 1216\n    B4    0  349",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#so-far",
    "href": "slides/03-logistic-regression-2.html#so-far",
    "title": "Logistic Regression (Part II)",
    "section": "So far‚Ä¶",
    "text": "So far‚Ä¶\n\nAs expected, looks like there‚Äôs moderate positive correlation between\n\nsdp and dbp\nheight and weight\n\nNot necessarily a problem (yet). But how could potentially fix any issues?\nAlso looks like some redunancy between the categorical variables dibep and behave",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#looking-more-closely-at-dibep",
    "href": "slides/03-logistic-regression-2.html#looking-more-closely-at-dibep",
    "title": "Logistic Regression (Part II)",
    "section": "Looking more closely at dibep",
    "text": "Looking more closely at dibep\nTry a decision tree:\n\n\nShow R code\n# What about categorical features?\nrpart::rpart(dibep ~ ., data = wcgs)  # perfect correlation?\n\n\nn= 3154 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 3154 1565 A (0.5038047 0.4961953)  \n  2) behave=A1,A2 1589    0 A (1.0000000 0.0000000) *\n  3) behave=B3,B4 1565    0 B (0.0000000 1.0000000) *\n\n\n\nLooks like dipep can be predicted perfectly from behave (i.e., they are redundant)",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#redundancy-analysis",
    "href": "slides/03-logistic-regression-2.html#redundancy-analysis",
    "title": "Logistic Regression (Part II)",
    "section": "Redundancy analysis",
    "text": "Redundancy analysis\nRedunancy analysis is a powerful tool available in the Hmisc package:\n\n\nShow R code\n# Notice we're ignoring the response here!\nHmisc::redun(~ . - chd, nk = 0, data = wcgs)\n\n\n\nRedundancy Analysis\n\n~age + height + weight + sdp + dbp + chol + behave + cigs + dibep + \n    arcus\n&lt;environment: 0x1480da710&gt;\n\nn: 3140     p: 10   nk: 0 \n\nNumber of NAs:   0 \n\nTransformation of target variables forced to be linear\n\nR-squared cutoff: 0.9   Type: ordinary \n\nR^2 with which each variable can be predicted from all other variables:\n\n   age height weight    sdp    dbp   chol behave   cigs  dibep  arcus \n 0.083  0.323  0.379  0.606  0.617  0.054  1.000  0.050  1.000  0.055 \n\nRendundant variables:\n\ndibep\n\n\nPredicted from variables:\n\nage height weight sdp dbp chol behave cigs arcus\n\n  Variable Deleted R^2 R^2 after later deletions\n1            dibep   1",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#battery-target",
    "href": "slides/03-logistic-regression-2.html#battery-target",
    "title": "Logistic Regression (Part II)",
    "section": "Battery target",
    "text": "Battery target\nCool idea from Salford Systems back in the day (now part of Minitab). Think of it as VIFs and redundancy analysis on steroids!\n\n\nSource",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#full-model-again",
    "href": "slides/03-logistic-regression-2.html#full-model-again",
    "title": "Logistic Regression (Part II)",
    "section": "Full model (again)",
    "text": "Full model (again)\nThis time we‚Äôve removed both the leakage and redundant predictors:\n\n\nShow R code\n# Refit model without leakage or redundant features\nsummary(lr.fit.all &lt;- glm(chd ~ . - dibep, family = binomial(link = \"logit\"), data = wcgs))\n\n\n\nCall:\nglm(formula = chd ~ . - dibep, family = binomial(link = \"logit\"), \n    data = wcgs)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -12.331126   2.350347  -5.247 1.55e-07 ***\nage            0.061812   0.012421   4.977 6.47e-07 ***\nheight         0.006903   0.033335   0.207  0.83594    \nweight         0.008637   0.003892   2.219  0.02647 *  \nsdp            0.018146   0.006435   2.820  0.00481 ** \ndbp           -0.000916   0.010903  -0.084  0.93305    \nchol           0.010726   0.001531   7.006 2.45e-12 ***\nbehaveA2       0.082920   0.222909   0.372  0.70990    \nbehaveB3      -0.618013   0.245032  -2.522  0.01166 *  \nbehaveB4      -0.487224   0.321325  -1.516  0.12944    \ncigs           0.021036   0.004298   4.895 9.84e-07 ***\narcuspresent   0.212796   0.143915   1.479  0.13924    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1769.2  on 3139  degrees of freedom\nResidual deviance: 1569.1  on 3128  degrees of freedom\n  (14 observations deleted due to missingness)\nAIC: 1593.1\n\nNumber of Fisher Scoring iterations: 6",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#variance-inflation-factors-vifs",
    "href": "slides/03-logistic-regression-2.html#variance-inflation-factors-vifs",
    "title": "Logistic Regression (Part II)",
    "section": "Variance inflation factors (VIFs)",
    "text": "Variance inflation factors (VIFs)\nVIFs aren‚Äôt built into base R, so here we‚Äôll use the car package:\n\n\nShow R code\n# Check (generalized) VIFs\ncar::vif(lr.fit.all)\n\n\n           GVIF Df GVIF^(1/(2*Df))\nage    1.097698  1        1.047711\nheight 1.479064  1        1.216168\nweight 1.603473  1        1.266283\nsdp    2.656210  1        1.629788\ndbp    2.796994  1        1.672421\nchol   1.029458  1        1.014622\nbehave 1.030106  3        1.004956\ncigs   1.054454  1        1.026866\narcus  1.060879  1        1.029990\n\n\n\nDoes anyone recall how VIFs are computed? Try running this with dibep still in the model!",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#body-mass-index-fheight-weight",
    "href": "slides/03-logistic-regression-2.html#body-mass-index-fheight-weight",
    "title": "Logistic Regression (Part II)",
    "section": "Body mass index = f(height, weight)",
    "text": "Body mass index = f(height, weight)\n\nFeature engineering is useful in cases where it makes sense!\nNot sure we can combine sdb and dbp in any useful way?!\nheight and weight can be combined into a single number called body mass index (BMI): \\(\\text{BMI} = \\frac{\\text{mass}_\\text{lb}}{\\text{height}_\\text{in}^2} \\times 703\\)\n\n\n\nShow R code\nwcgs$bmi &lt;- with(wcgs, 703 * weight / (height^2))\nhead(wcgs[, c(\"height\", \"weight\", \"bmi\")])",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#full-model-again-again",
    "href": "slides/03-logistic-regression-2.html#full-model-again-again",
    "title": "Logistic Regression (Part II)",
    "section": "Full model (again again)",
    "text": "Full model (again again)\nThis time, we‚Äôll remove sdb, height, and weight, and include bmi:\n\n\nShow R code\nsummary(lr.fit.all &lt;- update(lr.fit.all, formula = . ~ . + bmi - sdp - height - weight))\n\n\n\nCall:\nglm(formula = chd ~ age + dbp + chol + behave + cigs + arcus + \n    bmi, family = binomial(link = \"logit\"), data = wcgs)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -11.500594   1.026209 -11.207  &lt; 2e-16 ***\nage            0.062802   0.012227   5.136 2.80e-07 ***\ndbp            0.022652   0.007073   3.203  0.00136 ** \nchol           0.010520   0.001507   6.982 2.90e-12 ***\nbehaveA2       0.135021   0.222357   0.607  0.54370    \nbehaveB3      -0.586373   0.244718  -2.396  0.01657 *  \nbehaveB4      -0.469271   0.320962  -1.462  0.14372    \ncigs           0.022668   0.004255   5.327 9.99e-08 ***\narcuspresent   0.223473   0.143358   1.559  0.11903    \nbmi            0.060327   0.027311   2.209  0.02718 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1769.2  on 3139  degrees of freedom\nResidual deviance: 1579.9  on 3130  degrees of freedom\n  (14 observations deleted due to missingness)\nAIC: 1599.9\n\nNumber of Fisher Scoring iterations: 6",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#backward-elimination",
    "href": "slides/03-logistic-regression-2.html#backward-elimination",
    "title": "Logistic Regression (Part II)",
    "section": "Backward elimination",
    "text": "Backward elimination\n\nStepwise procedures work the same here as they did for ordinary linear models\nWhile base R has the step() function, the stepAIC() function from package MASS is a bit better:\n\n\n\nShow R code\nsummary(lr.fit.back &lt;- MASS::stepAIC(lr.fit.all, direction = \"backward\", trace = 0))\n\n\n\nCall:\nglm(formula = chd ~ age + dbp + chol + behave + cigs + arcus + \n    bmi, family = binomial(link = \"logit\"), data = wcgs)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -11.500594   1.026209 -11.207  &lt; 2e-16 ***\nage            0.062802   0.012227   5.136 2.80e-07 ***\ndbp            0.022652   0.007073   3.203  0.00136 ** \nchol           0.010520   0.001507   6.982 2.90e-12 ***\nbehaveA2       0.135021   0.222357   0.607  0.54370    \nbehaveB3      -0.586373   0.244718  -2.396  0.01657 *  \nbehaveB4      -0.469271   0.320962  -1.462  0.14372    \ncigs           0.022668   0.004255   5.327 9.99e-08 ***\narcuspresent   0.223473   0.143358   1.559  0.11903    \nbmi            0.060327   0.027311   2.209  0.02718 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1769.2  on 3139  degrees of freedom\nResidual deviance: 1579.9  on 3130  degrees of freedom\n  (14 observations deleted due to missingness)\nAIC: 1599.9\n\nNumber of Fisher Scoring iterations: 6",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#forward-selection",
    "href": "slides/03-logistic-regression-2.html#forward-selection",
    "title": "Logistic Regression (Part II)",
    "section": "Forward selection",
    "text": "Forward selection\nLet‚Äôs assume we know cigs is relevant for predicting chd (regardless of its statistical significance). So we start with that in the model:\n\n\nShow R code\n# Variable selection using forward selection with AIC; which variables were added?\nm &lt;- glm(chd ~ cigs, data = wcgs, \n         family = binomial(link = \"logit\"))\n(lr.fit.forward &lt;- MASS::stepAIC(m, direction = \"forward\", trace = 0))\n\n\n\nCall:  glm(formula = chd ~ cigs, family = binomial(link = \"logit\"), \n    data = wcgs)\n\nCoefficients:\n(Intercept)         cigs  \n   -2.74216      0.02322  \n\nDegrees of Freedom: 3153 Total (i.e. Null);  3152 Residual\nNull Deviance:      1781 \nResidual Deviance: 1750     AIC: 1754",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#regularized-regression",
    "href": "slides/03-logistic-regression-2.html#regularized-regression",
    "title": "Logistic Regression (Part II)",
    "section": "Regularized regression",
    "text": "Regularized regression\n\nRegression coefficients are estimated under various constraints\nMost common approaches include:\n\nRidge regression, which can be useful when dealing with multicollinearity\nLASSO, which can be useful for variable selection\nElastic net (ENet) \\(\\approx\\) Ridge + LASSO\n\nThe glmnet package in R, among others, can fit the entire regularization path for many kinds of models, including GLMs and the Cox PH model",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#useful-resources",
    "href": "slides/03-logistic-regression-2.html#useful-resources",
    "title": "Logistic Regression (Part II)",
    "section": "Useful resources",
    "text": "Useful resources\n\nSee Section 6.2 of ISL book (FREE!!)\nMy HOMLR book with Brad (FREE!!)\nNice intro video (Python):",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#enet-fit-to-wcgs-data",
    "href": "slides/03-logistic-regression-2.html#enet-fit-to-wcgs-data",
    "title": "Logistic Regression (Part II)",
    "section": "ENet fit to wcgs data",
    "text": "ENet fit to wcgs data\n\n\nShow R code\nlibrary(glmnet)\n\n# Fit an elastic net model (i.e., LASSO and ridge penalties) using 5-fold CV\nwcgs.complete &lt;- na.omit(wcgs)\nX &lt;- model.matrix(~. - chd - dibep - bmi - 1 , data = wcgs.complete)\n#lr.enet &lt;- cv.glmnet(X, y = ifelse(wcgs.complete$chd == \"yes\", 1, 0), \n#                     family = \"binomial\", nfold = 5, keep = TRUE)\nlr.enet &lt;- glmnet(X, y = ifelse(wcgs.complete$chd == \"yes\", 1, 0), \n                  family = \"binomial\")\nplot(lr.enet, label = TRUE, xvar = \"lambda\")",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#enet-fit-to-wcgs-data-1",
    "href": "slides/03-logistic-regression-2.html#enet-fit-to-wcgs-data-1",
    "title": "Logistic Regression (Part II)",
    "section": "ENet fit to wcgs data",
    "text": "ENet fit to wcgs data\n\n\nShow R code\nlibrary(glmnet)\n\n# Fit an elastic net model (i.e., LASSO and ridge penalties) using 5-fold CV\nlr.enet &lt;- cv.glmnet(X, y = ifelse(wcgs.complete$chd == \"yes\", 1, 0), \n                     family = \"binomial\", nfold = 5, keep = TRUE)\nplot(lr.enet)",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#enet-fit-to-wcgs-data-2",
    "href": "slides/03-logistic-regression-2.html#enet-fit-to-wcgs-data-2",
    "title": "Logistic Regression (Part II)",
    "section": "ENet fit to wcgs data",
    "text": "ENet fit to wcgs data\n\n\nShow R code\ncoef(lr.enet)\n\n\n13 x 1 sparse Matrix of class \"dgCMatrix\"\n               lambda.1se\n(Intercept)  -5.557386134\nage           0.016230481\nheight        .          \nweight        .          \nsdp           0.008155035\ndbp           .          \nchol          0.005476254\nbehaveA1      .          \nbehaveA2      0.017441706\nbehaveB3      .          \nbehaveB4      .          \ncigs          0.003618991\narcuspresent  .",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#prediction-accuracy",
    "href": "slides/03-logistic-regression-2.html#prediction-accuracy",
    "title": "Logistic Regression (Part II)",
    "section": "Prediction accuracy",
    "text": "Prediction accuracy\n\nScoring rules are used to evaluate probabilistic predictions\nA scoring rule is proper if it is minimized in expectation by the true probability\nIt is a metric that is optimized when the forecasted probabilities are identical to the true outcome probabilities\nSee Gneiting & Raftery (2007, JASA) and this post for details\nExamples include log loss and the Brier score (or MSE)",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#statistical-classification",
    "href": "slides/03-logistic-regression-2.html#statistical-classification",
    "title": "Logistic Regression (Part II)",
    "section": "Statistical classification",
    "text": "Statistical classification\n\nA classifier is a model that outputs a class label, as opposed to a probabilistic prediction\nLogistic regression is NOT a classifier!!\n\nBinary classification via logistic regression represents a forced choice based on a probability threshold\n\nClassification is rarely useful for decision making (think about a weather app that only prodiced classifications and not forecasts!)",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#classification-boundary",
    "href": "slides/03-logistic-regression-2.html#classification-boundary",
    "title": "Logistic Regression (Part II)",
    "section": "Classification boundary",
    "text": "Classification boundary\nPerfect seperation (or discrimination):\n\n\nShow R code\n# Simulate some data\nN &lt;- 200\nd1 &lt;- cbind(matrix(MASS::mvrnorm(2*N, mu = c(0, 0), Sigma = diag(2)), ncol = 2), 0)\nd2 &lt;- cbind(matrix(MASS::mvrnorm(2*N, mu = c(8, 8), Sigma = diag(2)), ncol = 2), 1)\nd &lt;- as.data.frame(rbind(d1, d2))\nnames(d) &lt;- c(\"x1\", \"x2\", \"y\")\n\n# Fit a logistic regression\nfit &lt;- glm(y ~ ., data = d, family = binomial)\n\n# Plot decision boundary using 0.5 threshold\npfun &lt;- function(object, newdata) {\n  prob &lt;- predict(object, newdata = newdata, type = \"response\")\n  label &lt;- ifelse(prob &gt; 0.5, 1, 0)  # force into class label\n  label\n}\nplot(x2 ~ x1, data = d, col = d$y + 1)\ntreemisc::decision_boundary(fit, train = d, y = \"y\", x1 = \"x1\", x2 = \"x2\", \n                            pfun = pfun, grid.resolution = 999)\nlegend(\"topleft\", legend = c(\"y = 0\", \"y = 1\"), col = c(1, 2), pch = 1)",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#classification-boundary-1",
    "href": "slides/03-logistic-regression-2.html#classification-boundary-1",
    "title": "Logistic Regression (Part II)",
    "section": "Classification boundary",
    "text": "Classification boundary\nClass overlap (four possibilities in terms of classification):\n\n\nShow R code\n# Simulate some data\nN &lt;- 200\nd1 &lt;- cbind(matrix(MASS::mvrnorm(2*N, mu = c(0, 0), Sigma = diag(2)), ncol = 2), 0)\nd2 &lt;- cbind(matrix(MASS::mvrnorm(2*N, mu = c(2, 2), Sigma = diag(2)), ncol = 2), 1)\nd &lt;- as.data.frame(rbind(d1, d2))\nnames(d) &lt;- c(\"x1\", \"x2\", \"y\")\n\n# Fit a logistic regression\nfit &lt;- glm(y ~ ., data = d, family = binomial)\n\n# Plot decision boundary using 0.5 threshold\npfun &lt;- function(object, newdata) {\n  prob &lt;- predict(object, newdata = newdata, type = \"response\")\n  label &lt;- ifelse(prob &gt; 0.5, 1, 0)  # force into class label\n  label\n}\nplot(x2 ~ x1, data = d, col = d$y + 1)\ntreemisc::decision_boundary(fit, train = d, y = \"y\", x1 = \"x1\", x2 = \"x2\", \n                            pfun = pfun, grid.resolution = 999)\nlegend(\"topleft\", legend = c(\"y = 0\", \"y = 1\"), col = c(1, 2), pch = 1)",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#confusion-matrix",
    "href": "slides/03-logistic-regression-2.html#confusion-matrix",
    "title": "Logistic Regression (Part II)",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\nA confusion matrix is a special contingency table that describes the performance of a binary classifier\nMore of a matrix of confusion üò±\nLots of statistics can be computed from a given confusion matrix\n\nThey are all improper scoring rules and can be optimized by a bogus model\nMost are not useful for decision making IMO",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#example-with-wcgs-data",
    "href": "slides/03-logistic-regression-2.html#example-with-wcgs-data",
    "title": "Logistic Regression (Part II)",
    "section": "Example with wcgs data",
    "text": "Example with wcgs data\n\n\nShow R code\n# Confusion matrix (i.e., 2x2 contingency table of classification results)\ny &lt;- na.omit(wcgs)$chd  # observed classes\nprob &lt;- predict(lr.fit.all, type = \"response\")  # predicted probabilities\nclasses &lt;- ifelse(prob &gt; 0.5, \"yes\", \"no\")  # classification based on 0.5 threshold\n(cm &lt;- table(\"actual\" = y, \"predicted\" = classes))  # confusion matrix\n\n\n      predicted\nactual   no  yes\n   no  2883    2\n   yes  253    2",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#confusion-matrix-1",
    "href": "slides/03-logistic-regression-2.html#confusion-matrix-1",
    "title": "Logistic Regression (Part II)",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n Source",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#roc-curves",
    "href": "slides/03-logistic-regression-2.html#roc-curves",
    "title": "Logistic Regression (Part II)",
    "section": "ROC curves",
    "text": "ROC curves\n\nReceiver operating characteristic (ROC) curves display the tradeoff between the true positive rate (TPR or sensitivity) and false positive rate (FPR or 1 - specificity) for a range of probability thresholds\nInvariant to monotone transformations of \\(\\hat{p}\\)\nPrecision-recall plots can be more informative when dealing with class imbalance (really not a problem with logistic regression or when dealing with probabilities)",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#transposed-conditionals",
    "href": "slides/03-logistic-regression-2.html#transposed-conditionals",
    "title": "Logistic Regression (Part II)",
    "section": "Transposed conditionals",
    "text": "Transposed conditionals\n\nConfusion of the inverse: \\(P\\left(A|B\\right) \\ne P\\left(B|A\\right)\\)\nThe error of the transposed conditional is rampant in research:\n\n‚ÄúConditioning on what is unknowable to predict what is already known leads to a host of complexities and interpretation problems.‚Äù\n\nTPR and FPR (and others) are transposed conditionals \\[\n\\begin{align}\nTPR &= P\\left(\\hat{Y} = 1 | Y = 1\\right) \\\\\n&= P\\left(\\text{known} | \\text{unknown}\\right)\n\\end{align}\n\\]",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#roc-curve-by-hand",
    "href": "slides/03-logistic-regression-2.html#roc-curve-by-hand",
    "title": "Logistic Regression (Part II)",
    "section": "ROC curve (by hand)",
    "text": "ROC curve (by hand)\n\n\nShow R code\nthreshold &lt;- seq(from = 0, to = 1, length = 999)\ntp &lt;- tn &lt;- fp &lt;- fn &lt;- numeric(length(threshold))\nfor (i in seq_len(length(threshold))) {\n  classes &lt;- ifelse(prob &gt; threshold[i], \"yes\", \"no\")\n  tp[i] &lt;- sum(classes == \"yes\" & y == \"yes\")  # true positives\n  tn[i] &lt;- sum(classes == \"no\"  & y == \"no\")  # true negatives\n  fp[i] &lt;- sum(classes == \"yes\" & y == \"no\")  # false positives\n  fn[i] &lt;- sum(classes == \"no\"  & y == \"yes\")  # false negatives\n}\ntpr &lt;- tp / (tp + fn)  # sensitivity\ntnr &lt;- tn / (tn + fp)  # specificity\n\n# Plot ROC curve\nplot(tnr, y = tpr, type = \"l\", col = 2, lwd = 2, xlab = \"TNR (or specificity)\", \n     ylab = \"TPR (or sensitivity)\")\nabline(1, -1, lty = 2)",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#roc-curve-proc-package",
    "href": "slides/03-logistic-regression-2.html#roc-curve-proc-package",
    "title": "Logistic Regression (Part II)",
    "section": "ROC curve (pROC package)",
    "text": "ROC curve (pROC package)\nCan be useful to use a package sometimes (e.g., for computing are under the ROC curve; AKA AUROC or AUC)\n\n\nShow R code\nplot(roc &lt;- pROC::roc(y, predictor = prob))\n\n\n\nShow R code\nroc\n\n\n\nCall:\nroc.default(response = y, predictor = prob)\n\nData: prob in 2885 controls (y no) &lt; 255 cases (y yes).\nArea under the curve: 0.751",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#leave-one-covariate-out-loco-importance",
    "href": "slides/03-logistic-regression-2.html#leave-one-covariate-out-loco-importance",
    "title": "Logistic Regression (Part II)",
    "section": "Leave-one-covariate-out (LOCO) importance",
    "text": "Leave-one-covariate-out (LOCO) importance\nA simple and intuitive way to measure the ‚Äúimportance‚Äù of each covariate in a model. In the simplest terms:\n\nEstimate baseline performance (e.g., AUROC or Brier score)\nFor \\(j = 1, 2, \\dots p\\), refit the model without feature \\(x_j\\) and compute the degredation to the baseline performance.\nSort these values in a table or plot them.\n\nSee here for more details.",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#loco-scores-for-wcgs-example",
    "href": "slides/03-logistic-regression-2.html#loco-scores-for-wcgs-example",
    "title": "Logistic Regression (Part II)",
    "section": "LOCO scores for wcgs example",
    "text": "LOCO scores for wcgs example\n\n\nShow R code\nwcgs &lt;- faraway::wcgs\nwcgs$bmi &lt;- with(wcgs, weight / (height^2) * 703)\nomit &lt;- c(\"typechd\", \"timechd\", \"dibep\", \"height\", \"weight\")\nkeep &lt;- setdiff(names(wcgs), y = omit)\nwcgs &lt;- wcgs[, keep]\nfit &lt;- glm(chd ~ ., data = wcgs, family = binomial)\nsummary(fit)\n\n\n\nCall:\nglm(formula = chd ~ ., family = binomial, data = wcgs)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.168e+01  1.026e+00 -11.387  &lt; 2e-16 ***\nage           5.922e-02  1.233e-02   4.804 1.56e-06 ***\nsdp           1.806e-02  6.435e-03   2.807   0.0050 ** \ndbp          -3.193e-04  1.087e-02  -0.029   0.9766    \nchol          1.047e-02  1.520e-03   6.889 5.64e-12 ***\nbehaveA2      8.498e-02  2.229e-01   0.381   0.7030    \nbehaveB3     -6.238e-01  2.449e-01  -2.547   0.0109 *  \nbehaveB4     -4.994e-01  3.211e-01  -1.555   0.1199    \ncigs          2.139e-02  4.294e-03   4.982 6.31e-07 ***\narcuspresent  2.238e-01  1.437e-01   1.558   0.1193    \nbmi           5.880e-02  2.717e-02   2.164   0.0305 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1769.2  on 3139  degrees of freedom\nResidual deviance: 1572.3  on 3129  degrees of freedom\n  (14 observations deleted due to missingness)\nAIC: 1594.3\n\nNumber of Fisher Scoring iterations: 6\n\n\nShow R code\n# Leave-one-covariate-out (LOCO) method\n#\n# Note: Would be better to incorporate some form of cross-validation (or bootstrap)\nx.names &lt;- attr(fit$terms, \"term.labels\")\nloco &lt;- numeric(length(x.names))\nbaseline &lt;- deviance(fit)  # smaller is better; could also use AUROC, Brier score, etc.\nloco &lt;- sapply(x.names, FUN = function(x.name) {\n  wcgs.copy &lt;- wcgs\n  wcgs.copy[[x.name]] &lt;- NULL\n  fit.new &lt;- glm(chd ~ ., data = wcgs.copy, family = binomial(link = \"logit\"))\n  deviance(fit.new) - baseline  # measure drop in performance\n})\nnames(loco) &lt;- x.names\nsort(loco, decreasing = TRUE)\n\n\n        chol         cigs          age       behave        arcus          sdp \n5.201189e+01 2.385372e+01 2.298085e+01 2.233397e+01 1.203822e+01 7.647463e+00 \n         bmi          dbp \n4.630518e+00 8.624341e-04 \n\n\nShow R code\ndotchart(sort(loco, decreasing = TRUE), pch = 19)",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#lift-charts",
    "href": "slides/03-logistic-regression-2.html#lift-charts",
    "title": "Logistic Regression (Part II)",
    "section": "Lift charts",
    "text": "Lift charts\n\nClassification is a forced choice!\nIn marketing, analysts generally know better than to try to classify a potential customer as someone to ignore or someone to spend resources on\n\nInstead, potential customers are sorted in decreasing order of estimated probability of purchasing a product\nThe marketer who can afford to advertise to \\(n\\) persons then picks the \\(n\\) highest-probability customers as targets!\n\nI like the idea of cumulative gain charts for this!",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#lift-charts-1",
    "href": "slides/03-logistic-regression-2.html#lift-charts-1",
    "title": "Logistic Regression (Part II)",
    "section": "Lift charts",
    "text": "Lift charts\nCumulative gains chart applied to wcgs example (lr.fit.all):\n\n\nShow R code\nres &lt;- treemisc::lift(prob, y = y, pos.class = \"yes\")\nplot(res)",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#probability-calibration",
    "href": "slides/03-logistic-regression-2.html#probability-calibration",
    "title": "Logistic Regression (Part II)",
    "section": "Probability calibration",
    "text": "Probability calibration\n\nA probability \\(p\\) is well calibrated if a fraction of about p of the events we predict with probability \\(p\\) actually occur\nCalibration curves are the ü•á gold standard ü•á\nCompared to general machine learning models, logistic regression tends to return well calibrated probabilities\nFor details, see Niculescu-Mizil & Caruana (2005) and Kull et al.¬†(2017) and",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  },
  {
    "objectID": "slides/03-logistic-regression-2.html#probability-calibration-1",
    "href": "slides/03-logistic-regression-2.html#probability-calibration-1",
    "title": "Logistic Regression (Part II)",
    "section": "Probability calibration",
    "text": "Probability calibration\nThe rms function val.prob() can be used for this:\n\n\nShow R code\nwcgs2 &lt;- na.omit(faraway::wcgs)\ny &lt;- ifelse(wcgs2$chd == \"yes\", 1, 0)\nfit &lt;- glm(y ~ cigs + height, data = wcgs2, family = binomial)\nprob &lt;- predict(fit, newdata = wcgs2, type = \"response\")\nrms::val.prob(prob, y = y)",
    "crumbs": [
      "Logistic Regression (Part II)"
    ]
  }
]