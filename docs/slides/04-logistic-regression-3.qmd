---
title: "Logistic Regression (Part III)"
author: "Brandon M. Greenwell, PhD"
institute: "University of Cincinnati"
from: markdown+emoji
format: 
    revealjs:
        df-print: paged
        code-fold: true
        code-summary: "Show R code"
        code-copy: true
        logo: images/uc.png
        chalkboard: true
        slide-number: true
        scrollable: true
        embed-resources: false
        footer: "BANA 7042: Statistical Modeling"
---

```{r, include=FALSE}
palette("Okabe-Ito")

library(knitr)

opts_chunk$set(
  echo = TRUE,
  dpi = 300,     
  #fig.retina = 3,  
  fig.align = "center",
  fig.width = 6,
  fig.asp = 0.618,
  out.width = "100%",
 # size = "small",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

knit_hooks$set(
  par = function(before, options, envir) {
    if (before && options$fig.show != "none") {
      par(
        mar = c(4, 4, 2, 0.1), 
        cex.lab = 0.95, 
        cex.axis = 0.8,  # was 0.9
        mgp = c(2, 0.7, 0), 
        tcl = -0.3, 
        las = 1
      )
      if (is.list(options$par)) {
        do.call(par, options$par)
      }
    }
  }
)
```


# Variations of logistic regression

## Logistic regression

* In logistic regression, we use the [logit](https://en.wikipedia.org/wiki/Logit):
$$
\text{logit}\left(p\right) = \boldsymbol{x}^\top\boldsymbol{\beta} = \eta
$$
which results in
$$
p = \left[1 + \exp\left(-\eta\right)\right]^{-1} = F\left(\eta\right)
$$

* Technically, $F$ can be any [*monotonic*](https://en.wikipedia.org/wiki/Monotonic_function) function that maps $\eta$ to $\left[0, 1\right]$ (e.g., any [CDF](https://en.wikipedia.org/wiki/Cumulative_distribution_function) will work)

* $F^{-1}$ is called the [link function](https://en.wikipedia.org/wiki/Generalized_linear_model)


## The probit model

* The [probit model](https://en.wikipedia.org/wiki/Probit_model) uses $F\left(\eta\right) = \Phi\left(\eta\right)$ (i.e., the CDF of a [standard normal distribution](https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution))
$$
P\left(Y = 1 | \boldsymbol{x}\right) = \Phi\left(\beta_0 + \beta_1x_1 + \cdots\right) = \Phi\left(\boldsymbol{x}^\top\boldsymbol{\beta}\right)
$$

* $F^{-1} = \Phi^{-1}$ is called the [probit link](https://en.wikipedia.org/wiki/Probit), which yields
$$
\text{probit}\left(p\right) = \Phi^{-1}\left(p\right) = \boldsymbol{x}^\top\boldsymbol{\beta}
$$

* The term probit is short for [**prob**]{style="color: green;"}ability un[**it**]{style="color: green;"}

* Proposed in [Bliss (1934)](https://www.science.org/doi/10.1126/science.79.2037.38) for analyzing data such as the percentage of a pest killed by a pesticide


## Dobson's beetle data

The data give the number of flour beetles killed after five hour exposure to the insecticide carbon disulphide at eight different concentrations.
```{r}
#| par: true
beetle <- investr::beetle
fit <- glm(cbind(y, n-y) ~ ldose, data = beetle, 
           family = binomial(link = "probit"))
investr::plotFit(
  fit, pch = 19, cex = 1.2, lwd = 2, 
  xlab = "Log dose of carbon disulphide", ylab = "Proportion killed",
  interval = "confidence", shade = TRUE, col.conf = "lightskyblue"
)
```


## Other link functions

Common link function for binary regression include:

* Logit (most common and the default an most software)
* Probit (next most common)
* Log-log
* Complimentary log-log
* Cauchit


## Application to wcgs data set

Comparing coefficients from different link functions:
```{r}
wcgs <- na.omit(subset(faraway::wcgs, select = -c(typechd, timechd, behave)))

# Fit binary GLMs with different link functions
fit.logit <- glm(chd ~ ., data = wcgs, family = binomial(link = "logit"))
fit.probit <- glm(chd ~ ., data = wcgs, family = binomial(link = "probit"))
fit.cloglog <- glm(chd ~ ., data = wcgs, family = binomial(link = "cloglog"))
fit.cauchit <- glm(chd ~ ., data = wcgs, family = binomial(link = "cauchit"))

# Compare coefficients
coefs <- cbind(
  "logit" = coef(fit.logit),
  "probit" = coef(fit.probit),
  "cloglog" = coef(fit.cloglog),
  "cauchit" = coef(fit.cauchit)
)
round(coefs, digits = 3)
```


## Application to wcgs data set

Comparing fitted values from different link functions:
```{r}
# Compare fitted values (i.e., predicted probabilities)
preds <- cbind(
  "logit" = fitted(fit.logit),
  "probit" = fitted(fit.probit),
  "cloglog" = fitted(fit.cloglog),
  "cauchit" = fitted(fit.cauchit)
)
head(round(preds, digits = 3))
```


## Application to wcgs data set

Comparing fitted values from different link functions:
```{r}
#| par: true
plot(sort(preds[, "logit"]), type = "l", ylab = "Fitted value")
lines(sort(preds[, "probit"]), col = 2)
lines(sort(preds[, "cloglog"]), col = 3)
lines(sort(preds[, "cauchit"]), col = 4)
legend("topleft", legend = c("logit", "probit", "cloglog", "cauchit"), 
       col = 1:4, lty = 1, inset = 0.01)
```


## As a latent variable model

* Logistic regression (and the other link functions) has an equivalent formulation as a [latent variable model](https://en.wikipedia.org/wiki/Latent_variable_model)

* Consider a linear model with continuous outcome $Y^\star = \boldsymbol{x}^\top\boldsymbol{\beta} + \epsilon$

* Imagine that we can only observe the binary variable 
$$
Y = \begin{cases}
  1 \quad \text{if } Y^\star > 0, \\
  0 \quad \text{otherwise}
\end{cases}
$$

* Assuming $\epsilon \sim \text{Logistic}\left(0, 1\right)$ leads to the usual logit model
* Assuming $\epsilon \sim N\left(0, 1\right)$ leads to the probit model
* And so on...


## Application: surrogate residual

* There are several types of residuals defined for logistic regression (e.g., deviance residuals and Pearson residuals)
* The [surrogate residual]() act like the usual residual from a normal linear model and has similar properties!
* See [Liu and Zhang (2018)](https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1292915), [Greenwell et al. (2018)](https://journal.r-project.org/archive/2018/RJ-2018-004/), and [Cheng et al. (2020)](https://www.tandfonline.com/doi/full/10.1080/10618600.2020.1775618) for details
* A novel R-squared measure based on the surrogate idea was proposed in [Liu et al. (2022)](https://bpspsychub.onlinelibrary.wiley.com/doi/10.1111/bmsp.12289)
* For implementation, see R packages [sure](https://cran.r-project.org/package=sure), [SurrogateRsq](https://cran.r-project.org/package=SurrogateRsq), and [PAsso](https://cran.r-project.org/package=PAsso)


# Binomial data

## O-rings example

On January 28, 1986, the [Space Shuttle Challenger broke apart](https://en.wikipedia.org/wiki/Space_Shuttle_Challenger_disaster) 73 seconds into its flight, killing all seven crew members aboard. The crash was linked to the failure of O-ring seals in the rocket engines. Data was collected on the 23 previous shuttle missions. The launch temperature on the day of the crash was 31 $^\circ$F.

```{r}
head(orings <- faraway::orings)
```


## O-rings example

```{r}
#| par: true
plot(damage/6 ~ temp, data = orings, xlim = c(25, 85), pch = 19,
     ylim = c(0, 1), ylab = "Proportion damaged")
```


## O-rings example

Expand binomial data into independent Bernoulli trials
```{r}
tmp <- rep(orings$temp, each = 6)
dmg <- sapply(orings$damage, FUN = function(x) rep(c(0, 1), times = c(6 - x, x)))
orings2 <- data.frame("temp" = tmp, "damage" = as.vector(dmg))
head(orings2, n = 15)
```


## O-rings example

Here we'll just fit a logistic regression to the expanded bernoulli version of the data
```{r}
# Fit a logistic regression (LR) model using 0/1 version of the data
orings.lr <- glm(damage ~ temp, data = orings2, 
                 family = binomial(link = "logit"))
summary(orings.lr)
```


## O-rings example

What's the estimated probability that an O-ring will be damaged at 31 $^\circ$F? Give a point estimate as well as a 95% confidence interval.

. . .

```{r}
predict(orings.lr, newdata = data.frame("temp" = 31), type = "response", 
        se = TRUE)


# Better approach for a 95% CI?
pred <- predict(orings.lr, newdata = data.frame("temp" = 31), 
                type = "link", se = TRUE)
plogis(pred$fit + c(-qnorm(0.975), qnorm(0.975)) * pred$se.fit)
```


## O-rings example

```{r}
#| par: true
# Is this extrapolating?
plot(damage / 6 ~ temp, data = orings, pch = 19, cex = 1.3,
     col = adjustcolor(1, alpha.f = 0.3), xlim = c(0, 100), ylim = c(0, 1))
x <- seq(from = 0, to = 100, length = 1000)
y <- predict(orings.lr, newdata = data.frame("temp" = x), 
             type = "response")
lines(x, y, lwd = 2, col = 2)
abline(v = 31, lty = 2, col = 3)
```


## O-rings examples

More interesting question: at what temperature(s) can we expect the risk/probability of damage to exceed 0.8?

. . .

This is a problem of inverse estimation, which is the purpose of the [investr package](https://journal.r-project.org/archive/2014/RJ-2014-009/index.html)!
```{r}
# To install from CRAN, use
#
# > install.packages("investr")
#
# See ?investr::invest for details and examples
investr::invest(orings.lr, y0 = 0.8, interval = "Wald", lower = 40, upper = 60)
```


## O-rings example

Here's an equivalent logistic regression model fit to the original binomial version of the data (need to provide number of successes and number of failures)
```{r}
orings.lr2 <- glm(cbind(damage, 6 - damage) ~ temp, data = orings,
                  family = binomial(link = "logit"))
summary(orings.lr2)
```


# Overdispersion


## Overdispersion

* Recall that for a Bernoulli random variable $Y$ the mean and variance are given by $E\left(Y\right) = p$ and $V\left(Y\right) = p\left(1 - p\right)$

* In other words, once the mean is specified by the logistic regression, the variance is determined at the same time! 

* This is different from the case of linear regression models

* Sometimes the data suggest that the variance is consistently greater than $p\left(1 - p\right)$

* Including a dispersion parameter can make the model more flexible: $V\left(Y\right) = \sigma^2 p\left(1 - p\right)$


## Overdispersion

* Over-dispersion is said to exist when there is more variability than expected under the response distribution; similar for underdispersion.

* For a correctly specified model, the Pearson chi-square statistic and the deviance, divided by their degrees of freedom, should be approximately equal to one. When their values are much larger than one, the assumption of binomial variability might not be valid and the data are said to exhibit overdispersion. Underdispersion, which results in the ratios being less than one, occurs less often in practice.


## Overdispersion

* Overall performance of the fitted model can be measured by several different goodness-of-fit tests. Two tests that require replicated data (multiple observations with the same values for all the predictors) are the Pearson chi-square goodness-of-fit test and the deviance goodness-of-fit test (analagous to the multiple linear regression lack-of-fit F-test).

* When fitting a model, there are several problems that can cause the goodness-of-fit statistics to exceed their degrees of freedom. Among these are such problems as outliers in the data, using the wrong link function, omitting important terms from the model, and needing to transform some predictors. These problems should be eliminated before proceeding to use the following methods to correct for overdispersion.

* A large difference between the Pearson statistic and the deviance provides some evidence that the data are too sparse to use either statistic.


## O-rings example

You can check for overdispersion manually or by using [performance](https://cran.r-project.org/package=performance):
```{r}
performance::check_overdispersion(orings.lr2)
```


## Overdispersion 

Two common (but equivalent) ways to handle overdispersion in R:

1. Estimate the dispersion parameter $\sigma^2$ and provide it to `summary()` to adjust  the standard errors approriately

2. Use the `quasibinomial()` family


## Overdispersion

Estimate the dispersion parameter $\sigma^2$; analagous to MSE in linear regression
```{r}
(sigma2 <- sum(residuals(orings.lr2, type = "pearson") ^ 2) / orings.lr2$df.residual)

# Print model summary based on estimated dispersion parameter
summary(orings.lr2, dispersion = sigma2)
```


## Overdispersion

Can use `quasibinomial()` family to account for over-dispersion automatically (notice the estimated coeficients don't change)
```{r}
orings.qb <- glm(cbind(damage, 6 - damage) ~ temp, data = orings, 
                 family = quasibinomial)
summary(orings.qb)
```


## Generalized additive models (GAMs)

* $\text{logit}\left(p\right) = \beta_0 + f_1\left(x_1\right) + f_2\left(x_2\right) + \dots + f_p\left(x_p\right)$

  - The $f_i$ are referred to as *shape functions* or *term contributions* and are often modeled using *splines*
  
  - Can also include pairwise interactions of the form $f_{ij}\left(x_i, x_j\right)$
  
* **Easy to interpret** (e.g., just plot the individual shape functions)!
  - Interaction effects can be understood with heat maps, etc.
  
* **Modern GAMs**, called [GA<sup>2</sup>Ms](https://www.cs.cornell.edu/~yinlou/papers/lou-kdd13.pdf), automatically include relevant pairwise interaction effects


## Explainable boosting machines (EBMs)

* EBM ≈ .darkblue[GA<sup>2</sup>M] + .green[Boosting] + .tomato[Bagging]
* Python library: [interpret](https://github.com/interpretml/interpret/)

<iframe width="700" height="393.75" src="https://www.youtube.com/embed/MREiHgHgl0k?si=kkPulwajyqlXekPa" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>


# Questions?