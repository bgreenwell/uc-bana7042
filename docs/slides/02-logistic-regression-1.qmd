---
title: "Logistic Regression (Part I)"
author: "Brandon M. Greenwell, PhD"
institute: "University of Cincinnati"
from: markdown+emoji
format: 
    revealjs:
        df-print: paged
        code-fold: true
        code-summary: "Show R code"
        code-copy: true
        logo: images/uc.png
        chalkboard: true
        slide-number: true
        scrollable: true
        embed-resources: false
        footer: "BANA 7042: Statistical Modeling"
---

```{r, include=FALSE}
palette("Okabe-Ito")

library(knitr)

opts_chunk$set(
  echo = TRUE,
  dpi = 300,     
  #fig.retina = 3,  
  fig.align = "center",
  fig.width = 6,
  fig.asp = 0.618,
  out.width = "100%",
 # size = "small",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

knit_hooks$set(
  par = function(before, options, envir) {
    if (before && options$fig.show != "none") {
      par(
        mar = c(4, 4, 2, 0.1), 
        cex.lab = 0.95, 
        cex.axis = 0.8,  # was 0.9
        mgp = c(2, 0.7, 0), 
        tcl = -0.3, 
        las = 1
      )
      if (is.list(options$par)) {
        do.call(par, options$par)
      }
    }
  }
)
```


## Western collaborative group study

$N = 3154$ healthy young men aged 39--59 from the San Francisco area were assessed for their personality type. All were free from coronary heart disease at the start of the research. Eight and a half years later change in this situation was recorded. See `?faraway::wcgs` in R.

```{r}
# install.packages("faraway")
head(wcgs <- faraway::wcgs)
```


## Structure of `wcgs` data

```{r}
str(wcgs)
```


## Description of each variable

```{r}
?wcgs
```
* `age`: age in years
* `height`: height in inches
* `weight`: weight in pounds
* `sdp`: systolic blood pressure in mm Hg
* `dbp`: diastolic blood pressure in mm Hg
* `chol`: fasting serum cholesterol in mm %
* `behave`: behavior type which is a factor with levels A1 A2 B3 B4
* `cigs`: number of cigarettes smoked per day
* `dibep`: behavior type a factor with levels A (Agressive) B (Passive)
* `chd`: coronary heat disease developed is a factor with levels no yes
* `typechd`: type of coronary heart disease is a factor with levels angina infdeath none silent
* `timechd`: time of CHD event or end of follow-up
* `arcus`: arcus senilis is a factor with levels absent present


## For now, we'll focus on 3 variables

```{r}
summary(wcgs[, c("chd", "height", "cigs")])
```

. . .

</br></br>
Anything interesting stick out?


## Visualizing discrete data

Pie chart of response :vomiting_face:

```{r}
#| par: true
# Construct a pie chart of the (binary) response; I'm not a fan of pie charts in general
ptab <- prop.table(table(wcgs$chd))  # convert frequencies to proportions
ptab  # inspect output
pie(prop.table(table(wcgs$chd)),
    main = "Pie chart of Coronary Heart Disease")
```


## Visualizing discrete data

Bar charts tend to be more effective
```{r}
#| par: true
barplot(ptab, las = 1, col = "forestgreen")
```


## Visualizing discrete data

[Mosaic plot](https://en.wikipedia.org/wiki/Mosaic_plot) showing relationship between `cigs` and `chd`; not incredibly useful IMO unless both variables are categorical

```{r}
#| par: true
plot(chd ~ cigs, data = wcgs) 
```


## Visualizing discrete data

Nonparametric density plot of `height` by `chd` status using [lattice](https://cran.r-project.org/package=lattice) graphics:

```{r}
#| par: true
library(lattice)

densityplot(~ height, groups = chd, data = wcgs, auto.key = TRUE)
```


## Visualizing discrete data

Boxplot of `cigs` vs. `chd` status
```{r}
#| par: true
plot(cigs ~ chd, data = wcgs, col = c(2, 3))
```


## Visualizing discrete data

Boxplot of `cigs` vs. `chd` status
```{r}
#| par: true
plot(cigs ~ chd, data = wcgs, col = c(2, 3))
```


## Visualizing discrete data

Boxplot of `height` vs. `chd` status with *notches*
```{r}
#| par: true
plot(height ~ chd, data = wcgs, col = c(2, 3), notch = TRUE)
```


## Visualizing discrete data (detour)

* [Decision trees](https://en.wikipedia.org/wiki/Decision_tree) :evergreen_tree::palm_tree::deciduous_tree: are immensely useful for exploring new data sets!

* Some useful resources: 

  - See the documentation for R packages [rpart](https://cran.r-project.org/package=rpart), [party](https://cran.r-project.org/package=party), and [partykit](https://cran.r-project.org/package=rpart)

  - [Fifty Years of Classification and Regression Trees](http://pages.stat.wisc.edu/~loh/treeprogs/guide/LohISI14.pdf)

  - [Recursive Partitioning and Applications](https://www.amazon.com/Recursive-Partitioning-Applications-Springer-Statistics/dp/1441968237) (logistic regression and trees)

  - The ultimate [tree book](https://bgreenwell.github.io/treebook/) :sunglasses:


## Visualizing discrete data

Standard [CART-like decision tree](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf) using [rpart](https://cran.r-project.org/package=rpart) and [rpart.plot](https://cran.r-project.org/package=rpart.plot) (all variables allowed):
```{r}
#| par: true
rpart.plot::rpart.plot(rpart::rpart(chd ~ ., data = wcgs))
```


## Visualizing discrete data

[Conditional inference tree](https://cran.r-project.org/web/packages/partykit/vignettes/ctree.pdf) using [partykit](https://cran.r-project.org/package=partykit) (using only variables of interest):
```{r}
#| par: true
plot(partykit::ctree(chd ~ height + cigs, data = wcgs))
```


## Observations so far...

* It seems that the `cigs` is positively associated with the binary
response `chd`
* It is not clear how, if at all, `height` is associated with `chd`
* **Question**: how can we build a model to examine these potential associations?


## Linear models

Recall that in linear regression we model the *conditional* mean response as a linear function in some fixed, but known parameters $\boldsymbol{\beta}$:

$$
E\left(Y|\boldsymbol{x}\right) = \beta_0 + \beta_1x_1 + \dots \beta_px_p = \boldsymbol{\beta}^\top\boldsymbol{x}
$$

. . .

If $Y$ is a binary random variable, then what is $E\left(Y|\boldsymbol{x}\right)$?


## The linear probability (LP) model

It turns out that $E\left(Y|\boldsymbol{x}\right) = P\left(Y = 1|\boldsymbol{x}\right)$. The [LP model](https://en.wikipedia.org/wiki/Linear_probability_model#:~:text=In%20statistics%2C%20a%20linear%20probability,one%20or%20more%20explanatory%20variables.) assumes that
$$
P\left(Y = 1|\boldsymbol{x}\right) = \beta_0 + \beta_1x_1 + \dots \beta_px_p = \boldsymbol{\beta}^\top\boldsymbol{x}
$$

. . .

Is this reasonable?


## The LP model: `chd` vs. `cigs`

```{r}
y <- ifelse(wcgs$chd == "yes", 1, 0)
summary(fit <- lm(y ~ cigs, data = wcgs))
```

Are the standard errors here appropriate? Why/why not?


## The LP model: `chd` vs. `cigs`

```{r}
#| par: true
plot(y ~ cigs, data = wcgs, ylim = c(0, 1), las = 1)
abline(fit, col = 2)
```


## The logistic regression (LR) model

Assume $Y \sim \mathrm{Bernoulli}\left(p\right)$, where 
$$
p = p\left(\boldsymbol{x}\right) = P\left(Y = 1|\boldsymbol{x}\right)
$$
and
$$
\mathrm{logit}\left(p\right) = \log\left(\frac{p}{1-p}\right) = \boldsymbol{\beta}^\top\boldsymbol{x}
$$
In other words, LR models the [logit](https://en.wikipedia.org/wiki/Logit) of the mean response as a linear function in $\boldsymbol{\beta}$; we'll refer to the term $\eta = \boldsymbol{\beta}^\top\boldsymbol{x}$ as the [linear predictor](https://en.wikipedia.org/wiki/Linear_predictor_function#:~:text=In%20statistics%20and%20in%20machine,outcome%20of%20a%20dependent%20variable.). Why does this make more sense?


## The logistic regression (LR) model

Can always solve for $p$ to get predictions on the raw probability scale (**Homework 2** :smile:):
$$
p\left(\boldsymbol{x}\right) = \frac{\exp\left(\boldsymbol{\beta}^\top\boldsymbol{x}\right)}{1 + \exp\left(\boldsymbol{\beta}^\top\boldsymbol{x}\right)}
$$

. . .

Note how the LR model is nonlinear in $p$!


## Fitting an LR model in R

* Use the `glm()` function instead of `lm()`
* GLM stands for [generalized linear model](https://en.wikipedia.org/wiki/Generalized_linear_model), which includes the LR and ordinary linear regression models as special cases
* Many (but not all) of the models we'll discuss in throughout this course belong to the class of GLMs
* Note how we have to specifcy the `family` argument! (see `?glm` for details)
* The response can be a 0/1 indicator or a factor variable (be careful with interpretation and [which class is used as the baseline](https://stats.stackexchange.com/questions/207427/confused-with-the-reference-level-in-logistic-regression-in-r)):
```{r}
levels(wcgs$chd)
```


## Fitting an LR model in R 

```{r}
summary(fit.lr <- glm(chd ~ cigs, data = wcgs, family = binomial))
```


## Fitting an LR model in R 

Let's compare the fitted LR and LP models:
```{r}
#| par: true
prob <- predict(fit.lr, newdata = data.frame(cigs = 0:99), type = "response")
plot(y ~ cigs, data = wcgs, ylab = "chd", las = 1, xlim = c(0, 99))
lines(0:99, y = prob, col = 2)
abline(fit, col = 3, lty = 2)
legend("topright", legend = c("LR fit", "LP fit"), lty = c(1, 2), col = c(2, 3))
```


## Fitting an LR model in R 

Let's compare the fitted LR and LP models:
```{r}
#| par: true
prob <- predict(fit.lr, newdata = data.frame(cigs = 0:999), type = "response")
plot(y ~ cigs, data = wcgs, ylab = "chd", las = 1, xlim = c(0, 999))
lines(0:999, y = prob, col = 2)
abline(fit, col = 3, lty = 2)
legend("topright", legend = c("LR fit", "LP fit"), lty = c(1, 2), col = c(2, 3))
```


## Fitting an LR model in R 

Now let's include an additional predictor (i.e., `height`):
```{r}
summary(fit.lr2 <- glm(chd ~ cigs + height, data = wcgs, family = binomial))
```


## Interpreting LR coefficients

* Let $p = P\left(Y = 1\right)$ and $1 - p = P\left(Y = 0\right)$
* The odds of $Y = 1$ occuring is defined as $p / \left(1 - p\right)$
* For a fair coin :coin:, the probability of getting tails is $p = 0.5$. Therefore, the odds of getting tails vs. heads is $p / (1 - p) = 0.5 / 0.5 = 1$. (We might also say the odds of getting tails is "1 to 1").

. . .

</br>

:::{.r-stack}
For a fair die :game_die:, what are the odds of rolling a 2?
:::


## Solution {visibility="hidden"}

* For a fair die :game_die:, the probability of rolling a 2 is $p = 1/6$ and the probability of not rolling a 2 is $1 - p = 5/6$. Therefore, the odds of rolling a 2 vs. not rolling a 2 is $\frac{p}{1 - p} = \frac{1/6}{5/6} = \frac{1}{5}$. (We might also say the odds of rolling a 2 is "1 to 5").


## Interpreting LR coefficients {.smaller}

The logit models the *log odds* of success (i.e., $Y = 1|\boldsymbol{x}$)
$$
\log\left(\mathrm{odds}\right) = \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots \beta_px_p
$$
Exponentiating both, we get
$$
\mathrm{odds} = \frac{p}{1-p} = \exp{\left(\beta_0\right)}\times\exp{\left(\beta_1x_1\right)}\times\exp{\left(\beta_2x_2\right)}\times\dots\times\exp{\left(\beta_px_p\right)}
$$

* In the LR model, $\beta_i$ represents the change in the *log odds* when $x_i$ increases by one unit (all else held constant)
* In the LR model, $\exp\left(\beta_i\right)$ represents the *multiplicative increase* in the odds when $x_i$ increases by one unit (all else held constant)
* CANNOT interpret the coefficients in terms of $p$ directly...effect plots to the rescue :ring_buoy:!!


## WCGS study

```{r}
coef(fit.lr2)
```

. . .

* Holding `height` constant, for every additional cigarette smoked per day the predicted log odds of developing `chd` increases by `r round(coef(fit.lr2)["cigs"], 3)`

* Holding `height` constant, for every additional cigarette smoked per day the predicted odds of developing `chd` increases multiplicatively by `r round(exp(coef(fit.lr2)["cigs"]), 3)`


## Effect plots

Lot's of different methods and packages:

* Marginal effects via [effects](https://cran.r-project.org/package=effects) library
* [Partial dependence (PD) plots](https://christophm.github.io/interpretable-ml-book/pdp.html) and [individual conditional expectation]() (ICE) plots via the [pdp]() package
* Marginal effect and PD plots via the [plotmo]([plotmo](http://www.milbo.org/doc/plotmo-notes.pdf)) library
* And many, many more (see [R's ML Task View](https://cran.r-project.org/web/views/MachineLearning.html))
* All have there own assumptions and drawbacks; typically, similar in shape when the model is additive in nature (i.e., no interaction effects)


## Effect plots

The [plotmo](http://www.milbo.org/doc/plotmo-notes.pdf) library is an "easy button" for quick and dirty effect plots (other variables are held fixed at their median or most frequent value) and supports a wide range of models
```{r}
#| par: true
plotmo::plotmo(fit.lr2)
```


## Effect plots

I generally prefer PD plots; see [Greenwell (2017)](https://journal.r-project.org/archive/2017/RJ-2017-016/RJ-2017-016.pdf) for details
```{r}
#| par: true
library(pdp)
library(ggplot2)

theme_set(theme_bw())
partial(fit.lr2, pred.var = "cigs", prob = TRUE, plot = TRUE, 
        plot.engine = "ggplot2", rug = TRUE) + 
    ylim(0, 1) + 
    ylab("Probability")
```


## Effect plots

Can easily extend these methods to two or three variables:
```{r}
#| par: true
pd.2d <- partial(fit.lr2, pred.var = c("cigs", "height"), chull = TRUE)
plotPartial(pd.2d)
```


## Effect plots

Three-dimensional plots look cool, but generally aren't all that useful:
```{r}
#| par: true
lattice::wireframe(yhat ~ height * cigs, data = pd.2d, shade = TRUE)
```


## ML estimation (Bernoulli)

* In the linear model, LS and ML estimation are equivalent and rather straightforward

* [In LR, ML estimation is more common](https://en.wikipedia.org/wiki/Logistic_regression#Model_fitting)^[Technically, there are a number of ways to estimate the parameters of an LR model, but ML estimation is most common.]

* Recall that if $Y_i \stackrel{iid}{\sim} Bernoulli\left(p\right)$, then the [likilhood function](https://en.wikipedia.org/wiki/Likelihood_function) is defined as

$$
L\left(p\right) = \prod_{i=1}^n p^{y_i} \left(1-p\right)^{1-y_i}
$$


## ML estimation (Bernoulli)

* Theoretically, goal is to maximize $L\left(p\right)$

* In practice, it's easier to work with the *log likelihood* $l\left(p\right) = log\left[L\left(p\right)\right]$

$$
\begin{align}
l\left(p\right) &= \log\left[\prod_{i=1}^n p^{y_i} \left(1-p\right)^{1-y_i}\right]\\
&= \cdots\\
&= \log\left(p\right)\sum_{i=1}^ny_i + \log\left(1-p\right)\sum_{i=1}^n\left(n-y_i\right)
\end{align}
$$


## ML estimation (LR)

* In LR, 
$$
p_i = p\left(\boldsymbol{x}_i\right) = \frac{\exp\left(\boldsymbol{\beta}^\top\boldsymbol{x}_i\right)}{1 + \exp\left(\boldsymbol{\beta}^\top\boldsymbol{x}_i\right)}
$$

* This becomes a more complicated optimization problem!

* Equivalent to minimizing [log loss](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.log_loss.html) in machine learning

* Log loss is a [proper scoring rule](https://www.fharrell.com/post/class-damage/)


## ML estimation (LR)

Maximizing the log-likelihood is equivalent to minimizing the *negative log-likelihood* (which is more convenient):
```{r}
# Response (as a binary 0/1 variable)
y <- ifelse(wcgs$chd == "yes", 1, 0)

# Model matrix; includes a column for the intercept by default
X <- model.matrix(~ cigs + height, data = wcgs)

# Function to compute the negative log-liklihood (as a function of the betas)
nll <- function(beta) {
  lp <- X %*% beta  # linear predictor; same as b0 + b1*x1 + b2*x2 + ...
  -sum(y * lp - log(1 + exp(lp)))
}

# Use general optimization; would be better to use gradient and hessian info (e.g., first and second derivative info)
lp.fit <- lm(y ~ cigs + height, data = wcgs)
optim(coef(lp.fit), fn = nll, 
      control = list("maxit" = 9999, "reltol" = 1e-20))
```


## Wald statistic

* Consider the usual marginal test: $H_0: \beta_j = 0$ vs. $H_1: \beta_j \ne 0$
* Assymptotically speaking, $Z_i = \hat{\beta}_i / \mathrm{SE}\left(\hat{\beta}_i\right)$ has a standard normal distribution under $H_0$
* This leads to the usual [Wald-based confidence intervals](https://en.wikipedia.org/wiki/Wald_test), etc.
* Better (but more complicated) approaches available, like [likelihood ratio tests](https://en.wikipedia.org/wiki/Likelihood-ratio_test#:~:text=In%20statistics%2C%20the%20likelihood%2Dratio,the%20ratio%20of%20their%20likelihoods.), [profile likelihood methods](https://en.wikipedia.org/wiki/Likelihood_function#Profile_likelihood), and the [bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)).


## Extending the general linear F-test

In LR, we move from a general linear F-test to a [likelihood ratio test](https://en.wikipedia.org/wiki/Likelihood-ratio_test) based on the $\chi^2$ distribution:
$$
X = -2\log\left(\frac{L_{H_0}}{L_{H_1}}\right) = D_{H_0} - D_{H_1} \sim \chi^2\left(df\right)
$$

Try by hand:
```{r}
fit.H0 <- glm(chd ~ cigs, data = wcgs, family = binomial)
fit.H1 <- glm(chd ~ cigs + height, data = wcgs, family = binomial)
D.H0 <- deviance(fit.H0)
D.H1 <- deviance(fit.H1)
chisq.stat <- D.H0 - D.H1
pval <- pchisq(chisq.stat, df = 1, lower.tail = FALSE)
c("stat" = chisq.stat, "pval" = pval)
```


## Extending the general linear F-test

Can do this automatically using R's `anova()` function:
```{r}
anova(fit.H0, fit.H1, test = "Chi")
```


## Confidence intervals

Lot's of ways to do this (some bad, some better). R defaults to using a [profile likelihood](https://en.wikipedia.org/wiki/Likelihood_function) method:

```{r}
confint(fit.lr2, level = 0.95)
```


## Questions:question::question::question:
