---
title: "Linear Regression (A Brief Review)"
author: "Brandon M. Greenwell, PhD"
institute: "University of Cincinnati"
from: markdown+emoji
format: 
    revealjs:
        df-print: paged
        code-fold: true
        code-summary: "Show R code"
        code-copy: true
        logo: images/uc.png
        chalkboard: true
        slide-number: true
        scrollable: true
        embed-resources: false
        footer: "BANA 7042: Statistical Modeling"
---

```{r, include=FALSE}
palette("Okabe-Ito")

```{r, setup}
library(knitr)

opts_chunk$set(
  echo = TRUE,
  dpi = 300,     
  #fig.retina = 3,  
  fig.align = "center",
  fig.width = 6,
  fig.asp = 0.618,
  out.width = "100%",
 # size = "small",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

knit_hooks$set(
  par = function(before, options, envir) {
    if (before && options$fig.show != "none") {
      par(
        mar = c(4, 4, 0.1, 0.1), 
        cex.lab = 0.95, 
        cex.axis = 0.8,  # was 0.9
        mgp = c(2, 0.7, 0), 
        tcl = -0.3, 
        las = 1
      )
      if (is.list(options$par)) {
        do.call(par, options$par)
      }
    }
  }
)
```

## A useful quote to remember

> All models are wrong, but some are useful.
> 
> -George Box


## Ames housing data

* Data describing the sale of individual residential property in Ames, Iowa
from 2006 to 2010
* There are 2930 observations on 81 variables involved in assessing home
values:
  - 23 nominal
  - 23 ordinal
  - 14 discrete
  - 20 continuous
* Paper: [https://jse.amstat.org/v19n3/decock.pdf](https://jse.amstat.org/v19n3/decock.pdf)


## Ames housing data {.scrollable}

```{r}
#| echo: true
ames <- AmesHousing::make_ames()  # install.packages("AmesHousing")
ames$Sale_Price <- ames$Sale_Price / 10000
head(ames)
```


## Ames housing data

We'll focus on a handful of variables:

* `Sale_Price` - Sale price of the house / \$10K (response variable)
* `Gr_Liv_Area` - Above grade (ground) living area square feet
* `Overall_Qualâ ` - Rates the overall material and finish of the house


## Inference for a single variable {.smaller}

* Is it useful to test the hypothesis that `Sale_Price` = \$160K?

. . .

* No! Because `Sale_Price` is not a constant, but a random variable whose value varies from home to home (and year to year, etc.)

. . .

* We're more interested in questions such as:

  - What is the chance that `Sale_Price` > \$160K? (above median sale price)
  - What is the chance that `Sale_Price` < \$105K? (lowest decile)
  - What is the chance that \$129,500 < `Sale_Price` < \$213,500? (within IQR)


## Distribution of `Sale_Price`

Can look at historgram and empirical [CDF](https://en.wikipedia.org/wiki/Cumulative_distribution_function):

```{r}
#| echo: true
#| fig-align: center
#| fig-width: 6
#| fig-asp: 0.5
par(mfrow = c(1, 2), las = 1)
hist(ames$Sale_Price, br = 50, xlab = "Sale price ($)", freq = FALSE, main = "")
plot(ecdf(ames$Sale_Price), xlab = "Sale price ($)", main = "",
     col = adjustcolor(1, alpha.f = 0.1))
```


## Distribution of `Sale_Price`

* Histograms and ECDFs are nonparammetric in nature
* A simple parametric approach might assume a particular distribution for `Sale_Price`
* For instance, we might assume `Sale_Price` $\sim N\left(\mu, \sigma^2\right)$
* How can we estimate $\mu$ and $\sigma^2$?

. . .

```{r}
#| echo: true
# Maximum likelihiid estimates
c("sample mean" = mean(ames$Sale_Price), "sample stdev" = sd(ames$Sale_Price))
```

. . .

* Is the normal distribution a reasonable assumption here?


## Normal QQ plot

* Normal quantile-quantile (Q-Q) plot* can be used to asses the "normalityness" of a set of observations

* Q-Q plots can, in general, be used to compare data with any distribution!

```{r}
#| echo: true
#| fig-align: center
#| fig-width: 6
#| fig-asp: 0.618
qqnorm(ames$Sale_Price, col = 2, las = 1)
qqline(ames$Sale_Price)
```


## Normality tests :vomiting_face: 

* Normality tests, like the [Shapiro-Wilk](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test)^[In R, see `?shapiro.test` for details.] and [Anderson-Darling](https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test) tests, can also be used to assess normality

  - I STRONGLY ADVISE AGAINST USING THEM!
  
* No data is normally distributes, what we care about is whether enough a normal approximation is close enough!

* Normality tests provide a $p$-value, which only gives a yes/no conclusion


## Normality tests :vomiting_face: 

Recall that $p$-values are a function of sample size!

```{r}
#| echo: true
# Shapiro-Wilk test results vs. sample size
set.seed(101)  # for reproducibility
x <- replicate(100, c(
  shapiro.test(rt(10, df = 40))$p.value,
  shapiro.test(rt(100, df = 40))$p.value,
  shapiro.test(rt(500, df = 40))$p.value,
  shapiro.test(rt(1000, df = 40))$p.value,
  shapiro.test(rt(2500, df = 40))$p.value,
  shapiro.test(rt(5000, df = 40))$p.value
))
rownames(x) <- paste0("n=", c(10, 100, 500, 1000, 2500, 5000))
rowMeans(x < 0.05)
```


## Normality tests :vomiting_face: 

```{r}
#| echo: false
#| par: true
#| fig.width: 6
#| fig.asp: 0.618
#| out.width: "100%"
x <- seq(from = -5, to = 5, length = 500)
y1 <- dnorm(x)
y2 <- dt(x, df = 40)
# palette("Okabe-Ito")
plot(x, y1, type = "l", xlab = "", ylab = "Density")
lines(x, y2, col = 2)
legend("topleft", legend = c("Standard normal", "t (df = 40)"), lty = 1,
       col = c(1, 2), inset = 0.01, bty = "n")
# palette("default")
```

1. Are these two distributions significantly different?

2. Are these two distributions practically different?


## Is linear regression reasonable here?

```{r}
x <- rep(c(1:10 / 10, 1.5, 2), each = 30)
y <- 1 + 2*x^2 + rnorm(length(x), sd = 1)
par(mfrow = c(1, 2), las = 1)
plot(x, y)
hist(y, 50)
```


## Is linear regression reasonable here?

```{r}
fit <- lm(y ~ x)
res <- residuals(fit)
qqnorm(res, las = 1)
qqline(res, col = 2)
```


## What can we do if the normality assumption isn't justified?

* Try transformations
  - Logarithm or square root for positive data
  - [Power transformation](https://en.wikipedia.org/wiki/Power_transform) (like the well-known Box-Cox procedure)

* Try a more appropriate distribution (e.g., Poisson or gamma distribution)

* Try more advanced approaches, like the nonparametric [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))!


## Modeling the mean response

* Assume that $Y \sim N\left(\mu, \sigma^2\right)$, where

$$\mu = \mu\left(x\right) = \beta_0 + \beta_1 x = E\left(Y|x\right)$$

. . .

* In other words: $Y \sim N\left(\beta_0 + \beta_1 x, \sigma^2\right)$
* Alternatively, we could write $Y = \beta_0 + \beta_1 x + \epsilon$, where $\epsilon \sim N\left(0, \sigma^2\right)$
* This is called the simple linear regression (SLR) model


## The idea behind SLR

![](https://2012books.lardbucket.org/books/beginning-statistics/section_14/88a6e0919d8617c025826c1e187ad591.jpg)

[Image source](https://2012books.lardbucket.org/books/beginning-statistics/s14-03-modelling-linear-relationships.html)


## Arsenic experiment example

```{r}
#| echo: true
#| fig-align: center
#| fig-width: 6
#| fig-asp: 0.618
plot(investr::arsenic, las = 1)  # see ?investr::arsenic for details
```


## Least squares (LS) estimation

Idea of LS is to find $\beta_0$ and $\beta_1$ so that the sum of squared residuals (i.e., errors) is minimized:
$$SSE = \sum_{i=1}^n\left(y_i - \beta_0 - \beta_1x_i\right)^2$$

. . .

Pretty straightforward optimization problem that leads to closed-form solution (but no point in memorizing the formulas!)


## Concept of LS estimation

![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*tQkyTR9yxDcS1GKVFhdQQA.jpeg)

[Image source](https://towardsdatascience.com/how-least-squares-regression-estimates-are-actually-calculated-662d237a4d7e)


## `Sale_Price` and `Gr_Liv_Area`

```{r}
#| echo: true
plot(Sale_Price ~ Gr_Liv_Area, data = ames, las = 1,
     col = adjustcolor(1, alpha.f = 0.3))
```


## SLR fit

```{r}
#| echo: true
summary(fit <- lm(Sale_Price ~ Gr_Liv_Area, data = ames))
```


## Is this a good fit?

```{r}
#| echo: true
#| fig-align: center
#| fig-width: 6
#| fig-asp: 0.618
plot(Sale_Price ~ Gr_Liv_Area, data = ames,
     col = adjustcolor(1, alpha.f = 0.3))
abline(fit, lwd = 2, col = 2)  # add SLR fit
```

Which assumptions seem violated to some degree?


## Residual diagnostics

* The standard residual is defined as $e_i = y_i - \hat{y}_i$ and can be regarded as the *observed error*
* The residuals hold a lot of properties that make them useful for diagnosing potential issues with the model (e.g., suggesting potential transformations to try)
* Many other kinds of residuals exist for different purposes (e.g., standardized, [studentized](https://en.wikipedia.org/wiki/Studentized_residual), jackknife or [PRESS](https://en.wikipedia.org/wiki/PRESS_statistic) residuals, etc.)


## Properties of the residuals

* $\sum_{i=1}^n e_i = 0$ (**Why?**)

* $\sum_{i=1}^n e_i^2$ is a minimum

* $\sum_{i=1}^n X_ie_i = 0$

* $\sum_{i=1}^n \hat{Y}_ie_i = 0$

* The LS regression line passes through the point $\left(\bar{X}, \bar{Y}\right)$ (i.e., the center of the training data)


## What can residual plots tell us?

* Residuals vs. predictor values (**checking non-linearity**).

* Residuals vs. fitted values (**non-constant variance, non-linearity, and outliers**)

* Residuals vs. time or another sequence (**checking independence**)

* Residuals vs. omitted predictor values (**missing potentially important predictors**)

* Normal QQ plot of residuals (**non-normality**).

* And much, much more!


## Residual analysis for `Sale_Price ~ Gr_Liv_Area`
```{r}
#| echo: true
#| fig-align: center
#| fig-width: 6
#| fig-asp: 0.618
par(mfrow = c(2, 3), las = 1)
plot(fit, which = 1:6)
```

What assumptions appear to be in violation?


## Let's try a log transformation

```{r}
#| echo: true
#| fig-align: center
#| fig-width: 6
#| fig-asp: 0.618
fit2 <- lm(log(Sale_Price) ~ Gr_Liv_Area, data = ames)
plot(log(Sale_Price) ~ Gr_Liv_Area, data = ames,
     col = adjustcolor(1, alpha.f = 0.3))
abline(fit2, lwd = 2, col = 2)  # add SLR fit
```


## Let's try a log transformation

```{r}
#| echo: true
summary(fit2)
```


## Residual analysis for `log(Sale_Price) ~ Gr_Liv_Area`
```{r}
#| echo: true
#| fig-align: center
#| fig-width: 6
#| fig-asp: 0.618
par(mfrow = c(2, 3), las = 1)
plot(fit2, which = 1:6)
```

Any better?


## Multiple linear regression (MLR)

* The (normal) multiple linear regression model assumes $Y \sim N\left(\mu\left(\boldsymbol{x}\right), \sigma^2\right)$, where $$\mu\left(\boldsymbol{x}\right) = \beta_0 + \sum_{i=1}^p \beta_i x_i = \boldsymbol{x}^\top\boldsymbol{\beta}$$

* LS estimation still provides unbiased estimate of $\boldsymbol{\beta} = \left(\beta_0, \beta_1, \dots, \beta_p\right)^\top$: $\hat{\boldsymbol{\beta}} = \left(\boldsymbol{X}^\top\boldsymbol{X}\right)^{-1}\boldsymbol{X}^\top\boldsymbol{y}$

* Fitted values: $\hat{\boldsymbol{y}} = \boldsymbol{X}\left(\boldsymbol{X}^\top\boldsymbol{X}\right)^{-1}\boldsymbol{X}^\top\boldsymbol{y} = \boldsymbol{H}\boldsymbol{y}$

* $\boldsymbol{H}$ is the well-known "[hat matrix](https://en.wikipedia.org/wiki/Projection_matrix)"


## Polynomial regression

* [Polynomial regression](https://en.wikipedia.org/wiki/Polynomial_regression) is just a special case of the MLR model

* A second order model in a single predictor: $$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon$$

* A *k*-th order model in a single predictor (Typically $k \le 3$): $$Y = \beta_0 + \sum_{j=1}^k\beta_j X^j + \epsilon$$ 


## Example: paper strength data

Data concerning the strength of kraft paper and the percentage of hardwood in the batch of pulp from which the paper was produced.

```{r}
# Load the hardwood conentration data
url <- "https://bgreenwell.github.io/uc-bana7052/data/hardwood.csv"
hardwood <- read.csv(url)

# Print first few observations
head(hardwood)
```


## Example: paper strength data

```{r}
#| par: true
plot(hardwood, pch = 19)
```


## Example: paper strength data

```{r}
#| par: true
fit1 <- lm(TsStr ~ HwdCon, data = hardwood)
investr::plotFit(fit1, pch = 19, col.fit = "red2")
```


## Example: paper strength data

```{r}
#| code-fold: true
par(mfrow = c(1, 2), las = 1)

# Plot residuals vs HwdCon (i.e., X)
par(mfrow = c(1, 2))
plot(x = hardwood$HwdCon, y = residuals(fit1), xlab = "HwdCon",
     ylab = "Residuals", main = "Residuals vs HwdCon")
abline(h = 0, lty = "dotted")
plot(fit1, which = 1, caption = "", main = "Residuals vs Fitted")
```


## Example: paper strength data

```{r}
#| par: true
fit2 <- lm(TsStr ~ HwdCon + I(HwdCon^2), data = hardwood)
investr::plotFit(fit2, pch = 19, col.fit = "red2")
```


## Example: paper strength data

```{r}
#| code-fold: true
par(mfrow = c(2, 3), las = 1)
for (i in 1:6) {  # try higher-order models
  fit <- lm(TsStr ~ poly(HwdCon, degree = i), data = hardwood)
  investr::plotFit(fit, main = paste("Degree =", i))
}
```


## Example: paper strength data

```{r}
#| code-fold: true
par(mfrow = c(2, 3))
for (i in 1:6) {  # try higher-order models
  fit <- lm(TsStr ~poly(HwdCon, degree = i), data = hardwood)
  investr::plotFit(fit, main = paste("Degree =", i), 
                   interval = "confidence", shade = TRUE,
                   xlim = c(-10, 30))
}
```


## Polynomial regression

Some cautions :warning:

Keep the order of the model as low as possible

* Avoid interpolating the data or *over fitting*

* Use the simplest model possible to explain the data, but no simpler (*parsimony*)

* An $n - 1$ order model can perfectly fit a data set with $n$ observations (Why is this bad :thinking:)


## Polynomial regression

Two model-building strategies:

1. Fit the lowest order polynomial possible and build up (forward selection)
    
2. Fit the highest order polynomial of interest, and remove terms one at a time (backward elimination)
    
These two procedures may not result in the same final model

Increasing the order can result in an ill-conditioned $\boldsymbol{X}^\top\boldsymbol{X}$ and [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity) 


## Categorical variables

* Categorical variables can be handled in a number of ways in linear models, including

  - [Dummy encoding](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) (nominal)
  - Orthogonal polynomials (ordinal)


## Categorical variables

Let's look at two (nominal) categorical variables:

```{r}
#| echo: true
table(ames$Central_Air)
table(ames$Paved_Drive)
```


## Categorical variables

```{r}
#| echo: true
#| fig-align: center
#| fig-width: 6
#| fig-asp: 0.618
plot(log(Sale_Price) ~ Central_Air, data = ames, las = 1, col = c(2, 3))
```


## Categorical variables

```{r}
#| echo: true
#| fig-align: center
#| fig-width: 6
#| fig-asp: 0.618
plot(log(Sale_Price) ~ Paved_Drive, data = ames, las = 1, col = c(2, 3, 4))
```

If one of these homes downgraded from a paved driveway to a gravel driveway, would that **cause** the sale price to decrease? (Think very carefully here!)


## Categorical variables

R dummy encodes nominal factors by default:

```{r}
#| echo: true
#| fig-align: center
#| fig-width: 6
#| fig-asp: 0.618
fit3 <- lm(log(Sale_Price) ~ Gr_Liv_Area + Central_Air + Paved_Drive, 
           data = ames)
summary(fit3)
```

How do you interpret the coefficients here?


## Coefficient of determination

The coefficient of determination is the proportion of the variance in the dependent variable that is predictable from the independent variables in the model.

:::: {.columns}

::: {.column width="50%"}
R-squared ($R^2$)

* $R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$

* $R^2$ will always increase as more terms are added to the model! 
:::

::: {.column width="50%"}
Adjusted R-squared ($R_{adj}^2$)

<!-- * $R_{adj}^2 = 1 - \left(\frac{n - 1}{n - p}\right)\frac{SSE}{SST}$ -->
* $R_{adj}^2 = 1 - \frac{MSE}{SST/\left(n - 1\right)}$

* Penalizes $R^2$ if there are "too many" terms in the model

* $R_{adj}^2$ and $MSE$ provide equivalent information
:::

::::


## Variable/model selection

* Variable/model selection is a very noisy problem! (Often best to avoid, if feasible)
* Ask the domain experts about important variables (don't just rely on algorithms)
* P(selecting the "right" variables) = 0 ([source](https://www.youtube.com/watch?v=DF1WsYZ94Es))
* "All ~~models~~ subsets of variables are wrong, but some are useful!"
* In regression settings, regularization (e.g., [ridge regression](https://en.wikipedia.org/wiki/Ridge_regression) and the [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics))) is often more useful! (Think about the impact of [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity) on variable selection)


## Data splitting

* If prediction is the goal (e.g., compared to inference and hypothesis testing), the model performance should be assessed rigorously

* Data splitting techniques ae key and the type of data splitting to use often depends on the situation (e.g., cross-sectional vs. time-series data)

* In simplest terms, random split the data into two parts: A and B. Build a model on part A and see how well it does with predicting the response in part B.

* [Leakage](https://reproducible.cs.princeton.edu/) is a huge concern here, so data splitting has to be done carefully!


## Data splitting: $k$-fold cross-validation {.smaller}

![](https://bradleyboehmke.github.io/HOML/images/cv.png)

The [PRESS statistic](https://en.wikipedia.org/wiki/PRESS_statistic) in linear regression is a special case ($k = n$) we get for free!


## Questions?